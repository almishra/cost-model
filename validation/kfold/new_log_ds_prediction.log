['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
3198
34
<class 'numpy.dtype'> float64
3198
train batches:  144  validate samples: 255  test samples: 639
Epoch [0/150], Batch loss: 48.015
Epoch: 0 RMSE:  31.881156024802323  MAPE: 0.9915183899758028  L2+L1 loss: 12.789
Epoch [1/150], Batch loss: 17.089
Epoch: 1 RMSE:  6.737945491890721  MAPE: 1.56633029567483  L2+L1 loss: 3.874
Epoch [2/150], Batch loss: 9.328
Epoch: 2 RMSE:  31.892390006945348  MAPE: 1.8365396210764549  L2+L1 loss: 14.928
Epoch [3/150], Batch loss: 22.52
Epoch: 3 RMSE:  28.95380412054455  MAPE: 1.4246436271118914  L2+L1 loss: 11.295
Epoch [4/150], Batch loss: 18.739
Epoch: 4 RMSE:  26.94460463724553  MAPE: 0.642092422707107  L2+L1 loss: 8.291
Epoch [5/150], Batch loss: 15.818
Epoch: 5 RMSE:  25.467813666446794  MAPE: 1.6982191178191153  L2+L1 loss: 7.605
Epoch [6/150], Batch loss: 13.99
Epoch: 6 RMSE:  24.486917697751885  MAPE: 0.8138146845441735  L2+L1 loss: 7.302
Epoch [7/150], Batch loss: 12.888
Epoch: 7 RMSE:  23.563710324893908  MAPE: 0.689246412709826  L2+L1 loss: 6.93
Epoch [8/150], Batch loss: 14.964
Epoch: 8 RMSE:  23.167370629913314  MAPE: 1.0175248456384027  L2+L1 loss: 6.93
Epoch [9/150], Batch loss: 12.646
Epoch: 9 RMSE:  25.63009754435273  MAPE: 7.622552185707615  L2+L1 loss: 16.361
Epoch [10/150], Batch loss: 14.497
Epoch: 10 RMSE:  24.192950930699872  MAPE: 0.692738208827684  L2+L1 loss: 9.487
Epoch [11/150], Batch loss: 14.665
Epoch: 11 RMSE:  22.441688371085394  MAPE: 0.5899084803976712  L2+L1 loss: 5.292
Epoch [12/150], Batch loss: 10.988
Epoch: 12 RMSE:  22.00881717080281  MAPE: 0.8959668987260653  L2+L1 loss: 6.234
Epoch [13/150], Batch loss: 10.294
Epoch: 13 RMSE:  21.340787518519353  MAPE: 0.6085150708608529  L2+L1 loss: 5.279
Epoch [14/150], Batch loss: 10.901
Epoch: 14 RMSE:  25.53738772199967  MAPE: 2.8815820350221175  L2+L1 loss: 15.761
Epoch [15/150], Batch loss: 15.2
Epoch: 15 RMSE:  20.869602279813787  MAPE: 0.6212713807765914  L2+L1 loss: 4.863
Epoch [16/150], Batch loss: 35.893
Epoch: 16 RMSE:  34.48188929091097  MAPE: 26.87703325241102  L2+L1 loss: 24.662
Epoch [17/150], Batch loss: 37.823
Epoch: 17 RMSE:  32.76502848875265  MAPE: 24.9366349373337  L2+L1 loss: 21.042
Epoch [18/150], Batch loss: 27.718
Epoch: 18 RMSE:  33.08982043707953  MAPE: 23.468726911775757  L2+L1 loss: 22.374
Epoch [19/150], Batch loss: 28.266
Epoch: 19 RMSE:  30.526528433711494  MAPE: 20.0567959427669  L2+L1 loss: 19.893
Epoch [20/150], Batch loss: 37.815
Epoch: 20 RMSE:  35.03357499863455  MAPE: 20.099070999081533  L2+L1 loss: 27.257
Epoch [21/150], Batch loss: 28.296
Epoch: 21 RMSE:  31.835565510798272  MAPE: 16.32201827317708  L2+L1 loss: 22.516
Epoch [22/150], Batch loss: 27.099
Epoch: 22 RMSE:  29.691117015812743  MAPE: 13.137124684219518  L2+L1 loss: 18.821
Epoch [23/150], Batch loss: 21.579
Epoch: 23 RMSE:  29.126008051184957  MAPE: 10.76266633261  L2+L1 loss: 16.713
Epoch [24/150], Batch loss: 21.128
Epoch: 24 RMSE:  28.95656980493707  MAPE: 9.445638991618651  L2+L1 loss: 15.744
Epoch [25/150], Batch loss: 20.733
Epoch: 25 RMSE:  28.923413047285376  MAPE: 9.196402534510005  L2+L1 loss: 15.542
Epoch [26/150], Batch loss: 20.626
Epoch: 26 RMSE:  28.928255212482288  MAPE: 9.328003211653716  L2+L1 loss: 15.629
Epoch [27/150], Batch loss: 20.599
Epoch: 27 RMSE:  28.926567637836687  MAPE: 9.287536998707258  L2+L1 loss: 15.601
Epoch [28/150], Batch loss: 20.493
Epoch: 28 RMSE:  28.920528319878507  MAPE: 9.06967764950035  L2+L1 loss: 15.458
Epoch [29/150], Batch loss: 20.666
Epoch: 29 RMSE:  28.919374200212857  MAPE: 8.930307675138183  L2+L1 loss: 15.366
Epoch [30/150], Batch loss: 20.512
Epoch: 30 RMSE:  28.919404036343916  MAPE: 8.948214911285108  L2+L1 loss: 15.378
Epoch [31/150], Batch loss: 20.335
Epoch: 31 RMSE:  28.91939537582214  MAPE: 8.94467919750982  L2+L1 loss: 15.375
Epoch [32/150], Batch loss: 20.588
Epoch: 32 RMSE:  28.91947398671521  MAPE: 8.967228502100385  L2+L1 loss: 15.39
Epoch [33/150], Batch loss: 15.226
Epoch: 33 RMSE:  10.283741162667239  MAPE: 8.546804487243703  L2+L1 loss: 9.034
Epoch [34/150], Batch loss: 13.832
Epoch: 34 RMSE:  97.86274653275836  MAPE: 10.11427060357077  L2+L1 loss: 27.043
Epoch [35/150], Batch loss: 12.034
Epoch: 35 RMSE:  8.76582385554853  MAPE: 7.7283811545335706  L2+L1 loss: 7.507
Epoch [36/150], Batch loss: 9.605
Epoch: 36 RMSE:  8.741455753316986  MAPE: 7.3126104916439525  L2+L1 loss: 7.019
Epoch [37/150], Batch loss: 8.063
Epoch: 37 RMSE:  7.544132875294369  MAPE: 6.723960440019759  L2+L1 loss: 6.665
Epoch [38/150], Batch loss: 7.777
Epoch: 38 RMSE:  6.607647899506651  MAPE: 6.2412778032967005  L2+L1 loss: 5.79
Epoch [39/150], Batch loss: 7.454
Epoch: 39 RMSE:  10.55197553159456  MAPE: 5.858988022895438  L2+L1 loss: 8.576
Epoch [40/150], Batch loss: 6.131
Epoch: 40 RMSE:  7.754394772749725  MAPE: 5.356276936081196  L2+L1 loss: 6.438
Epoch [41/150], Batch loss: 3.247
Epoch: 41 RMSE:  4.552202013354726  MAPE: 0.3845867009246462  L2+L1 loss: 2.068
Epoch [42/150], Batch loss: 2.836
Epoch: 42 RMSE:  7.712761163297258  MAPE: 0.5814042014142697  L2+L1 loss: 3.053
Epoch [43/150], Batch loss: 4.994
Epoch: 43 RMSE:  1.7742212029910251  MAPE: 0.5904620347936743  L2+L1 loss: 1.695
Epoch [44/150], Batch loss: 2.293
Epoch: 44 RMSE:  3.854475182651699  MAPE: 0.1647793000191065  L2+L1 loss: 1.736
Epoch [45/150], Batch loss: 3.958
Epoch: 45 RMSE:  3.6540073791906176  MAPE: 0.2993976303107881  L2+L1 loss: 2.085
Epoch [46/150], Batch loss: 2.864
Epoch: 46 RMSE:  2.6897524739363856  MAPE: 0.376121921269009  L2+L1 loss: 1.849
Epoch [47/150], Batch loss: 3.092
Epoch: 47 RMSE:  1.9361927940033565  MAPE: 0.8034213564632968  L2+L1 loss: 1.715
Epoch [48/150], Batch loss: 4.275
Epoch: 48 RMSE:  4.228679699852321  MAPE: 0.7633762545548586  L2+L1 loss: 2.379
Epoch [49/150], Batch loss: 5.115
Epoch: 49 RMSE:  3.6688871245929735  MAPE: 0.9182219848339896  L2+L1 loss: 2.117
Epoch [50/150], Batch loss: 3.333
Epoch: 50 RMSE:  4.205566941363727  MAPE: 0.27887038865422376  L2+L1 loss: 2.045
Epoch [51/150], Batch loss: 2.937
Epoch: 51 RMSE:  7.000571347206704  MAPE: 0.14271786243373116  L2+L1 loss: 2.49
Epoch [52/150], Batch loss: 2.816
Epoch: 52 RMSE:  1.590498724958507  MAPE: 0.6453440302449013  L2+L1 loss: 1.485
Epoch [53/150], Batch loss: 2.775
Epoch: 53 RMSE:  3.0548476895001118  MAPE: 0.23623682869523316  L2+L1 loss: 1.741
Epoch [54/150], Batch loss: 3.341
Epoch: 54 RMSE:  3.9281565873841253  MAPE: 0.18091928278141436  L2+L1 loss: 1.64
Epoch [55/150], Batch loss: 3.23
Epoch: 55 RMSE:  1.9373688163452043  MAPE: 0.18708930800168816  L2+L1 loss: 1.305
Epoch [56/150], Batch loss: 2.734
Epoch: 56 RMSE:  1.869948433159639  MAPE: 0.6308499415944432  L2+L1 loss: 1.691
Epoch [57/150], Batch loss: 2.856
Epoch: 57 RMSE:  5.745317596323646  MAPE: 0.22467440916469025  L2+L1 loss: 2.277
Epoch [58/150], Batch loss: 2.567
Epoch: 58 RMSE:  4.0215318629374215  MAPE: 0.21429753434559762  L2+L1 loss: 1.591
Epoch [59/150], Batch loss: 4.412
Epoch: 59 RMSE:  4.5012671000788504  MAPE: 0.18887868884138023  L2+L1 loss: 2.225
Epoch [60/150], Batch loss: 1.832
Epoch: 60 RMSE:  1.346327506395924  MAPE: 0.12929278243849868  L2+L1 loss: 1.122
Epoch [61/150], Batch loss: 1.472
Epoch: 61 RMSE:  1.3748910554976672  MAPE: 0.12470301651228356  L2+L1 loss: 1.145
Epoch [62/150], Batch loss: 1.432
Epoch: 62 RMSE:  1.3415880766749932  MAPE: 0.1099861732064638  L2+L1 loss: 1.139
Epoch [63/150], Batch loss: 1.468
Epoch: 63 RMSE:  1.6001391475629343  MAPE: 0.0910144654311594  L2+L1 loss: 1.209
Epoch [64/150], Batch loss: 1.387
Epoch: 64 RMSE:  1.6276468367553643  MAPE: 0.10095730973239947  L2+L1 loss: 1.209
Epoch [65/150], Batch loss: 1.414
Epoch: 65 RMSE:  1.2102694001386178  MAPE: 0.08255262608613223  L2+L1 loss: 1.057
Epoch [66/150], Batch loss: 1.491
Epoch: 66 RMSE:  1.185288285076603  MAPE: 0.15931799827646712  L2+L1 loss: 1.106
Epoch [67/150], Batch loss: 1.409
Epoch: 67 RMSE:  1.2729064577194338  MAPE: 0.09555253658736533  L2+L1 loss: 1.051
Epoch [68/150], Batch loss: 1.346
Epoch: 68 RMSE:  1.3543591737782177  MAPE: 0.1510720831697523  L2+L1 loss: 1.095
Epoch [69/150], Batch loss: 1.373
Epoch: 69 RMSE:  1.2324634971663608  MAPE: 0.0891012530575844  L2+L1 loss: 1.065
Epoch [70/150], Batch loss: 1.341
Epoch: 70 RMSE:  1.7204613575222079  MAPE: 0.08026459273989725  L2+L1 loss: 1.196
Epoch [71/150], Batch loss: 1.399
Epoch: 71 RMSE:  1.1960074677418222  MAPE: 0.24040664797372746  L2+L1 loss: 1.162
Epoch [72/150], Batch loss: 1.46
Epoch: 72 RMSE:  1.1543165829592485  MAPE: 0.07290653276391958  L2+L1 loss: 1.032
Epoch [73/150], Batch loss: 1.388
Epoch: 73 RMSE:  1.5104004295084956  MAPE: 0.15333397498830914  L2+L1 loss: 1.195
Epoch [74/150], Batch loss: 1.358
Epoch: 74 RMSE:  1.270719955355386  MAPE: 0.0715944955807109  L2+L1 loss: 1.076
Epoch [75/150], Batch loss: 1.359
Epoch: 75 RMSE:  2.4642805741214264  MAPE: 0.2065033452754432  L2+L1 loss: 1.381
Epoch [76/150], Batch loss: 1.525
Epoch: 76 RMSE:  1.3794171085343871  MAPE: 0.13431722645331554  L2+L1 loss: 1.137
Epoch [77/150], Batch loss: 1.426
Epoch: 77 RMSE:  1.9278260357726145  MAPE: 0.09192002756682044  L2+L1 loss: 1.261
Epoch [78/150], Batch loss: 1.354
Epoch: 78 RMSE:  1.3109026651790405  MAPE: 0.11350236624885683  L2+L1 loss: 1.042
Epoch [79/150], Batch loss: 1.317
Epoch: 79 RMSE:  1.1788940587312098  MAPE: 0.07652767597286471  L2+L1 loss: 1.018
Epoch [80/150], Batch loss: 1.347
Epoch: 80 RMSE:  1.4969038428295582  MAPE: 0.06843668471954467  L2+L1 loss: 1.112
Epoch [81/150], Batch loss: 1.28
Epoch: 81 RMSE:  1.1441092397390864  MAPE: 0.06764288159172793  L2+L1 loss: 0.974
Epoch [82/150], Batch loss: 1.378
Epoch: 82 RMSE:  1.1363472036221174  MAPE: 0.07141218751006369  L2+L1 loss: 1.015
Epoch [83/150], Batch loss: 1.328
Epoch: 83 RMSE:  1.1711864153864122  MAPE: 0.13559154272709797  L2+L1 loss: 1.065
Epoch [84/150], Batch loss: 1.395
Epoch: 84 RMSE:  1.1427839069655943  MAPE: 0.07092967287415401  L2+L1 loss: 1.018
Epoch [85/150], Batch loss: 1.511
Epoch: 85 RMSE:  1.6983944940520161  MAPE: 0.08138637351229912  L2+L1 loss: 1.181
Epoch [86/150], Batch loss: 1.334
Epoch: 86 RMSE:  1.148338279183439  MAPE: 0.06804735757017573  L2+L1 loss: 1.013
Epoch [87/150], Batch loss: 1.273
Epoch: 87 RMSE:  1.8130088253897312  MAPE: 0.08076523803385867  L2+L1 loss: 1.146
Epoch [88/150], Batch loss: 1.296
Epoch: 88 RMSE:  1.8643493161738192  MAPE: 0.1914884556242681  L2+L1 loss: 1.262
Epoch [89/150], Batch loss: 1.3
Epoch: 89 RMSE:  1.145078583354768  MAPE: 0.09203268660562818  L2+L1 loss: 1.017
Epoch [90/150], Batch loss: 1.199
Epoch: 90 RMSE:  1.1958286070771018  MAPE: 0.07295203628745209  L2+L1 loss: 1.025
Epoch [91/150], Batch loss: 1.2
Epoch: 91 RMSE:  1.2093715431182839  MAPE: 0.06501133011020639  L2+L1 loss: 1.019
Epoch [92/150], Batch loss: 1.175
Epoch: 92 RMSE:  1.1133233076161926  MAPE: 0.06334286706606482  L2+L1 loss: 0.977
Epoch [93/150], Batch loss: 1.188
Epoch: 93 RMSE:  1.160016860942097  MAPE: 0.07783322502979823  L2+L1 loss: 1.011
Epoch [94/150], Batch loss: 1.18
Epoch: 94 RMSE:  1.1910669811745638  MAPE: 0.08134701872410642  L2+L1 loss: 1.024
Epoch [95/150], Batch loss: 1.178
Epoch: 95 RMSE:  1.2869624745154375  MAPE: 0.06630451867358601  L2+L1 loss: 1.044
Epoch [96/150], Batch loss: 1.188
Epoch: 96 RMSE:  1.1786776916900632  MAPE: 0.06570214630584517  L2+L1 loss: 1.007
Epoch [97/150], Batch loss: 1.164
Epoch: 97 RMSE:  1.1411919128661259  MAPE: 0.06279089272573532  L2+L1 loss: 0.99
Epoch [98/150], Batch loss: 1.179
Epoch: 98 RMSE:  1.1679095455755617  MAPE: 0.0635094443932555  L2+L1 loss: 1.006
Epoch [99/150], Batch loss: 1.189
Epoch: 99 RMSE:  1.132611008231725  MAPE: 0.06198579961414958  L2+L1 loss: 0.987
Epoch [100/150], Batch loss: 1.181
Epoch: 100 RMSE:  1.183094587636516  MAPE: 0.0626730546695864  L2+L1 loss: 1.006
Epoch [101/150], Batch loss: 1.185
Epoch: 101 RMSE:  1.3584950683313055  MAPE: 0.07231715195909627  L2+L1 loss: 1.071
Epoch [102/150], Batch loss: 1.185
Epoch: 102 RMSE:  1.1300513838648651  MAPE: 0.06470731421972523  L2+L1 loss: 0.983
Epoch [103/150], Batch loss: 1.187
Epoch: 103 RMSE:  1.2721717021689736  MAPE: 0.061184999430967876  L2+L1 loss: 1.029
Epoch [104/150], Batch loss: 1.179
Epoch: 104 RMSE:  1.1698528996429467  MAPE: 0.06061535173255053  L2+L1 loss: 0.998
Epoch [105/150], Batch loss: 1.181
Epoch: 105 RMSE:  1.1372904190148712  MAPE: 0.0711191594733379  L2+L1 loss: 0.993
Epoch [106/150], Batch loss: 1.172
Epoch: 106 RMSE:  1.1309941704973872  MAPE: 0.06510504616193835  L2+L1 loss: 0.98
Epoch [107/150], Batch loss: 1.179
Epoch: 107 RMSE:  1.2320266145549337  MAPE: 0.0896861732736631  L2+L1 loss: 1.044
Epoch [108/150], Batch loss: 1.162
Epoch: 108 RMSE:  1.2919851760551502  MAPE: 0.06980068762061234  L2+L1 loss: 1.055
Epoch [109/150], Batch loss: 1.177
Epoch: 109 RMSE:  1.1728421748770324  MAPE: 0.07607279429938411  L2+L1 loss: 1.015
Epoch [110/150], Batch loss: 1.167
Epoch: 110 RMSE:  1.2734605316382681  MAPE: 0.0657439509482184  L2+L1 loss: 1.045
Epoch [111/150], Batch loss: 1.175
Epoch: 111 RMSE:  1.179077796452585  MAPE: 0.07432452021388447  L2+L1 loss: 1.021
Epoch [112/150], Batch loss: 1.167
Epoch: 112 RMSE:  1.1387082380137679  MAPE: 0.0609946675741434  L2+L1 loss: 0.986
Epoch [113/150], Batch loss: 1.162
Epoch: 113 RMSE:  1.2120698536012846  MAPE: 0.062024587558920125  L2+L1 loss: 1.016
Epoch [114/150], Batch loss: 1.161
Epoch: 114 RMSE:  1.2155165471262737  MAPE: 0.06024492398596998  L2+L1 loss: 1.013
Epoch [115/150], Batch loss: 1.153
Epoch: 115 RMSE:  1.3135279292140218  MAPE: 0.061599259140447934  L2+L1 loss: 1.05
Epoch [116/150], Batch loss: 1.167
Epoch: 116 RMSE:  1.2429229938494306  MAPE: 0.07251025170529403  L2+L1 loss: 1.027
Epoch [117/150], Batch loss: 1.175
Epoch: 117 RMSE:  1.294564280352665  MAPE: 0.06030514579758477  L2+L1 loss: 1.046
Epoch [118/150], Batch loss: 1.159
Epoch: 118 RMSE:  1.1893934165012763  MAPE: 0.061342285267988995  L2+L1 loss: 1.004
Epoch [119/150], Batch loss: 1.171
Epoch: 119 RMSE:  1.2270082380141982  MAPE: 0.06738571952978299  L2+L1 loss: 1.02
Epoch [120/150], Batch loss: 1.168
Epoch: 120 RMSE:  1.181878769628539  MAPE: 0.062378867585982  L2+L1 loss: 1.006
Epoch [121/150], Batch loss: 1.152
Epoch: 121 RMSE:  1.1722166603503152  MAPE: 0.05886563492743373  L2+L1 loss: 0.997
Epoch [122/150], Batch loss: 1.153
Epoch: 122 RMSE:  1.1716569164911033  MAPE: 0.05897815896601981  L2+L1 loss: 0.997
Epoch [123/150], Batch loss: 1.159
Epoch: 123 RMSE:  1.1602665182814147  MAPE: 0.058595556614578374  L2+L1 loss: 0.991
Epoch [124/150], Batch loss: 1.145
Epoch: 124 RMSE:  1.1592908117536693  MAPE: 0.05974823237778793  L2+L1 loss: 0.992
Epoch [125/150], Batch loss: 1.159
Epoch: 125 RMSE:  1.1712312424299594  MAPE: 0.05839266111313122  L2+L1 loss: 0.995
Epoch [126/150], Batch loss: 1.16
Epoch: 126 RMSE:  1.1663268358951777  MAPE: 0.05886295175152686  L2+L1 loss: 0.994
Epoch [127/150], Batch loss: 1.153
Epoch: 127 RMSE:  1.1816293385831693  MAPE: 0.05835288272586335  L2+L1 loss: 1.0
Epoch [128/150], Batch loss: 1.162
Epoch: 128 RMSE:  1.1623984971176695  MAPE: 0.0582495914910634  L2+L1 loss: 0.992
Epoch [129/150], Batch loss: 1.148
Epoch: 129 RMSE:  1.1604659567733446  MAPE: 0.05834294598275678  L2+L1 loss: 0.991
Epoch [130/150], Batch loss: 1.146
Epoch: 130 RMSE:  1.1600139467676194  MAPE: 0.05831278222847314  L2+L1 loss: 0.991
Epoch [131/150], Batch loss: 1.154
Epoch: 131 RMSE:  1.1719210771675639  MAPE: 0.05872995936993563  L2+L1 loss: 0.996
Epoch [132/150], Batch loss: 1.16
Epoch: 132 RMSE:  1.1646608879798674  MAPE: 0.05863366468517051  L2+L1 loss: 0.993
Epoch [133/150], Batch loss: 1.156
Epoch: 133 RMSE:  1.1642732073386313  MAPE: 0.05977767162494372  L2+L1 loss: 0.995
Epoch [134/150], Batch loss: 1.156
Epoch: 134 RMSE:  1.1653332254271838  MAPE: 0.05874571570219598  L2+L1 loss: 0.993
Epoch [135/150], Batch loss: 1.156
Epoch: 135 RMSE:  1.1742941273083811  MAPE: 0.058486458165239376  L2+L1 loss: 0.996
Epoch [136/150], Batch loss: 1.164
Epoch: 136 RMSE:  1.1668697634193717  MAPE: 0.058532151094778086  L2+L1 loss: 0.994
Epoch [137/150], Batch loss: 1.144
Epoch: 137 RMSE:  1.1698651377873879  MAPE: 0.05816974278461567  L2+L1 loss: 0.995
Epoch [138/150], Batch loss: 1.149
Epoch: 138 RMSE:  1.160668540206087  MAPE: 0.05828219334165878  L2+L1 loss: 0.99
Epoch [139/150], Batch loss: 1.166
Epoch: 139 RMSE:  1.160917207459607  MAPE: 0.060284631284811256  L2+L1 loss: 0.992
Epoch [140/150], Batch loss: 1.148
Epoch: 140 RMSE:  1.1761984470640046  MAPE: 0.05819199151431026  L2+L1 loss: 0.997
Epoch [141/150], Batch loss: 1.141
Epoch: 141 RMSE:  1.1771547868703016  MAPE: 0.05892458983099228  L2+L1 loss: 0.998
Epoch [142/150], Batch loss: 1.146
Epoch: 142 RMSE:  1.1704389260123917  MAPE: 0.05844867529673133  L2+L1 loss: 0.995
Epoch [143/150], Batch loss: 1.147
Epoch: 143 RMSE:  1.1658236683217174  MAPE: 0.05856117046574572  L2+L1 loss: 0.994
Epoch [144/150], Batch loss: 1.154
Epoch: 144 RMSE:  1.1616073448612219  MAPE: 0.05810770208181033  L2+L1 loss: 0.991
Epoch [145/150], Batch loss: 1.158
Epoch: 145 RMSE:  1.1687834360609302  MAPE: 0.05878378198087311  L2+L1 loss: 0.996
Epoch [146/150], Batch loss: 1.142
Epoch: 146 RMSE:  1.1737264433344297  MAPE: 0.058217171349281234  L2+L1 loss: 0.996
Epoch [147/150], Batch loss: 1.155
Epoch: 147 RMSE:  1.169436642271117  MAPE: 0.058041096358242464  L2+L1 loss: 0.994
Epoch [148/150], Batch loss: 1.148
Epoch: 148 RMSE:  1.1706995291418054  MAPE: 0.05807684524485971  L2+L1 loss: 0.995
Epoch [149/150], Batch loss: 1.156
Epoch: 149 RMSE:  1.157637110231391  MAPE: 0.0581223655376633  L2+L1 loss: 0.989


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
42.762844 , 38.3628
37.021973 , 35.7662
2.5073404 , 2.3342
5.7803936 , 6.2987
45.94339 , 44.2047
14.9309 , 14.7707
8.340694 , 9.0972
49.94037 , 48.5873
23.37234 , 23.7019
155.21053 , 151.7387
25.226843 , 24.7684
0.29084015 , 0.2654
6.872202 , 7.4098
0.29785538 , 0.2872
7.4421763 , 7.9104
124.49505 , 123.8493
0.28208256 , 0.2551
115.15735 , 113.5576
19.161827 , 17.741
26.74985 , 26.9464
8.4175825 , 8.0919
0.63783836 , 0.6276
108.778854 , 106.9881
4.1792083 , 1.260477
33.449745 , 31.5653
29.25222 , 33.5429
0.82599735 , 0.8362
1.2374763 , 1.2218
7.1882358 , 7.0233
6.927862 , 6.7153
0.27588463 , 0.2793
109.83167 , 107.1644
0.07575512 , 0.1241
51.411724 , 49.9912
6.2927423 , 6.6805
8.554203 , 9.0275
11.264668 , 12.0438
6.789789 , 7.2113
22.157183 , 20.9077
7.701767 , 7.4394
10.118212 , 9.5344
3.7085328 , 3.5998
45.750523 , 42.004
9.214766 , 9.6484
25.3255 , 24.9922
14.292218 , 16.736
37.514923 , 36.5048
12.199683 , 11.5346
105.70692 , 103.9804
19.74258 , 19.6615
9.199179 , 9.8436
0.38989162 , 0.2756
26.988976 , 27.0771
8.630102 , 8.3098
0.44622135 , 0.3805
0.5143204 , 0.4738
0.10266495 , 0.1332
0.65266514 , 0.5872
6.291848 , 6.0012
3.8923826 , 3.7402
56.145103 , 59.1525
0.94677734 , 0.9636
8.056829 , 7.7848
14.19421 , 16.6194
23.47876 , 23.3418
22.8215 , 21.5826
18.379658 , 20.8535
7.091302 , 6.9425
5.453655 , 5.9743
20.36219 , 20.7134
47.188778 , 48.6982
1.7361326 , 1.7109
21.00342 , 20.2461
3.8824415 , 4.1954
26.035534 , 26.1773
27.438274 , 27.5252
10.294126 , 11.4665
23.937695 , 23.2245
32.20258 , 35.6393
0.8156576 , 0.8068
5.9350686 , 6.4981
53.169754 , 49.1498
6.2037373 , 5.9478
22.941502 , 21.9917
14.2811 , 13.0604
5.6120396 , 6.0227
17.102034 , 16.3357
27.439232 , 30.8901
0.6383543 , 0.587
8.143634 , 7.6976
9.001988 , 8.5862
6.9123 , 6.6063
24.81883 , 27.2151
7.1007676 , 7.5619
57.47813 , 58.0819
36.46758 , 37.423
22.920992 , 24.0057
18.26915 , 19.8812
23.62587 , 23.974
7.973073 , 7.7334
6.1326456 , 5.9049
21.84845 , 21.3346
48.64979 , 45.3726
31.107327 , 30.9112
20.848824 , 21.1151
5.244987 , 5.0649
49.52427 , 47.4598
4.637727 , 4.4636
13.988195 , 12.5977
9.173587 , 9.9055
1.5463247 , 1.5991
13.576879 , 12.3963
5.2306194 , 5.3659
8.441865 , 8.8908
41.040806 , 36.3443
27.649914 , 30.9296
50.219116 , 48.9162
22.604939 , 21.9218
0.53154564 , 0.5307
9.4575405 , 9.1294
32.58865 , 35.6265
7.8110304 , 8.3651
21.203054 , 20.0737
14.496294 , 14.6678
5.6985455 , 6.3201
33.68382 , 35.6792
42.58683 , 43.2605
25.660917 , 24.3901
16.347126 , 15.2889
5.088715 , 5.04
24.620657 , 23.2657
14.349899 , 16.6429
24.009647 , 24.3004
7.25447 , 7.7784
15.592687 , 17.2815
10.626272 , 10.3814
47.207344 , 50.2946
6.71547 , 6.3812
2.1536531 , 2.2053
47.074146 , 42.1239
4.4999347 , 4.3199
22.132208 , 20.9335
33.127228 , 35.811
5.8189163 , 6.2017
6.041279 , 5.5767
38.999676 , 37.1992
33.19491 , 33.6769
49.832047 , 50.8865
24.490036 , 21.9688
11.001419 , 10.3687
1.7623644 , 1.8609
24.332214 , 23.9524
52.028435 , 51.0403
10.48258 , 10.7465
12.822779 , 13.3502
1.8027506 , 1.8183
23.238953 , 26.002
13.506326 , 14.5378
9.6331415 , 10.0384
15.728718 , 15.1584
1.09373 , 1.2831
1.4187708 , 1.3704
9.169211 , 8.7995
5.2723584 , 5.2468
10.7968855 , 10.185
0.9569092 , 0.9282
0.94049263 , 0.9133
1.2620134 , 1.1486
16.119165 , 14.7131
11.152731 , 10.287
10.071229 , 11.3294
7.2868433 , 7.1081
8.201862 , 8.7503
61.88178 , 58.4527
10.292566 , 9.6461
0.5909996 , 0.5571
27.323235 , 27.0852
54.564026 , 55.2325
0.9389391 , 0.9819
1.0964069 , 1.2145
7.680665 , 8.124
7.862726 , 7.736
48.409275 , 44.7839
12.737255 , 15.4025
13.068374 , 12.2183
1.5199871 , 1.4629
4.903593 , 4.6728
26.307362 , 25.2415
24.975685 , 24.3646
6.4173784 , 7.0138
6.6066875 , 7.1494
112.043564 , 112.9185
4.541827 , 4.4947
0.5253773 , 0.4444
2.0718822 , 2.0558
7.073217 , 7.4403
22.83818 , 21.7034
5.97611 , 5.8728
19.350315 , 18.5841
7.707056 , 7.6558
0.9530268 , 0.8839
0.36693478 , 0.3311
250.03743 , 247.6741
121.10772 , 117.6756
0.62709045 , 0.7042
12.52526 , 13.1771
24.079294 , 23.1548
11.6526375 , 11.4784
5.7652717 , 5.4356
27.283752 , 30.611
5.943824 , 6.492
28.270164 , 29.1971
54.94057 , 52.003
0.5742941 , 0.5481
4.485762 , 4.2849
4.06308 , 4.3839
9.462013 , 9.2201
0.6256981 , 0.5679
8.390981 , 8.1455
18.06045 , 17.8934
0.6753111 , 0.6453
1.6265049 , 1.6691
9.391054 , 9.9667
22.050446 , 20.8584
23.85257 , 24.0109
0.6408167 , 0.6963
7.467724 , 7.2643
18.171452 , 18.2105
11.102921 , 11.4148
44.92686 , 42.1079
6.620597 , 6.3124
13.788883 , 12.7356
251.75298 , 246.393
43.80555 , 38.9111
0.60212994 , 0.596
5.466572 , 5.9161
0.7090769 , 0.5888
12.979206 , 12.2378
0.47259617 , 0.4979
18.49413 , 19.0191
24.932835 , 23.6844
0.3055439 , 0.2615
59.732517 , 56.8805
23.49188 , 23.1345
19.353828 , 18.3269
10.933711 , 11.4909
6.894218 , 7.329
12.262593 , 10.9715
0.52854156 , 0.5202
24.869757 , 26.11
5.0907497 , 5.5885
1.4075403 , 1.4283
27.140308 , 26.524
8.530667 , 9.2376
19.693886 , 18.5241
11.482815 , 12.1878
1.387311 , 1.4646
48.806557 , 46.5667
6.7518473 , 6.474
6.6268053 , 7.0445
26.382027 , 27.5912
0.89774895 , 0.8749
7.1443834 , 6.8306
5.254334 , 5.8329
23.563251 , 22.9978
0.51098347 , 0.5279
0.325737 , 0.2477
0.5778847 , 0.5861
0.21966076 , 0.2202
40.697327 , 36.9076
7.361461 , 7.8505
7.8820324 , 7.8252
6.38492 , 6.1207
0.9172754 , 0.9191
25.974628 , 26.7897
1.3189564 , 1.3672
0.38338852 , 0.3705
25.169827 , 25.5547
9.734917 , 10.1878
14.460391 , 14.3968
0.5933933 , 0.5773
1.9829664 , 2.0053
0.14326477 , 0.1563
7.727403 , 7.316
11.034679 , 10.3898
43.091465 , 42.1463
0.6645851 , 0.6578
2.1255293 , 2.0887
30.70353 , 30.0212
28.749386 , 28.5942
34.707653 , 37.8497
24.617054 , 23.9209
5.9142365 , 6.3298
6.4779677 , 6.1932
3.3281646 , 3.4006
7.499283 , 7.994
16.601131 , 15.3147
22.817654 , 21.8035
118.67589 , 118.2395
6.5002847 , 6.2881
1.0854816 , 1.139
8.0031395 , 8.5203
11.506865 , 12.5156
5.4099693 , 5.7481
20.61247 , 21.9339
15.167317 , 15.1258
106.688324 , 105.0823
284.22006 , 285.1027
7.2860527 , 6.8504
15.86408 , 16.6034
6.656877 , 6.5018
50.45951 , 48.0
26.065012 , 24.8992
119.30464 , 116.1073
23.901676 , 26.8065
8.952142 , 9.4574
26.534172 , 27.6303
13.325119 , 12.5822
10.672934 , 10.5193
9.119243 , 8.8335
51.191536 , 49.5902
6.277197 , 6.047
131.85149 , 130.0792
0.19943237 , 0.1991
21.804054 , 20.9592
6.7194233 , 6.38
31.132454 , 29.6946
5.8447757 , 6.513
1.0540257 , 1.0866
13.520641 , 14.0082
6.2124877 , 6.5943
0.4291029 , 0.4182
7.395858 , 7.2689
0.70169353 , 0.677
48.944885 , 44.1933
16.31126 , 15.3701
23.10197 , 23.3276
11.214318 , 11.0535
1.4847641 , 1.4747
8.944983 , 8.7845
0.35058403 , 0.298
18.27002 , 16.6514
0.30593204 , 0.2875
0.4734831 , 0.427
24.884912 , 27.9182
5.8813 , 6.3632
12.52723 , 14.0004
6.4790344 , 6.167
25.467749 , 24.2818
7.201459 , 7.821
5.248887 , 5.0928
22.185247 , 20.9359
2.971933 , 3.1975
1.4473886 , 1.5037
21.375214 , 23.5645
0.6138859 , 0.5823
23.656143 , 25.5762
5.2870846 , 5.0992
24.99812 , 23.4815
4.9973288 , 5.4168
7.9181995 , 7.5965
5.663852 , 6.0462
4.8857164 , 5.3792
5.6677175 , 5.5928
28.617832 , 27.8479
4.6302495 , 4.4566
40.108044 , 36.7105
12.305975 , 11.6122
28.04219 , 26.9587
5.1025915 , 4.8358
12.297834 , 11.7775
7.4547386 , 7.2651
5.2784505 , 5.0903
63.16388 , 57.9944
54.570007 , 50.4055
25.35006 , 24.4508
2.1295938 , 2.1985
41.54882 , 37.7304
6.8626575 , 6.5559
5.4543 , 5.9005
13.928109 , 13.9819
7.437025 , 7.1314
17.883865 , 19.1833
4.658969 , 4.9685
10.035852 , 10.4909
6.46137 , 6.3236
10.730606 , 11.2161
5.060893 , 4.8917
24.34646 , 23.97
0.260108 , 0.2289
16.700644 , 15.1693
0.57191277 , 0.5539
9.714547 , 10.3379
5.2962875 , 5.6248
13.383703 , 13.2088
140.36024 , 138.8376
21.756268 , 19.4974
1.1425953 , 1.1983
14.3675 , 13.701
6.9703507 , 7.2963
0.22066116 , 0.2821
0.1481905 , 0.1283
19.416649 , 18.7083
0.5281248 , 0.4995
2.332981 , 2.2606
22.21962 , 24.796
11.360194 , 12.3439
62.445282 , 67.1718
1.5710688 , 1.7204
17.773645 , 16.9493
22.732176 , 21.7444
11.923262 , 12.7497
6.120686 , 6.4902
9.918814 , 11.4524
6.1356792 , 6.5692
8.492266 , 8.1915
45.097664 , 43.125
31.779156 , 34.9449
0.901577 , 0.9658
5.5198174 , 6.0533
7.3259506 , 7.0598
6.4311194 , 7.0086
12.908648 , 13.3105
29.004646 , 27.4131
34.817173 , 32.1735
57.01474 , 57.8777
13.639285 , 12.4106
35.06841 , 35.5275
6.4456625 , 7.0406
7.710104 , 8.0952
55.991146 , 56.6679
4.1792083 , 5.334602
13.146034 , 12.8144
23.0406 , 22.1034
0.20428944 , 0.2872
22.182983 , 22.4958
4.530824 , 4.8038
7.9363775 , 8.5479
21.36263 , 22.4799
7.561146 , 7.2552
3.323481 , 3.3514
2.3589344 , 2.361
42.28083 , 38.9943
6.799247 , 7.2424
5.483095 , 5.283
8.729135 , 9.3095
43.61842 , 44.1991
24.358889 , 24.0994
39.82935 , 40.1541
22.393085 , 21.782
0.61661816 , 0.611
6.84718 , 6.5173
17.554745 , 16.9755
1.8031654 , 1.8164
0.43250847 , 0.3792
7.6966524 , 8.0118
10.7198 , 10.1257
48.07949 , 50.4707
10.045989 , 10.4711
25.84852 , 29.0599
6.048316 , 5.6265
0.20418072 , 0.187
12.982563 , 12.87
22.717928 , 22.0534
49.716396 , 51.174
29.557693 , 30.13
25.70811 , 24.4232
39.77946 , 39.0849
4.4541616 , 4.7698
30.630173 , 30.5207
24.439468 , 26.9055
0.77543926 , 0.7314
8.923616 , 8.5514
29.200527 , 29.0011
5.1551967 , 5.563
0.774745 , 0.738
0.7443409 , 0.7334
0.23753738 , 0.2447
3.9454932 , 3.7663
7.4353466 , 7.8925
235.7092 , 232.3166
1.0886831 , 1.0975
0.18047333 , 0.1857
5.235234 , 5.6767
8.22517 , 8.7206
5.299669 , 5.0243
14.038298 , 14.0031
18.698269 , 18.2755
23.260517 , 22.2235
42.572765 , 40.9847
1.697464 , 1.7504
49.24793 , 48.7818
7.1989393 , 7.6169
6.661095 , 6.5457
0.33389854 , 0.2976
8.267818 , 7.6396
30.946453 , 29.7459
6.3717923 , 5.9952
0.83067226 , 0.8
7.7131767 , 8.1919
27.526718 , 27.7922
15.51227 , 15.7636
1.1773062 , 1.2234
0.22180653 , 0.192
7.222698 , 6.9189
6.1954637 , 6.6165
4.783314 , 4.8176
28.675777 , 30.7995
1.3345594 , 1.3892
19.676361 , 18.8619
55.850395 , 51.0727
0.6386223 , 0.5492
24.638783 , 27.3895
5.8156414 , 5.7518
0.25656605 , 0.2281
12.314301 , 11.6448
11.394429 , 10.7552
7.9675026 , 7.7674
0.5041914 , 0.4877
19.137787 , 17.7208
1.429842 , 1.4372
10.189836 , 10.1114
5.873712 , 5.6269
5.1134505 , 4.9387
0.9978962 , 1.0006
13.141939 , 12.1518
7.8441734 , 8.392
6.8717375 , 7.2559
14.255043 , 12.6414
1.0800476 , 1.1665
7.1095953 , 6.7397
16.607157 , 16.8933
34.557873 , 37.0987
0.74847317 , 0.7307
8.878065 , 8.5874
0.21792221 , 0.2095
31.461342 , 34.7377
48.146683 , 46.7556
10.818696 , 10.0222
9.163278 , 9.6527
6.710223 , 6.3589
0.7799034 , 0.778
24.759716 , 22.6
6.6694846 , 7.2164
2.1491652 , 2.135
18.92316 , 19.1269
44.149727 , 44.3655
5.53704 , 6.083
6.8851123 , 6.5771
5.9592934 , 5.7563
0.5002947 , 0.476
11.097935 , 11.5908
20.558943 , 20.4755
51.06987 , 52.0959
0.38519478 , 0.3943
0.09076786 , 0.1387
21.2454 , 23.4274
29.964344 , 33.0543
1.128994 , 1.174
8.512301 , 9.0187
49.503723 , 50.7677
24.38998 , 23.5483
1.2382135 , 1.3083
24.58843 , 23.1168
8.585876 , 8.3609
24.798664 , 23.7591
1.6613493 , 1.7168
10.522524 , 10.4981
0.25119686 , 0.2479
7.9628334 , 7.4496
1.2452173 , 1.33
38.650208 , 37.2204
7.190851 , 6.8451
11.190925 , 11.8164
0.8815479 , 0.8925
12.844709 , 13.4375
7.4731507 , 7.9394
10.151924 , 9.7133
25.651192 , 25.8936
41.564304 , 37.722
0.10167217 , 0.129
8.084782 , 7.9294
20.140938 , 20.8002
10.247505 , 9.8154
6.832711 , 7.3019
4.1792083 , 2.15169
0.25815105 , 0.2672
4.866606 , 4.6251
0.17743015 , 0.1608
6.129751 , 6.4739
5.306809 , 5.6401
11.212647 , 12.0049
25.226612 , 22.3208
2.0158367 , 2.0459
9.593057 , 9.1562
22.94287 , 23.8768
1.6066952 , 1.7502
8.247253 , 7.9409
13.405289 , 13.1649
9.158642 , 8.7805
7.9638996 , 8.6734
11.995007 , 11.3083
3.1624265 , 3.5232
10.076696 , 10.7548
9.05894 , 9.639
0.622633 , 0.5905
29.45126 , 26.3573
9.263827 , 8.8253
4.7778087 , 5.2405
0.5585823 , 0.5113
8.850976 , 8.742
54.731953 , 58.7516
27.59956 , 30.2767
7.006733 , 6.6813
6.207714 , 6.0367
22.62503 , 22.8771
9.28812 , 9.9301
42.216297 , 43.2874
12.962479 , 13.449
34.434566 , 34.2801
0.34634972 , 0.3012
0.2662325 , 0.2154
23.991776 , 24.5305
23.494322 , 24.8597
98.16102 , 97.8793
9.377081 , 8.9075
6.572332 , 6.4337
7.144638 , 6.8138
0.80567265 , 0.7932
7.843485 , 8.1281
0.3514471 , 0.3506
32.14713 , 32.5123
7.2522163 , 7.0188
0.43504524 , 0.388
26.26131 , 25.9002
7.8070254 , 8.3835
2.121603 , 2.2243
0.7742634 , 0.7735
2.715211 , 2.6348
RMSE:  1.2700488018041385  MAPE: 0.0596778381036648
5: ground truth total-  160  predicted total -  158
100: ground truth total-  462  predicted total -  462
 more 100: ground truth total -  17  predicted total -  17
