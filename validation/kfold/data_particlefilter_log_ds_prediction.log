['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
291
34
<class 'numpy.dtype'> float64
291
train batches:  53  validate samples: 23  test samples: 58
Epoch [0/75], Batch loss: 63.006
Epoch: 0 RMSE:  1.8433544775213806  MAPE: 1.1976647365196114  L2+L1 loss: 2.265
Epoch [1/75], Batch loss: 1.367
Epoch: 1 RMSE:  0.7782421195000747  MAPE: 0.46368641802891636  L2+L1 loss: 1.118
Epoch [2/75], Batch loss: 0.605
Epoch: 2 RMSE:  0.30524525698337057  MAPE: 0.15043946565720304  L2+L1 loss: 0.529
Epoch [3/75], Batch loss: 0.415
Epoch: 3 RMSE:  0.267543341789678  MAPE: 0.1383265067591953  L2+L1 loss: 0.457
Epoch [4/75], Batch loss: 0.409
Epoch: 4 RMSE:  0.27071445248346815  MAPE: 0.14293320887051858  L2+L1 loss: 0.463
Epoch [5/75], Batch loss: 0.411
Epoch: 5 RMSE:  0.26977308764910796  MAPE: 0.14189111621895517  L2+L1 loss: 0.463
Epoch [6/75], Batch loss: 0.402
Epoch: 6 RMSE:  0.2879104618834915  MAPE: 0.15858474634434622  L2+L1 loss: 0.477
Epoch [7/75], Batch loss: 0.474
Epoch: 7 RMSE:  0.2774603113207191  MAPE: 0.1392378167061004  L2+L1 loss: 0.488
Epoch [8/75], Batch loss: 0.405
Epoch: 8 RMSE:  5.578750306330285  MAPE: 3.476696009552365  L2+L1 loss: 5.822
Epoch [9/75], Batch loss: 2.934
Epoch: 9 RMSE:  0.33467263732299135  MAPE: 0.20220937422586005  L2+L1 loss: 0.572
Epoch [10/75], Batch loss: 0.396
Epoch: 10 RMSE:  0.27124915975534725  MAPE: 0.14349049325051597  L2+L1 loss: 0.462
Epoch [11/75], Batch loss: 0.406
Epoch: 11 RMSE:  0.270244204430592  MAPE: 0.14243053093940533  L2+L1 loss: 0.463
Epoch [12/75], Batch loss: 0.457
Epoch: 12 RMSE:  0.28270240799223195  MAPE: 0.15440177596677507  L2+L1 loss: 0.477
Epoch [13/75], Batch loss: 0.413
Epoch: 13 RMSE:  0.31278558075414137  MAPE: 0.18301287859067175  L2+L1 loss: 0.534
Epoch [14/75], Batch loss: 0.413
Epoch: 14 RMSE:  0.29730193151403  MAPE: 0.16790792448910466  L2+L1 loss: 0.5
Epoch [15/75], Batch loss: 0.414
Epoch: 15 RMSE:  0.31900003511925384  MAPE: 0.18875948717804225  L2+L1 loss: 0.547
Epoch [16/75], Batch loss: 0.42
Epoch: 16 RMSE:  0.2675161567043532  MAPE: 0.13825921475207564  L2+L1 loss: 0.456
Epoch [17/75], Batch loss: 0.424
Epoch: 17 RMSE:  0.34895308475191184  MAPE: 0.2135225308409729  L2+L1 loss: 0.59
Epoch [18/75], Batch loss: 0.426
Epoch: 18 RMSE:  0.2676627608864555  MAPE: 0.13625717371850724  L2+L1 loss: 0.465
Epoch [19/75], Batch loss: 0.405
Epoch: 19 RMSE:  0.30739039268093876  MAPE: 0.1512036905362108  L2+L1 loss: 0.531
Epoch [20/75], Batch loss: 0.43
Epoch: 20 RMSE:  0.2669113419857134  MAPE: 0.1363182802553372  L2+L1 loss: 0.459
Epoch [21/75], Batch loss: 0.432
Epoch: 21 RMSE:  0.289442683833494  MAPE: 0.15991087130560927  L2+L1 loss: 0.479
Epoch [22/75], Batch loss: 0.43
Epoch: 22 RMSE:  0.2732914580270685  MAPE: 0.1458631699870685  L2+L1 loss: 0.468
Epoch [23/75], Batch loss: 0.959
Epoch: 23 RMSE:  0.5337893831552107  MAPE: 0.28225361451177866  L2+L1 loss: 0.801
Epoch [24/75], Batch loss: 2.123
Epoch: 24 RMSE:  0.4201263994467168  MAPE: 0.2675928419997503  L2+L1 loss: 0.685
Epoch [25/75], Batch loss: 0.413
Epoch: 25 RMSE:  0.26806335403154513  MAPE: 0.13942081087250968  L2+L1 loss: 0.459
Epoch [26/75], Batch loss: 0.415
Epoch: 26 RMSE:  0.2782372759333295  MAPE: 0.15040712443614915  L2+L1 loss: 0.471
Epoch [27/75], Batch loss: 0.53
Epoch: 27 RMSE:  1.2887088389989327  MAPE: 0.5790018951902622  L2+L1 loss: 1.24
Epoch [28/75], Batch loss: 0.474
Epoch: 28 RMSE:  0.26962056497929504  MAPE: 0.1362061949868897  L2+L1 loss: 0.467
Epoch [29/75], Batch loss: 0.415
Epoch: 29 RMSE:  0.2669468956935398  MAPE: 0.13638379948925713  L2+L1 loss: 0.448
Epoch [30/75], Batch loss: 0.409
Epoch: 30 RMSE:  0.2725426919625241  MAPE: 0.14504289737096795  L2+L1 loss: 0.466
Epoch [31/75], Batch loss: 0.402
Epoch: 31 RMSE:  0.28638896290974225  MAPE: 0.15741655100061286  L2+L1 loss: 0.478
Epoch [32/75], Batch loss: 0.42
Epoch: 32 RMSE:  0.27357664487587313  MAPE: 0.14616334171740644  L2+L1 loss: 0.468
Epoch [33/75], Batch loss: 0.405
Epoch: 33 RMSE:  0.26983057670691757  MAPE: 0.14195913424957954  L2+L1 loss: 0.463
Epoch [34/75], Batch loss: 0.408
Epoch: 34 RMSE:  0.27230523351361857  MAPE: 0.14477194202774932  L2+L1 loss: 0.466
Epoch [35/75], Batch loss: 0.408
Epoch: 35 RMSE:  0.2691744973769545  MAPE: 0.14113910107280536  L2+L1 loss: 0.462
Epoch [36/75], Batch loss: 0.403
Epoch: 36 RMSE:  0.2933787933323917  MAPE: 0.16385284504814132  L2+L1 loss: 0.489
Epoch [37/75], Batch loss: 0.409
Epoch: 37 RMSE:  0.30263509857584797  MAPE: 0.1730804021913849  L2+L1 loss: 0.51
Epoch [38/75], Batch loss: 0.409
Epoch: 38 RMSE:  0.28802109045342317  MAPE: 0.1586681888688986  L2+L1 loss: 0.477
Epoch [39/75], Batch loss: 0.405
Epoch: 39 RMSE:  0.26687689720451335  MAPE: 0.13634382661954958  L2+L1 loss: 0.454
Epoch [40/75], Batch loss: 0.415
Epoch: 40 RMSE:  0.2669702282360823  MAPE: 0.13630832082747937  L2+L1 loss: 0.46
Epoch [41/75], Batch loss: 0.412
Epoch: 41 RMSE:  0.27523405869246703  MAPE: 0.14779820218953188  L2+L1 loss: 0.47
Epoch [42/75], Batch loss: 0.419
Epoch: 42 RMSE:  0.271190010917457  MAPE: 0.14341448691508207  L2+L1 loss: 0.461
Epoch [43/75], Batch loss: 0.413
Epoch: 43 RMSE:  0.27353802387496906  MAPE: 0.14612305644648604  L2+L1 loss: 0.468
Epoch [44/75], Batch loss: 0.408
Epoch: 44 RMSE:  0.28445785491546255  MAPE: 0.15587310077318392  L2+L1 loss: 0.478
Epoch [45/75], Batch loss: 0.411
Epoch: 45 RMSE:  0.2779972148537761  MAPE: 0.15021162881681863  L2+L1 loss: 0.471
Epoch [46/75], Batch loss: 0.428
Epoch: 46 RMSE:  0.27940464766784523  MAPE: 0.15141774291566704  L2+L1 loss: 0.472
Epoch [47/75], Batch loss: 0.384
Epoch: 47 RMSE:  0.20340399003836  MAPE: 0.11733695098645422  L2+L1 loss: 0.408
Epoch [48/75], Batch loss: 0.252
Epoch: 48 RMSE:  0.2365787753932435  MAPE: 0.1328080359763068  L2+L1 loss: 0.481
Epoch [49/75], Batch loss: 0.165
Epoch: 49 RMSE:  0.2079260246693808  MAPE: 0.1096371909683381  L2+L1 loss: 0.419
Epoch [50/75], Batch loss: 0.168
Epoch: 50 RMSE:  0.1788142826525201  MAPE: 0.09401075859289819  L2+L1 loss: 0.369
Epoch [51/75], Batch loss: 0.147
Epoch: 51 RMSE:  0.19130936282580005  MAPE: 0.09891362268853499  L2+L1 loss: 0.379
Epoch [52/75], Batch loss: 0.142
Epoch: 52 RMSE:  0.17048652132857564  MAPE: 0.09058573326744898  L2+L1 loss: 0.362
Epoch [53/75], Batch loss: 0.396
Epoch: 53 RMSE:  0.17305994436447494  MAPE: 0.09889376810387028  L2+L1 loss: 0.396
Epoch [54/75], Batch loss: 0.154
Epoch: 54 RMSE:  0.16877786142669443  MAPE: 0.09661252189278324  L2+L1 loss: 0.386
Epoch [55/75], Batch loss: 0.195
Epoch: 55 RMSE:  0.19029188079516418  MAPE: 0.09691745463044758  L2+L1 loss: 0.379
Epoch [56/75], Batch loss: 0.152
Epoch: 56 RMSE:  0.17852569162193874  MAPE: 0.09764204211060923  L2+L1 loss: 0.389
Epoch [57/75], Batch loss: 0.146
Epoch: 57 RMSE:  0.18154534209287335  MAPE: 0.0929045840714556  L2+L1 loss: 0.359
Epoch [58/75], Batch loss: 0.146
Epoch: 58 RMSE:  0.20496233908441236  MAPE: 0.11217225093907703  L2+L1 loss: 0.431
Epoch [59/75], Batch loss: 0.14
Epoch: 59 RMSE:  0.16568880397168673  MAPE: 0.08612051009574859  L2+L1 loss: 0.342
Epoch [60/75], Batch loss: 0.129
Epoch: 60 RMSE:  0.17252976573486117  MAPE: 0.08477716852666943  L2+L1 loss: 0.323
Epoch [61/75], Batch loss: 0.129
Epoch: 61 RMSE:  0.1748321055122899  MAPE: 0.086030052611908  L2+L1 loss: 0.327
Epoch [62/75], Batch loss: 0.109
Epoch: 62 RMSE:  0.05995577354859993  MAPE: 0.03437933138654868  L2+L1 loss: 0.207
Epoch [63/75], Batch loss: 0.076
Epoch: 63 RMSE:  0.06504441951476303  MAPE: 0.036636130944503374  L2+L1 loss: 0.211
Epoch [64/75], Batch loss: 0.072
Epoch: 64 RMSE:  0.07458098478164196  MAPE: 0.04137247071161755  L2+L1 loss: 0.229
Epoch [65/75], Batch loss: 0.063
Epoch: 65 RMSE:  0.059947733818886836  MAPE: 0.03349263147099517  L2+L1 loss: 0.205
Epoch [66/75], Batch loss: 0.064
Epoch: 66 RMSE:  0.05222309062428027  MAPE: 0.029077351078637837  L2+L1 loss: 0.186
Epoch [67/75], Batch loss: 0.065
Epoch: 67 RMSE:  0.059647209337517715  MAPE: 0.032833881234885245  L2+L1 loss: 0.202
Epoch [68/75], Batch loss: 0.063
Epoch: 68 RMSE:  0.06182652990289619  MAPE: 0.03560942074789781  L2+L1 loss: 0.218
Epoch [69/75], Batch loss: 0.061
Epoch: 69 RMSE:  0.058222990881287386  MAPE: 0.03348313526404511  L2+L1 loss: 0.209
Epoch [70/75], Batch loss: 0.063
Epoch: 70 RMSE:  0.06788446533190459  MAPE: 0.03962886642541696  L2+L1 loss: 0.223
Epoch [71/75], Batch loss: 0.052
Epoch: 71 RMSE:  0.0339953946263692  MAPE: 0.019316080427577258  L2+L1 loss: 0.156
Epoch [72/75], Batch loss: 0.05
Epoch: 72 RMSE:  0.04166542824560975  MAPE: 0.023275589556946787  L2+L1 loss: 0.172
Epoch [73/75], Batch loss: 0.052
Epoch: 73 RMSE:  0.05203059016774854  MAPE: 0.027101055929348365  L2+L1 loss: 0.19
Epoch [74/75], Batch loss: 0.044
Epoch: 74 RMSE:  0.03645338217238366  MAPE: 0.021500445770381876  L2+L1 loss: 0.166


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
1.0003121 , 1.02856
1.9470485 , 1.933091
1.1419178 , 1.19448
1.4399194 , 1.444636
1.5304302 , 1.538614
1.3072919 , 1.372113
1.8678819 , 1.871532
1.6220379 , 1.611501
1.2819678 , 1.249866
2.214213 , 2.209489
1.6159718 , 1.634444
1.0449314 , 1.086886
1.9470485 , 1.918243
1.5703993 , 1.533982
1.5304302 , 1.55164
1.0449314 , 1.24401
2.038536 , 2.039546
1.3473663 , 1.359459
2.3008575 , 2.280933
1.0449314 , 1.097668
2.3737762 , 2.337912
1.209645 , 1.24896
1.2819678 , 1.237503
1.6220379 , 1.673119
1.5920404 , 1.569635
1.6220379 , 1.671687
1.0806865 , 1.145255
2.3008575 , 2.260587
1.209645 , 1.397804
2.038536 , 2.037304
1.9470485 , 1.920514
1.6220379 , 1.611623
1.2774979 , 1.294149
1.174301 , 1.118476
1.0449314 , 1.087145
2.3008575 , 2.279909
1.5703993 , 1.535139
1.209645 , 1.249813
1.0931212 , 1.085487
1.3026028 , 1.281061
1.174301 , 1.119028
1.3473663 , 1.358268
1.0449314 , 1.220984
1.0931212 , 1.085601
1.5304302 , 1.537342
1.418559 , 1.428918
1.3473663 , 1.512436
1.7798431 , 1.751759
1.3072919 , 1.37222
1.7040728 , 1.731266
1.5002239 , 1.46881
1.5703993 , 1.523518
2.3737762 , 2.370021
1.6220379 , 1.667526
1.3026028 , 1.279893
1.3072919 , 1.371252
1.6220379 , 1.65531
1.9470485 , 1.920819
RMSE:  0.05832693590376142  MAPE: 0.02868432797322078
5: ground truth total-  58  predicted total -  58
100: ground truth total-  0  predicted total -  0
 more 100: ground truth total -  0  predicted total -  0
