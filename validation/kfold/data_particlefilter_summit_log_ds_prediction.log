['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
605
34
<class 'numpy.dtype'> float64
605
train batches:  109  validate samples: 48  test samples: 121
Epoch [0/75], Batch loss: 4.938
Epoch: 0 RMSE:  0.5450569651659668  MAPE: 0.6125824003081536  L2+L1 loss: 0.368
Epoch [1/75], Batch loss: 0.265
Epoch: 1 RMSE:  0.1072786695169337  MAPE: 0.49124044499910796  L2+L1 loss: 0.289
Epoch [2/75], Batch loss: 0.412
Epoch: 2 RMSE:  0.23509773426517303  MAPE: 1.1496058824813797  L2+L1 loss: 0.498
Epoch [3/75], Batch loss: 0.413
Epoch: 3 RMSE:  0.273435020219631  MAPE: 0.886108674428562  L2+L1 loss: 0.41
Epoch [4/75], Batch loss: 0.473
Epoch: 4 RMSE:  0.3115262901671964  MAPE: 0.7653809598576733  L2+L1 loss: 0.434
Epoch [5/75], Batch loss: 0.35
Epoch: 5 RMSE:  0.09578805701304093  MAPE: 0.4094230216502421  L2+L1 loss: 0.282
Epoch [6/75], Batch loss: 0.088
Epoch: 6 RMSE:  0.0950883052344716  MAPE: 0.3960388041894271  L2+L1 loss: 0.285
Epoch [7/75], Batch loss: 0.388
Epoch: 7 RMSE:  0.3838760226819517  MAPE: 1.865892369586236  L2+L1 loss: 0.711
Epoch [8/75], Batch loss: 0.249
Epoch: 8 RMSE:  0.09553847772352142  MAPE: 0.4059284777130265  L2+L1 loss: 0.282
Epoch [9/75], Batch loss: 0.458
Epoch: 9 RMSE:  0.0960277395479245  MAPE: 0.41244726501116125  L2+L1 loss: 0.282
Epoch [10/75], Batch loss: 0.089
Epoch: 10 RMSE:  0.09497545008130545  MAPE: 0.40498015352674915  L2+L1 loss: 0.282
Epoch [11/75], Batch loss: 1.599
Epoch: 11 RMSE:  0.10575497266466052  MAPE: 0.33754246008075367  L2+L1 loss: 0.286
Epoch [12/75], Batch loss: 0.588
Epoch: 12 RMSE:  0.15055135377674891  MAPE: 0.4110360055593041  L2+L1 loss: 0.345
Epoch [13/75], Batch loss: 0.095
Epoch: 13 RMSE:  0.09518395640066606  MAPE: 0.3991474384286393  L2+L1 loss: 0.284
Epoch [14/75], Batch loss: 0.088
Epoch: 14 RMSE:  0.09558020698737067  MAPE: 0.3782614220563591  L2+L1 loss: 0.288
Epoch [15/75], Batch loss: 0.089
Epoch: 15 RMSE:  0.09719743426653862  MAPE: 0.42363201442053827  L2+L1 loss: 0.28
Epoch [16/75], Batch loss: 0.09
Epoch: 16 RMSE:  0.11538461315119808  MAPE: 0.5357898064691459  L2+L1 loss: 0.297
Epoch [17/75], Batch loss: 0.09
Epoch: 17 RMSE:  0.09902963907799597  MAPE: 0.43825539924754836  L2+L1 loss: 0.281
Epoch [18/75], Batch loss: 0.093
Epoch: 18 RMSE:  0.09505463702609561  MAPE: 0.3939710761987007  L2+L1 loss: 0.285
Epoch [19/75], Batch loss: 0.115
Epoch: 19 RMSE:  0.21943808867090459  MAPE: 1.07211854587607  L2+L1 loss: 0.471
Epoch [20/75], Batch loss: 0.099
Epoch: 20 RMSE:  0.09573742288527975  MAPE: 0.37635366547931626  L2+L1 loss: 0.288
Epoch [21/75], Batch loss: 0.543
Epoch: 21 RMSE:  0.534394829377373  MAPE: 2.2329096659844914  L2+L1 loss: 0.894
Epoch [22/75], Batch loss: 0.178
Epoch: 22 RMSE:  0.09736707503995942  MAPE: 0.42497265786699473  L2+L1 loss: 0.28
Epoch [23/75], Batch loss: 0.09
Epoch: 23 RMSE:  0.09762648385912004  MAPE: 0.42702587132691705  L2+L1 loss: 0.279
Epoch [24/75], Batch loss: 4.866
Epoch: 24 RMSE:  1.0171353583254097  MAPE: 4.791218150416795  L2+L1 loss: 1.427
Epoch [25/75], Batch loss: 0.647
Epoch: 25 RMSE:  0.36450638132563234  MAPE: 1.7744616272005953  L2+L1 loss: 0.686
Epoch [26/75], Batch loss: 0.186
Epoch: 26 RMSE:  0.11438184112607822  MAPE: 0.5304966826215282  L2+L1 loss: 0.296
Epoch [27/75], Batch loss: 11.958
Epoch: 27 RMSE:  0.9340433810331705  MAPE: 4.074224694334126  L2+L1 loss: 1.338
Epoch [28/75], Batch loss: 0.99
Epoch: 28 RMSE:  0.6368940612519788  MAPE: 2.7071966417535003  L2+L1 loss: 1.012
Epoch [29/75], Batch loss: 12.542
Epoch: 29 RMSE:  0.5181577750257585  MAPE: 2.1473724948155373  L2+L1 loss: 0.874
Epoch [30/75], Batch loss: 0.519
Epoch: 30 RMSE:  0.49184570075748163  MAPE: 2.034184544573888  L2+L1 loss: 0.844
Epoch [31/75], Batch loss: 0.488
Epoch: 31 RMSE:  0.4566397526239148  MAPE: 1.8711669063586827  L2+L1 loss: 0.802
Epoch [32/75], Batch loss: 0.445
Epoch: 32 RMSE:  0.4029289806548807  MAPE: 1.619704200528844  L2+L1 loss: 0.735
Epoch [33/75], Batch loss: 0.379
Epoch: 33 RMSE:  0.32559866149225275  MAPE: 1.2538288786163345  L2+L1 loss: 0.635
Epoch [34/75], Batch loss: 0.29
Epoch: 34 RMSE:  0.22823736195481675  MAPE: 0.77943176286655  L2+L1 loss: 0.491
Epoch [35/75], Batch loss: 0.187
Epoch: 35 RMSE:  0.13734897527729487  MAPE: 0.3627627156387783  L2+L1 loss: 0.315
Epoch [36/75], Batch loss: 0.113
Epoch: 36 RMSE:  0.09687845936995823  MAPE: 0.36640249233371547  L2+L1 loss: 0.287
Epoch [37/75], Batch loss: 0.09
Epoch: 37 RMSE:  0.09586779468181734  MAPE: 0.41047639511815337  L2+L1 loss: 0.282
Epoch [38/75], Batch loss: 0.112
Epoch: 38 RMSE:  0.09506538088603507  MAPE: 0.39480814494086447  L2+L1 loss: 0.285
Epoch [39/75], Batch loss: 0.132
Epoch: 39 RMSE:  0.09671645536927023  MAPE: 0.419523265379385  L2+L1 loss: 0.281
Epoch [40/75], Batch loss: 0.088
Epoch: 40 RMSE:  0.09698842341485804  MAPE: 0.4219078422035647  L2+L1 loss: 0.281
Epoch [41/75], Batch loss: 0.088
Epoch: 41 RMSE:  0.09605951270399087  MAPE: 0.41281944558815425  L2+L1 loss: 0.282
Epoch [42/75], Batch loss: 0.089
Epoch: 42 RMSE:  0.09622183708376494  MAPE: 0.4146372655083243  L2+L1 loss: 0.282
Epoch [43/75], Batch loss: 0.107
Epoch: 43 RMSE:  0.09687086927324187  MAPE: 0.42089845553887056  L2+L1 loss: 0.281
Epoch [44/75], Batch loss: 0.088
Epoch: 44 RMSE:  0.09813377686412743  MAPE: 0.43124387049032203  L2+L1 loss: 0.28
Epoch [45/75], Batch loss: 0.094
Epoch: 45 RMSE:  0.09634516147857491  MAPE: 0.41593723622577855  L2+L1 loss: 0.282
Epoch [46/75], Batch loss: 0.088
Epoch: 46 RMSE:  0.09839224937857334  MAPE: 0.433265412855158  L2+L1 loss: 0.28
Epoch [47/75], Batch loss: 0.089
Epoch: 47 RMSE:  0.10164460814230372  MAPE: 0.45620526115194204  L2+L1 loss: 0.282
Epoch [48/75], Batch loss: 0.148
Epoch: 48 RMSE:  0.10442809345138282  MAPE: 0.3398674022210182  L2+L1 loss: 0.286
Epoch [49/75], Batch loss: 0.1
Epoch: 49 RMSE:  0.09582133537054383  MAPE: 0.4098691161248785  L2+L1 loss: 0.282
Epoch [50/75], Batch loss: 0.088
Epoch: 50 RMSE:  0.09801585556424904  MAPE: 0.43029488426767193  L2+L1 loss: 0.28
Epoch [51/75], Batch loss: 0.089
Epoch: 51 RMSE:  0.0969637687824995  MAPE: 0.4216986538220546  L2+L1 loss: 0.281
Epoch [52/75], Batch loss: 0.09
Epoch: 52 RMSE:  0.09748350490849222  MAPE: 0.42586560001495294  L2+L1 loss: 0.279
Epoch [53/75], Batch loss: 0.096
Epoch: 53 RMSE:  0.09700972603621989  MAPE: 0.4220875554098596  L2+L1 loss: 0.281
Epoch [54/75], Batch loss: 0.109
Epoch: 54 RMSE:  0.09685025035907435  MAPE: 0.4207181899868731  L2+L1 loss: 0.281
Epoch [55/75], Batch loss: 0.167
Epoch: 55 RMSE:  0.12711258810958437  MAPE: 0.5996745658528848  L2+L1 loss: 0.315
Epoch [56/75], Batch loss: 0.095
Epoch: 56 RMSE:  0.0963455860608405  MAPE: 0.41594160477815284  L2+L1 loss: 0.282
Epoch [57/75], Batch loss: 0.089
Epoch: 57 RMSE:  0.09686012634543306  MAPE: 0.420804657195937  L2+L1 loss: 0.281
Epoch [58/75], Batch loss: 0.088
Epoch: 58 RMSE:  0.0970289220948432  MAPE: 0.42224868971525226  L2+L1 loss: 0.281
Epoch [59/75], Batch loss: 0.089
Epoch: 59 RMSE:  0.09573809733581776  MAPE: 0.40873434698571703  L2+L1 loss: 0.282
Epoch [60/75], Batch loss: 0.089
Epoch: 60 RMSE:  0.09616316882623123  MAPE: 0.41399538958877463  L2+L1 loss: 0.282
Epoch [61/75], Batch loss: 0.088
Epoch: 61 RMSE:  0.09702914395431914  MAPE: 0.4222505476053425  L2+L1 loss: 0.281
Epoch [62/75], Batch loss: 0.088
Epoch: 62 RMSE:  0.09690027507624302  MAPE: 0.4211538401064079  L2+L1 loss: 0.281
Epoch [63/75], Batch loss: 0.088
Epoch: 63 RMSE:  0.09732204251328716  MAPE: 0.42462151663994246  L2+L1 loss: 0.28
Epoch [64/75], Batch loss: 0.088
Epoch: 64 RMSE:  0.09695265754413432  MAPE: 0.42160395164069886  L2+L1 loss: 0.281
Epoch [65/75], Batch loss: 0.088
Epoch: 65 RMSE:  0.09642701009842258  MAPE: 0.4167668594709321  L2+L1 loss: 0.282
Epoch [66/75], Batch loss: 0.088
Epoch: 66 RMSE:  0.09669957105703822  MAPE: 0.41936921114163345  L2+L1 loss: 0.281
Epoch [67/75], Batch loss: 0.088
Epoch: 67 RMSE:  0.09641605049650642  MAPE: 0.41665719374236315  L2+L1 loss: 0.282
Epoch [68/75], Batch loss: 0.088
Epoch: 68 RMSE:  0.09609005523849788  MAPE: 0.41317184214634867  L2+L1 loss: 0.282
Epoch [69/75], Batch loss: 0.087
Epoch: 69 RMSE:  0.08956668519786319  MAPE: 0.3750960739716313  L2+L1 loss: 0.274
Epoch [70/75], Batch loss: 0.071
Epoch: 70 RMSE:  0.055284695282369165  MAPE: 0.1636206201476599  L2+L1 loss: 0.172
Epoch [71/75], Batch loss: 0.041
Epoch: 71 RMSE:  0.036951576214315794  MAPE: 0.10215493483668907  L2+L1 loss: 0.137
Epoch [72/75], Batch loss: 0.019
Epoch: 72 RMSE:  0.014704421575021312  MAPE: 0.056313319733526164  L2+L1 loss: 0.088
Epoch [73/75], Batch loss: 0.012
Epoch: 73 RMSE:  0.014167837550368259  MAPE: 0.05934272366682224  L2+L1 loss: 0.095
Epoch [74/75], Batch loss: 0.012
Epoch: 74 RMSE:  0.018198552053432834  MAPE: 0.06989121798977371  L2+L1 loss: 0.107


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
0.2029615 , 0.207702
0.13544326 , 0.131594
0.36633468 , 0.340981
0.34720552 , 0.331227
0.31278828 , 0.302097
0.3222328 , 0.299486
0.14805746 , 0.103127
0.33368337 , 0.327009
0.43006796 , 0.40585
0.22171777 , 0.205244
0.39927968 , 0.368048
0.43737122 , 0.416779
0.15020718 , 0.142058
0.15095645 , 0.107015
0.21099463 , 0.20323
0.18034384 , 0.181137
0.14591293 , 0.156144
0.39927968 , 0.373374
0.1648634 , 0.157123
0.3065384 , 0.290507
0.3065384 , 0.290813
0.32288066 , 0.309636
0.35368732 , 0.340498
0.3222328 , 0.299088
0.3341143 , 0.309609
0.26210165 , 0.25594
0.19181453 , 0.195126
0.44766998 , 0.444435
0.33373424 , 0.319303
0.21794456 , 0.218112
0.34504664 , 0.32082
0.43737122 , 0.417219
0.14805746 , 0.103075
0.43737122 , 0.415953
0.1457588 , 0.143278
0.44766998 , 0.448429
0.21099463 , 0.201195
0.29508197 , 0.292974
0.3065384 , 0.291164
0.35167506 , 0.34824
0.43737122 , 0.417412
0.26210165 , 0.263855
0.43737122 , 0.41641
0.28308356 , 0.270527
0.3341143 , 0.311037
0.35368732 , 0.340625
0.14805746 , 0.103666
0.28966704 , 0.308242
0.29426858 , 0.271502
0.34359637 , 0.362741
0.15020718 , 0.151728
0.36633468 , 0.341616
0.16585022 , 0.171692
0.30885085 , 0.303024
0.30141893 , 0.280053
0.44766998 , 0.444898
0.13110398 , 0.117505
0.4408116 , 0.499121
0.26210165 , 0.265227
0.26945436 , 0.250116
0.32631028 , 0.331036
0.18034384 , 0.187475
0.21794456 , 0.218258
0.31278828 , 0.313515
0.33368337 , 0.326661
0.17736045 , 0.165719
0.28206053 , 0.263955
0.12281118 , 0.10866
0.34055987 , 0.340887
0.28966704 , 0.279038
0.22171777 , 0.200614
0.34720552 , 0.335627
0.36633468 , 0.340872
0.26354393 , 0.258812
0.29426858 , 0.272448
0.1751858 , 0.1826
0.19983457 , 0.200725
0.44766998 , 0.444153
0.26210165 , 0.254182
0.36633468 , 0.340817
0.14805746 , 0.181584
0.15020718 , 0.146666
0.43006796 , 0.397569
0.2446203 , 0.223901
0.12696822 , 0.127248
0.38730222 , 0.361992
0.1648634 , 0.157025
0.19983457 , 0.200194
0.23363051 , 0.234435
0.1457588 , 0.133383
0.3177211 , 0.305013
0.23363051 , 0.231879
0.17736045 , 0.169903
0.25103614 , 0.246834
0.30141893 , 0.291132
0.34055987 , 0.325209
0.38730222 , 0.359352
0.28308356 , 0.270404
0.14805746 , 0.102915
0.14730893 , 0.168729
0.22171777 , 0.206603
0.18811852 , 0.178108
0.4232174 , 0.387967
0.44424182 , 0.435928
0.1648634 , 0.146499
0.37689218 , 0.352073
0.4408116 , 0.471843
0.45105383 , 0.455276
0.34504664 , 0.321388
0.26945436 , 0.250379
0.43006796 , 0.40874
0.2446203 , 0.23517
0.19983457 , 0.200676
0.39927968 , 0.380055
0.37689218 , 0.355018
0.11838046 , 0.105349
0.12499204 , 0.111251
0.15661846 , 0.158548
0.35167506 , 0.357105
0.35167506 , 0.349739
0.26210165 , 0.253963
RMSE:  0.018662537082909787  MAPE: 0.06491727177773028
5: ground truth total-  121  predicted total -  121
100: ground truth total-  0  predicted total -  0
 more 100: ground truth total -  0  predicted total -  0
