['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1694
34
<class 'numpy.dtype'> float64
1694
train batches:  77  validate samples: 135  test samples: 338
Epoch [0/75], Batch loss: 5.991
Epoch: 0 RMSE:  2.994210487639206  MAPE: 0.7573072710331537  L2+L1 loss: 1.039
Epoch [1/75], Batch loss: 1.099
Epoch: 1 RMSE:  2.9491571353559323  MAPE: 0.6569516685561595  L2+L1 loss: 0.894
Epoch [2/75], Batch loss: 1.011
Epoch: 2 RMSE:  2.9226367696999183  MAPE: 1.104211545752066  L2+L1 loss: 0.92
Epoch [3/75], Batch loss: 13.199
Epoch: 3 RMSE:  3.147599493556574  MAPE: 2.64580669659816  L2+L1 loss: 1.616
Epoch [4/75], Batch loss: 1.404
Epoch: 4 RMSE:  3.058388593092592  MAPE: 1.653222185304358  L2+L1 loss: 1.331
Epoch [5/75], Batch loss: 36.398
Epoch: 5 RMSE:  3.3249336128570555  MAPE: 3.150250715170986  L2+L1 loss: 1.886
Epoch [6/75], Batch loss: 1.285
Epoch: 6 RMSE:  2.9177245460721584  MAPE: 1.2458234915395363  L2+L1 loss: 0.942
Epoch [7/75], Batch loss: 3.413
Epoch: 7 RMSE:  3.3808582310520814  MAPE: 4.66126716085342  L2+L1 loss: 2.163
Epoch [8/75], Batch loss: 1.372
Epoch: 8 RMSE:  2.9147159434376673  MAPE: 1.348669255629586  L2+L1 loss: 0.958
Epoch [9/75], Batch loss: 1.063
Epoch: 9 RMSE:  2.9039230474108  MAPE: 2.339714761898152  L2+L1 loss: 1.157
Epoch [10/75], Batch loss: 1.083
Epoch: 10 RMSE:  2.917784131183173  MAPE: 1.2439464919247214  L2+L1 loss: 0.941
Epoch [11/75], Batch loss: 1.422
Epoch: 11 RMSE:  2.9408109705497765  MAPE: 0.7540256997111248  L2+L1 loss: 0.888
Epoch [12/75], Batch loss: 1.076
Epoch: 12 RMSE:  2.9137170623025934  MAPE: 1.3867112307670262  L2+L1 loss: 0.965
Epoch [13/75], Batch loss: 1.056
Epoch: 13 RMSE:  2.9071552291805256  MAPE: 1.7323014614472405  L2+L1 loss: 1.028
Epoch [14/75], Batch loss: 1.048
Epoch: 14 RMSE:  2.906017008937033  MAPE: 1.8123260105544223  L2+L1 loss: 1.043
Epoch [15/75], Batch loss: 1.052
Epoch: 15 RMSE:  2.903991232111903  MAPE: 2.3607553504789878  L2+L1 loss: 1.162
Epoch [16/75], Batch loss: 1.078
Epoch: 16 RMSE:  2.9242109124973967  MAPE: 1.0645716117662427  L2+L1 loss: 0.915
Epoch [17/75], Batch loss: 10.117
Epoch: 17 RMSE:  2.9133558263890786  MAPE: 1.4009327472030648  L2+L1 loss: 0.967
Epoch [18/75], Batch loss: 1.037
Epoch: 18 RMSE:  2.909760434603741  MAPE: 1.5636631625606368  L2+L1 loss: 0.994
Epoch [19/75], Batch loss: 1.052
Epoch: 19 RMSE:  2.9096783513326785  MAPE: 1.5679288216912515  L2+L1 loss: 0.994
Epoch [20/75], Batch loss: 0.989
Epoch: 20 RMSE:  2.9069806245149934  MAPE: 1.7349184048439976  L2+L1 loss: 1.029
Epoch [21/75], Batch loss: 1.74
Epoch: 21 RMSE:  17.406837526065257  MAPE: 67.88174968400523  L2+L1 loss: 11.653
Epoch [22/75], Batch loss: 1.272
Epoch: 22 RMSE:  2.9041854503498303  MAPE: 1.989534289333855  L2+L1 loss: 1.08
Epoch [23/75], Batch loss: 2.165
Epoch: 23 RMSE:  2.9249588177967123  MAPE: 1.0392500185854143  L2+L1 loss: 0.912
Epoch [24/75], Batch loss: 1.117
Epoch: 24 RMSE:  2.9106188613396284  MAPE: 1.520737911179672  L2+L1 loss: 0.987
Epoch [25/75], Batch loss: 1.064
Epoch: 25 RMSE:  2.907668491953144  MAPE: 1.686733557415347  L2+L1 loss: 1.018
Epoch [26/75], Batch loss: 1.052
Epoch: 26 RMSE:  2.905495551186943  MAPE: 1.8617333602085822  L2+L1 loss: 1.054
Epoch [27/75], Batch loss: 1.061
Epoch: 27 RMSE:  2.9069114212835583  MAPE: 1.740041212669354  L2+L1 loss: 1.03
Epoch [28/75], Batch loss: 1.076
Epoch: 28 RMSE:  2.9068499869272904  MAPE: 1.7457118000495142  L2+L1 loss: 1.031
Epoch [29/75], Batch loss: 1.341
Epoch: 29 RMSE:  2.827078752450211  MAPE: 0.9168017355310575  L2+L1 loss: 0.845
Epoch [30/75], Batch loss: 0.909
Epoch: 30 RMSE:  2.8053914842209364  MAPE: 0.9188137782578114  L2+L1 loss: 0.834
Epoch [31/75], Batch loss: 0.897
Epoch: 31 RMSE:  2.7883696997938197  MAPE: 0.8978882244949703  L2+L1 loss: 0.822
Epoch [32/75], Batch loss: 0.956
Epoch: 32 RMSE:  2.9161989181480674  MAPE: 1.2958839914491014  L2+L1 loss: 0.949
Epoch [33/75], Batch loss: 1.012
Epoch: 33 RMSE:  2.9091501922183767  MAPE: 1.5964392431385093  L2+L1 loss: 0.999
Epoch [34/75], Batch loss: 1.055
Epoch: 34 RMSE:  2.9079162010775237  MAPE: 1.670531298738387  L2+L1 loss: 1.015
Epoch [35/75], Batch loss: 1.008
Epoch: 35 RMSE:  2.908636156387533  MAPE: 1.625991939041023  L2+L1 loss: 1.005
Epoch [36/75], Batch loss: 1.001
Epoch: 36 RMSE:  2.908245841314834  MAPE: 1.6496979150070725  L2+L1 loss: 1.011
Epoch [37/75], Batch loss: 1.035
Epoch: 37 RMSE:  2.912255705096805  MAPE: 1.446445181673772  L2+L1 loss: 0.974
Epoch [38/75], Batch loss: 1.323
Epoch: 38 RMSE:  2.9379560446729025  MAPE: 0.7976407321813549  L2+L1 loss: 0.892
Epoch [39/75], Batch loss: 0.998
Epoch: 39 RMSE:  2.9205139546831633  MAPE: 1.1623773083736966  L2+L1 loss: 0.93
Epoch [40/75], Batch loss: 1.166
Epoch: 40 RMSE:  2.9186675573109446  MAPE: 0.6521198176730691  L2+L1 loss: 0.823
Epoch [41/75], Batch loss: 1.047
Epoch: 41 RMSE:  2.910719765085218  MAPE: 1.515902613880799  L2+L1 loss: 0.986
Epoch [42/75], Batch loss: 1.058
Epoch: 42 RMSE:  2.9091477600633824  MAPE: 1.5965750286049547  L2+L1 loss: 0.999
Epoch [43/75], Batch loss: 1.069
Epoch: 43 RMSE:  1.3594062872742605  MAPE: 1.3510031256235335  L2+L1 loss: 0.77
Epoch [44/75], Batch loss: 0.809
Epoch: 44 RMSE:  1.8707471208991746  MAPE: 1.0289234412828514  L2+L1 loss: 0.692
Epoch [45/75], Batch loss: 0.647
Epoch: 45 RMSE:  1.5042500143455373  MAPE: 0.953480266367139  L2+L1 loss: 0.689
Epoch [46/75], Batch loss: 0.634
Epoch: 46 RMSE:  1.309395256418183  MAPE: 0.7422601206857594  L2+L1 loss: 0.632
Epoch [47/75], Batch loss: 0.631
Epoch: 47 RMSE:  1.6971761574531996  MAPE: 0.7201900540405906  L2+L1 loss: 0.642
Epoch [48/75], Batch loss: 0.578
Epoch: 48 RMSE:  1.4605758201664554  MAPE: 0.5500774547158194  L2+L1 loss: 0.609
Epoch [49/75], Batch loss: 0.671
Epoch: 49 RMSE:  2.1608714861747242  MAPE: 0.6295214558886526  L2+L1 loss: 0.658
Epoch [50/75], Batch loss: 0.798
Epoch: 50 RMSE:  1.2785941695869973  MAPE: 0.5535549972814662  L2+L1 loss: 0.653
Epoch [51/75], Batch loss: 0.717
Epoch: 51 RMSE:  1.8000319712337818  MAPE: 0.49397771982569527  L2+L1 loss: 0.664
Epoch [52/75], Batch loss: 0.686
Epoch: 52 RMSE:  3.082104098403039  MAPE: 0.6037279298504583  L2+L1 loss: 0.804
Epoch [53/75], Batch loss: 0.784
Epoch: 53 RMSE:  2.1239236145376688  MAPE: 0.44520336704809466  L2+L1 loss: 0.641
Epoch [54/75], Batch loss: 0.594
Epoch: 54 RMSE:  1.44220013949132  MAPE: 0.3719176858675731  L2+L1 loss: 0.553
Epoch [55/75], Batch loss: 0.59
Epoch: 55 RMSE:  1.34670000714282  MAPE: 0.46884321654679256  L2+L1 loss: 0.575
Epoch [56/75], Batch loss: 0.553
Epoch: 56 RMSE:  1.6322963768689531  MAPE: 0.37211183421542054  L2+L1 loss: 0.556
Epoch [57/75], Batch loss: 0.576
Epoch: 57 RMSE:  1.381802958036772  MAPE: 0.3923822555470925  L2+L1 loss: 0.558
Epoch [58/75], Batch loss: 0.494
Epoch: 58 RMSE:  2.2652449965151997  MAPE: 0.3637395108518622  L2+L1 loss: 0.64
Epoch [59/75], Batch loss: 0.85
Epoch: 59 RMSE:  1.367827111989315  MAPE: 0.8968545541957544  L2+L1 loss: 0.761
Epoch [60/75], Batch loss: 0.574
Epoch: 60 RMSE:  1.3690395351994735  MAPE: 0.6358195456565867  L2+L1 loss: 0.608
Epoch [61/75], Batch loss: 0.549
Epoch: 61 RMSE:  1.3435427582405233  MAPE: 0.5756983739874144  L2+L1 loss: 0.592
Epoch [62/75], Batch loss: 0.539
Epoch: 62 RMSE:  1.4347723249049134  MAPE: 0.5355288743245645  L2+L1 loss: 0.596
Epoch [63/75], Batch loss: 0.507
Epoch: 63 RMSE:  1.27809648059228  MAPE: 0.5097946853821903  L2+L1 loss: 0.566
Epoch [64/75], Batch loss: 0.485
Epoch: 64 RMSE:  1.3210342944247437  MAPE: 0.2862757749566296  L2+L1 loss: 0.509
Epoch [65/75], Batch loss: 0.445
Epoch: 65 RMSE:  1.272577877109001  MAPE: 0.3084319497307944  L2+L1 loss: 0.511
Epoch [66/75], Batch loss: 0.449
Epoch: 66 RMSE:  1.2590195833569608  MAPE: 0.27983268160749647  L2+L1 loss: 0.499
Epoch [67/75], Batch loss: 0.454
Epoch: 67 RMSE:  1.3115476595614668  MAPE: 0.2713065792466024  L2+L1 loss: 0.505
Epoch [68/75], Batch loss: 0.44
Epoch: 68 RMSE:  1.2557094621997573  MAPE: 0.37799256274032933  L2+L1 loss: 0.556
Epoch [69/75], Batch loss: 0.438
Epoch: 69 RMSE:  1.382225080668946  MAPE: 0.2964669353143734  L2+L1 loss: 0.546
Epoch [70/75], Batch loss: 0.485
Epoch: 70 RMSE:  1.4599775496147358  MAPE: 0.3041150624926758  L2+L1 loss: 0.549
Epoch [71/75], Batch loss: 0.436
Epoch: 71 RMSE:  1.2358904463792904  MAPE: 0.2993135621465987  L2+L1 loss: 0.5
Epoch [72/75], Batch loss: 0.434
Epoch: 72 RMSE:  1.3519664652144971  MAPE: 0.2651307258632133  L2+L1 loss: 0.495
Epoch [73/75], Batch loss: 0.469
Epoch: 73 RMSE:  1.355412711011065  MAPE: 0.3142321008156621  L2+L1 loss: 0.564
Epoch [74/75], Batch loss: 0.437
Epoch: 74 RMSE:  1.2393937240428452  MAPE: 0.30047477479646095  L2+L1 loss: 0.506


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
0.34250405 , 0.3824
0.44679195 , 0.3669
0.84798145 , 0.5101
0.37702778 , 0.3942
0.13893251 , 0.1178
0.16518742 , 0.2133
0.105385795 , 0.1139
0.4465574 , 0.67
0.9413349 , 1.2372
0.288368 , 0.2483
0.44004482 , 0.3476
0.15889615 , 0.1119
0.19034186 , 0.1319
1.1854197 , 0.9507
1.1878412 , 0.9779
0.5706446 , 0.5168
0.16508678 , 0.2565
0.52077895 , 0.4895
1.9698288 , 1.2321
0.19512014 , 0.1282
0.23189887 , 0.2268
0.13952361 , 0.1347
1.5017514 , 1.0237
0.288368 , 0.3563
1.4747671 , 1.2289
0.1497328 , 0.1672
0.35394755 , 0.2944
0.35976166 , 0.4092
0.5049383 , 0.3788
0.1685858 , 0.1185
0.18299682 , 0.1049
0.19719669 , 0.1762
0.14639631 , 0.1128
0.24698353 , 0.1939
1.3616383 , 1.6718
0.45866776 , 0.4647
0.1767816 , 0.1641
0.2624789 , 0.2701
1.2412605 , 0.7264
0.9811729 , 0.6711
0.18871444 , 0.2454
0.1430226 , 0.103
0.15730153 , 0.1108
0.15042154 , 0.2068
1.9044976 , 2.527
0.22712886 , 0.1512
0.61191356 , 0.6376
0.15764585 , 0.1631
0.4794153 , 0.366
0.27693763 , 0.3905
0.36978874 , 0.3995
1.0365119 , 0.7025
0.2870017 , 0.2285
1.3666043 , 1.1336
0.20362806 , 0.2051
0.5981296 , 0.558
0.7115338 , 0.6884
1.3329057 , 0.9675
0.14666608 , 0.1141
0.19924256 , 0.2569
2.7337208 , 2.0507
14.725718 , 7.9125
0.3985741 , 0.423
0.056789428 , 0.104
0.271229 , 0.2749
0.12081489 , 0.1236
1.2238238 , 0.8301
0.80844706 , 0.6018
0.14840816 , 0.1234
0.5876579 , 0.2824
0.26530436 , 0.1922
0.6000048 , 0.4557
0.1848083 , 0.1902
0.39030057 , 0.3966
0.8438965 , 0.615
0.15214998 , 0.1397
0.14927757 , 0.1957
0.35588664 , 0.2796
0.1419467 , 0.1305
0.66397035 , 0.5188
0.288368 , 0.2231
0.5416353 , 0.6012
0.24723914 , 0.2084
2.5319593 , 4.2524
0.16753772 , 0.1362
0.13409314 , 0.161
0.13555348 , 0.113
0.33836937 , 0.3433
0.617095 , 0.4461
0.6866056 , 0.6139
0.15504326 , 0.1198
0.42365634 , 0.544
0.41663408 , 0.311
0.80005115 , 0.7384
1.4611716 , 1.7706
0.7339755 , 0.6614
0.3897046 , 0.433
0.17226413 , 0.1647
0.25561985 , 0.4239
0.13687102 , 0.1678
0.75405514 , 0.7731
0.1947538 , 0.1462
0.288368 , 0.2912
0.3338971 , 0.326
1.1227024 , 0.8739
0.76797104 , 0.7769
0.93512446 , 0.7542
0.15471601 , 0.3024
0.35247004 , 0.2638
0.24492921 , 0.3382
1.084884 , 1.2395
0.49692315 , 0.5934
1.2419226 , 0.6465
0.17836668 , 0.1825
0.1587259 , 0.1006
0.20704708 , 0.1819
0.60813165 , 0.429
1.7772392 , 0.9813
0.1450403 , 0.1751
0.30164632 , 0.2985
0.6123481 , 0.4188
0.288368 , 0.3716
1.1143781 , 1.218
1.07602 , 1.3705
0.16349639 , 0.1452
0.14945075 , 0.1005
0.13957952 , 0.1174
1.9076586 , 0.8834
1.546326 , 0.7421
0.73367596 , 0.452
0.24652144 , 0.203
0.15343924 , 0.2499
0.13294399 , 0.1305
1.108242 , 0.8864
0.20070328 , 0.1611
1.2613554 , 0.6918
0.34678775 , 0.1947
1.6291028 , 1.025
0.18367323 , 0.2144
0.288368 , 0.3336
0.13045888 , 0.1283
0.15045556 , 0.1106
0.16063486 , 0.1209
1.1112643 , 1.2535
0.44928947 , 0.493
1.1925199 , 1.7051
0.17783564 , 0.2289
0.18189007 , 0.1038
0.22457024 , 0.1536
0.89140046 , 0.4659
0.3026657 , 0.187
0.32825732 , 0.3093
0.31912518 , 0.1444
0.15985665 , 0.2455
0.6322466 , 0.6413
0.14344443 , 0.1564
0.04702726 , 0.1227
0.51734406 , 0.5128
0.8352551 , 0.7623
0.38638878 , 0.3354
0.3900618 , 0.2686
0.175798 , 0.1515
0.1457614 , 0.1339
1.213207 , 0.8713
0.3098387 , 0.2981
0.1441347 , 0.1597
0.32926255 , 0.3107
1.1651223 , 1.052
0.3783189 , 0.3414
0.15287934 , 0.1058
0.17321566 , 0.128
0.63436675 , 0.5001
0.15081827 , 0.1381
0.288368 , 0.2872
0.15836523 , 0.1171
0.1417995 , 0.1606
1.0393718 , 0.9348
0.18160182 , 0.1719
1.2731214 , 1.0729
0.17413723 , 0.2547
1.0836496 , 0.9324
0.12782872 , 0.1007
1.3419613 , 1.7361
0.17016253 , 0.1308
0.32872486 , 0.4391
0.29232576 , 0.2505
0.28288123 , 0.2769
1.4458829 , 1.3075
0.17013955 , 0.2253
0.93061763 , 0.9095
0.31209132 , 0.4303
0.14358443 , 0.2073
0.3280873 , 0.4964
0.14937165 , 0.1283
0.6843462 , 0.4566
0.13766569 , 0.1072
0.18233657 , 0.2656
1.3412287 , 1.6363
0.30412173 , 0.2954
0.39788038 , 0.29
0.7316291 , 0.6117
0.48174804 , 0.4204
0.32503355 , 0.2336
0.1548183 , 0.1363
1.9886734 , 1.1861
0.42599136 , 0.3209
0.83539855 , 0.8929
0.288368 , 0.2204
0.16578476 , 0.1318
1.2146597 , 1.4296
0.15992996 , 0.1142
0.41290644 , 0.5288
0.7891332 , 0.7448
1.2541654 , 1.7792
0.48736665 , 0.5077
0.16996497 , 0.1169
0.2938752 , 0.312
0.2626477 , 0.1834
0.18718383 , 0.2521
0.7834177 , 0.618
0.15259536 , 0.1285
1.1042466 , 1.6461
0.20085543 , 0.1261
0.79158515 , 1.1972
0.0 , 0.1176
0.94519377 , 0.9354
0.17812246 , 0.1051
1.2678466 , 0.8253
2.4656422 , 1.6108
0.17912672 , 0.1267
0.1556061 , 0.1253
0.21270102 , 0.1709
0.22976539 , 0.1273
0.52329916 , 0.6678
0.16413009 , 0.1263
1.6650321 , 0.8304
0.143723 , 0.1314
0.17787512 , 0.1703
0.14539723 , 0.101
0.87519693 , 0.8449
0.1617562 , 0.1015
0.181553 , 0.1254
0.288368 , 0.2686
0.71932995 , 0.4381
0.2074454 , 0.1189
0.9687175 , 0.8068
1.0023097 , 0.6239
1.2454889 , 0.8693
1.375251 , 0.9516
1.2194462 , 0.8164
0.13932472 , 0.1134
0.7352377 , 0.8389
0.49609873 , 0.4383
0.16068031 , 0.1534
0.5708077 , 0.3813
1.4220215 , 1.7468
0.5722322 , 0.8114
0.90606797 , 0.5809
0.288368 , 0.1401
1.6376746 , 1.2347
0.12122643 , 0.1203
0.18325144 , 0.1495
0.9154522 , 1.2457
0.21701998 , 0.1534
0.17526439 , 0.1129
0.85311556 , 0.65
0.7718377 , 0.5393
0.288368 , 0.2356
1.0273354 , 0.9262
0.39712906 , 0.4891
0.55295074 , 0.6222
0.15500546 , 0.1086
0.624822 , 0.4919
0.25690475 , 0.2303
0.16943668 , 0.1639
0.47591996 , 0.5123
0.16048326 , 0.1509
0.16089734 , 0.1443
0.15708655 , 0.1403
0.1487477 , 0.1409
0.55089897 , 0.6585
0.15793559 , 0.1569
1.3691326 , 0.7767
0.20974395 , 0.1964
0.9092061 , 0.6209
0.5242655 , 0.3697
1.6299706 , 1.7596
0.5979749 , 0.3938
0.45847058 , 0.4954
0.51638293 , 0.4627
0.4595785 , 0.6158
1.7778846 , 2.4573
0.4318802 , 0.3445
0.14651103 , 0.1943
0.78418016 , 0.5233
0.1528471 , 0.1445
0.97274023 , 0.6573
1.7691164 , 0.9705
1.0333275 , 1.0957
0.21530384 , 0.1822
0.18064722 , 0.1394
0.16268943 , 0.1484
1.6755142 , 1.9206
1.4404283 , 0.9996
0.33104396 , 0.2401
0.18777955 , 0.1111
0.1415787 , 0.104
0.3051538 , 0.2597
0.16604853 , 0.1438
0.5869925 , 0.5714
2.2058945 , 1.6433
0.14070062 , 0.1352
0.16350466 , 0.184
0.15160055 , 0.1114
0.63541496 , 0.514
2.5045705 , 1.7425
0.28462717 , 0.211
0.17499322 , 0.2119
0.40389475 , 0.4734
0.17732084 , 0.1613
1.6214672 , 1.0557
0.172396 , 0.1475
0.17143759 , 0.1193
2.237667 , 1.2873
1.1335185 , 0.6591
0.14521255 , 0.1111
0.6702008 , 0.5567
1.8037709 , 3.2464
1.5484573 , 1.72
0.13996418 , 0.1264
0.16283335 , 0.1752
0.15198258 , 0.1305
0.1558458 , 0.1083
0.38591835 , 0.4845
1.3592672 , 0.8122
0.16001515 , 0.1107
0.8523469 , 0.6652
0.14735997 , 0.1523
RMSE:  0.4541495566863761  MAPE: 0.2849187725433338
5: ground truth total-  337  predicted total -  337
100: ground truth total-  1  predicted total -  1
 more 100: ground truth total -  0  predicted total -  0
