['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1574
34
<class 'numpy.dtype'> float64
1574
train batches:  71  validate samples: 125  test samples: 314
Epoch [0/150], Batch loss: 50.246
Epoch: 0 RMSE:  66.51071002176857  MAPE: 63.262753053513286  L2+L1 loss: 24.646
Epoch [1/150], Batch loss: 39.432
Epoch: 1 RMSE:  66.84184104834026  MAPE: 83.00181717081512  L2+L1 loss: 27.649
Epoch [2/150], Batch loss: 41.343
Epoch: 2 RMSE:  66.88228888985293  MAPE: 40.86150112577286  L2+L1 loss: 22.05
Epoch [3/150], Batch loss: 38.476
Epoch: 3 RMSE:  66.51027984365197  MAPE: 62.61894706192383  L2+L1 loss: 24.561
Epoch [4/150], Batch loss: 38.79
Epoch: 4 RMSE:  66.54835915437444  MAPE: 69.44975582747492  L2+L1 loss: 25.496
Epoch [5/150], Batch loss: 40.427
Epoch: 5 RMSE:  66.59453893614833  MAPE: 52.215476957960604  L2+L1 loss: 23.239
Epoch [6/150], Batch loss: 39.32
Epoch: 6 RMSE:  66.51034567834057  MAPE: 62.21532749681925  L2+L1 loss: 24.507
Epoch [7/150], Batch loss: 39.388
Epoch: 7 RMSE:  63.22312471364598  MAPE: 55.24859445796953  L2+L1 loss: 23.014
Epoch [8/150], Batch loss: 56.711
Epoch: 8 RMSE:  66.55828856938646  MAPE: 54.74190046190168  L2+L1 loss: 23.546
Epoch [9/150], Batch loss: 38.631
Epoch: 9 RMSE:  66.51485558092824  MAPE: 64.92315071552747  L2+L1 loss: 24.866
Epoch [10/150], Batch loss: 39.109
Epoch: 10 RMSE:  66.51569568908398  MAPE: 65.13419041920999  L2+L1 loss: 24.896
Epoch [11/150], Batch loss: 40.253
Epoch: 11 RMSE:  66.54016585426571  MAPE: 68.65889221276115  L2+L1 loss: 25.382
Epoch [12/150], Batch loss: 39.211
Epoch: 12 RMSE:  66.55601843919993  MAPE: 70.11505383931785  L2+L1 loss: 25.591
Epoch [13/150], Batch loss: 40.323
Epoch: 13 RMSE:  66.60565709878152  MAPE: 73.49095119579819  L2+L1 loss: 26.093
Epoch [14/150], Batch loss: 40.156
Epoch: 14 RMSE:  66.58296102122105  MAPE: 72.09561859820637  L2+L1 loss: 25.879
Epoch [15/150], Batch loss: 40.005
Epoch: 15 RMSE:  66.71680396859796  MAPE: 78.67406814197702  L2+L1 loss: 26.911
Epoch [16/150], Batch loss: 40.721
Epoch: 16 RMSE:  66.51029726605387  MAPE: 62.698118885513665  L2+L1 loss: 24.572
Epoch [17/150], Batch loss: 41.076
Epoch: 17 RMSE:  46.557456068990476  MAPE: 69.91881778095848  L2+L1 loss: 38.004
Epoch [18/150], Batch loss: 49.083
Epoch: 18 RMSE:  66.76532887650754  MAPE: 44.588582663065765  L2+L1 loss: 22.389
Epoch [19/150], Batch loss: 39.516
Epoch: 19 RMSE:  66.51325148221918  MAPE: 60.582416612390574  L2+L1 loss: 24.292
Epoch [20/150], Batch loss: 38.791
Epoch: 20 RMSE:  66.51368722887017  MAPE: 64.59431546623298  L2+L1 loss: 24.821
Epoch [21/150], Batch loss: 40.148
Epoch: 21 RMSE:  66.53298504188874  MAPE: 67.87072514680334  L2+L1 loss: 25.272
Epoch [22/150], Batch loss: 39.848
Epoch: 22 RMSE:  66.52685946657604  MAPE: 67.09241715497686  L2+L1 loss: 25.166
Epoch [23/150], Batch loss: 38.606
Epoch: 23 RMSE:  66.52952689472862  MAPE: 57.594854974411284  L2+L1 loss: 23.912
Epoch [24/150], Batch loss: 39.299
Epoch: 24 RMSE:  66.65748493993176  MAPE: 76.15397610605172  L2+L1 loss: 26.514
Epoch [25/150], Batch loss: 41.156
Epoch: 25 RMSE:  66.52072253780085  MAPE: 66.14909751125035  L2+L1 loss: 25.036
Epoch [26/150], Batch loss: 39.284
Epoch: 26 RMSE:  66.51838948321465  MAPE: 65.71834564151987  L2+L1 loss: 24.977
Epoch [27/150], Batch loss: 39.079
Epoch: 27 RMSE:  46.64863411271979  MAPE: 61.0670908023151  L2+L1 loss: 19.642
Epoch [28/150], Batch loss: 43.294
Epoch: 28 RMSE:  66.93552759659995  MAPE: 39.36133635778878  L2+L1 loss: 21.913
Epoch [29/150], Batch loss: 39.565
Epoch: 29 RMSE:  66.62593115585558  MAPE: 50.44677046664908  L2+L1 loss: 23.021
Epoch [30/150], Batch loss: 38.067
Epoch: 30 RMSE:  66.60996449373383  MAPE: 51.311530870210234  L2+L1 loss: 23.128
Epoch [31/150], Batch loss: 39.055
Epoch: 31 RMSE:  66.59118055476623  MAPE: 52.422975100305244  L2+L1 loss: 23.264
Epoch [32/150], Batch loss: 39.043
Epoch: 32 RMSE:  66.57633330371253  MAPE: 53.39656832939567  L2+L1 loss: 23.383
Epoch [33/150], Batch loss: 38.193
Epoch: 33 RMSE:  66.55958703631852  MAPE: 54.63743268339096  L2+L1 loss: 23.534
Epoch [34/150], Batch loss: 39.21
Epoch: 34 RMSE:  66.54637551717093  MAPE: 55.775675266159105  L2+L1 loss: 23.68
Epoch [35/150], Batch loss: 38.962
Epoch: 35 RMSE:  66.53462330855818  MAPE: 56.981235412067164  L2+L1 loss: 23.834
Epoch [36/150], Batch loss: 38.958
Epoch: 36 RMSE:  66.52626150925546  MAPE: 58.0318736083706  L2+L1 loss: 23.968
Epoch [37/150], Batch loss: 39.304
Epoch: 37 RMSE:  66.5162371405508  MAPE: 59.77859834314683  L2+L1 loss: 24.19
Epoch [38/150], Batch loss: 37.858
Epoch: 38 RMSE:  65.0252166138964  MAPE: 21.43134562313359  L2+L1 loss: 17.232
Epoch [39/150], Batch loss: 33.551
Epoch: 39 RMSE:  64.21675706592862  MAPE: 7.99847137374292  L2+L1 loss: 12.862
Epoch [40/150], Batch loss: 31.527
Epoch: 40 RMSE:  63.58407262690892  MAPE: 3.5726579990949556  L2+L1 loss: 11.98
Epoch [41/150], Batch loss: 30.781
Epoch: 41 RMSE:  62.983636717900424  MAPE: 1.6146281553632467  L2+L1 loss: 10.566
Epoch [42/150], Batch loss: 27.702
Epoch: 42 RMSE:  37.195534891225485  MAPE: 1.5221442825587812  L2+L1 loss: 6.673
Epoch [43/150], Batch loss: 22.551
Epoch: 43 RMSE:  26.00241226207853  MAPE: 1.8692907895811983  L2+L1 loss: 11.326
Epoch [44/150], Batch loss: 17.627
Epoch: 44 RMSE:  6.921899331544748  MAPE: 1.5897670917696798  L2+L1 loss: 3.15
Epoch [45/150], Batch loss: 6.623
Epoch: 45 RMSE:  5.680634449755438  MAPE: 0.8482649904067716  L2+L1 loss: 2.532
Epoch [46/150], Batch loss: 8.657
Epoch: 46 RMSE:  22.77180840593386  MAPE: 5.454049335431873  L2+L1 loss: 5.333
Epoch [47/150], Batch loss: 12.945
Epoch: 47 RMSE:  16.44655974292918  MAPE: 8.015992496328533  L2+L1 loss: 9.551
Epoch [48/150], Batch loss: 14.865
Epoch: 48 RMSE:  15.771782047747543  MAPE: 3.322018522842637  L2+L1 loss: 5.42
Epoch [49/150], Batch loss: 7.942
Epoch: 49 RMSE:  6.0074427488362545  MAPE: 2.338195431449997  L2+L1 loss: 2.929
Epoch [50/150], Batch loss: 5.64
Epoch: 50 RMSE:  4.2137708106661815  MAPE: 0.7038748766110259  L2+L1 loss: 1.733
Epoch [51/150], Batch loss: 6.837
Epoch: 51 RMSE:  14.941651193166875  MAPE: 10.16271906799769  L2+L1 loss: 5.244
Epoch [52/150], Batch loss: 9.589
Epoch: 52 RMSE:  9.36361493626103  MAPE: 21.16766790329947  L2+L1 loss: 7.19
Epoch [53/150], Batch loss: 5.74
Epoch: 53 RMSE:  10.304733820390428  MAPE: 0.7965452303639878  L2+L1 loss: 3.057
Epoch [54/150], Batch loss: 6.079
Epoch: 54 RMSE:  4.556953603752754  MAPE: 2.8523951064976725  L2+L1 loss: 2.166
Epoch [55/150], Batch loss: 9.602
Epoch: 55 RMSE:  24.33166083694842  MAPE: 1.877859703833965  L2+L1 loss: 7.305
Epoch [56/150], Batch loss: 17.739
Epoch: 56 RMSE:  19.621806603902073  MAPE: 15.448325695905641  L2+L1 loss: 7.386
Epoch [57/150], Batch loss: 18.425
Epoch: 57 RMSE:  12.12116832541554  MAPE: 32.869491436533515  L2+L1 loss: 9.468
Epoch [58/150], Batch loss: 10.26
Epoch: 58 RMSE:  6.457133268154307  MAPE: 15.911570761894552  L2+L1 loss: 4.287
Epoch [59/150], Batch loss: 8.228
Epoch: 59 RMSE:  11.85760107599406  MAPE: 17.959169714643114  L2+L1 loss: 9.395
Epoch [60/150], Batch loss: 5.832
Epoch: 60 RMSE:  4.9069973795403055  MAPE: 3.162779694444986  L2+L1 loss: 3.157
Epoch [61/150], Batch loss: 4.162
Epoch: 61 RMSE:  4.516631278977661  MAPE: 4.015172533165887  L2+L1 loss: 3.082
Epoch [62/150], Batch loss: 3.541
Epoch: 62 RMSE:  3.635569901544398  MAPE: 4.633303184427806  L2+L1 loss: 2.356
Epoch [63/150], Batch loss: 3.485
Epoch: 63 RMSE:  3.926325514832779  MAPE: 2.6071153376001153  L2+L1 loss: 2.308
Epoch [64/150], Batch loss: 3.015
Epoch: 64 RMSE:  2.944093118630025  MAPE: 5.329627421456913  L2+L1 loss: 2.092
Epoch [65/150], Batch loss: 2.838
Epoch: 65 RMSE:  2.9830482773243605  MAPE: 1.6429733908545192  L2+L1 loss: 1.997
Epoch [66/150], Batch loss: 2.834
Epoch: 66 RMSE:  2.7769655760361727  MAPE: 4.337078428142666  L2+L1 loss: 2.018
Epoch [67/150], Batch loss: 2.843
Epoch: 67 RMSE:  2.4434743570112363  MAPE: 2.628898976787167  L2+L1 loss: 2.0
Epoch [68/150], Batch loss: 3.561
Epoch: 68 RMSE:  2.575738556839198  MAPE: 0.7633723170416682  L2+L1 loss: 1.561
Epoch [69/150], Batch loss: 2.665
Epoch: 69 RMSE:  2.1503114240182737  MAPE: 0.7479481082526804  L2+L1 loss: 1.455
Epoch [70/150], Batch loss: 2.87
Epoch: 70 RMSE:  3.1137624353040145  MAPE: 2.9599464843667715  L2+L1 loss: 1.924
Epoch [71/150], Batch loss: 2.849
Epoch: 71 RMSE:  2.1599222437413914  MAPE: 0.7185162365373892  L2+L1 loss: 1.413
Epoch [72/150], Batch loss: 2.775
Epoch: 72 RMSE:  3.784904366682359  MAPE: 4.776202239420157  L2+L1 loss: 2.706
Epoch [73/150], Batch loss: 2.676
Epoch: 73 RMSE:  2.5854272825395426  MAPE: 0.5796318319685685  L2+L1 loss: 1.408
Epoch [74/150], Batch loss: 2.591
Epoch: 74 RMSE:  3.682037842906934  MAPE: 1.2474679329537728  L2+L1 loss: 1.792
Epoch [75/150], Batch loss: 3.143
Epoch: 75 RMSE:  4.366428023401234  MAPE: 5.5936087449037055  L2+L1 loss: 2.571
Epoch [76/150], Batch loss: 2.735
Epoch: 76 RMSE:  3.1309636074398846  MAPE: 0.7249294228443339  L2+L1 loss: 1.611
Epoch [77/150], Batch loss: 2.556
Epoch: 77 RMSE:  3.6894754536016587  MAPE: 1.641875679892677  L2+L1 loss: 1.973
Epoch [78/150], Batch loss: 2.807
Epoch: 78 RMSE:  2.381858067467401  MAPE: 3.0994778383476453  L2+L1 loss: 1.858
Epoch [79/150], Batch loss: 2.388
Epoch: 79 RMSE:  3.4510272642406097  MAPE: 0.5007910456674649  L2+L1 loss: 1.558
Epoch [80/150], Batch loss: 2.752
Epoch: 80 RMSE:  4.0323890133162195  MAPE: 2.0239122315727585  L2+L1 loss: 1.969
Epoch [81/150], Batch loss: 2.834
Epoch: 81 RMSE:  3.1227397341451706  MAPE: 0.5006653134128802  L2+L1 loss: 1.43
Epoch [82/150], Batch loss: 2.259
Epoch: 82 RMSE:  5.348521367708734  MAPE: 0.5642620713207014  L2+L1 loss: 1.761
Epoch [83/150], Batch loss: 3.113
Epoch: 83 RMSE:  3.701276116169644  MAPE: 0.9326420008371364  L2+L1 loss: 2.327
Epoch [84/150], Batch loss: 3.903
Epoch: 84 RMSE:  3.253012805590557  MAPE: 3.8349859254199496  L2+L1 loss: 2.169
Epoch [85/150], Batch loss: 2.598
Epoch: 85 RMSE:  2.134709285442946  MAPE: 1.1594061874712063  L2+L1 loss: 1.392
Epoch [86/150], Batch loss: 2.36
Epoch: 86 RMSE:  2.5826557632409317  MAPE: 1.0041736680441093  L2+L1 loss: 1.524
Epoch [87/150], Batch loss: 2.511
Epoch: 87 RMSE:  2.449284261936573  MAPE: 2.019043263415796  L2+L1 loss: 1.762
Epoch [88/150], Batch loss: 2.291
Epoch: 88 RMSE:  2.9516546182044316  MAPE: 1.7231713388025294  L2+L1 loss: 1.692
Epoch [89/150], Batch loss: 2.828
Epoch: 89 RMSE:  3.6529984385628986  MAPE: 4.189253823714774  L2+L1 loss: 2.121
Epoch [90/150], Batch loss: 2.319
Epoch: 90 RMSE:  2.3173090503966884  MAPE: 0.6201087350370893  L2+L1 loss: 1.355
Epoch [91/150], Batch loss: 2.048
Epoch: 91 RMSE:  2.2189272548147394  MAPE: 0.46447374864881746  L2+L1 loss: 1.264
Epoch [92/150], Batch loss: 2.032
Epoch: 92 RMSE:  2.2471283042999333  MAPE: 0.4706690821085461  L2+L1 loss: 1.257
Epoch [93/150], Batch loss: 2.081
Epoch: 93 RMSE:  2.180191434682212  MAPE: 0.6251341049249467  L2+L1 loss: 1.316
Epoch [94/150], Batch loss: 2.023
Epoch: 94 RMSE:  2.2971701818201575  MAPE: 0.49222769843762654  L2+L1 loss: 1.292
Epoch [95/150], Batch loss: 2.054
Epoch: 95 RMSE:  2.175223693642735  MAPE: 0.6638884661161898  L2+L1 loss: 1.298
Epoch [96/150], Batch loss: 2.041
Epoch: 96 RMSE:  2.2528546673503005  MAPE: 0.7375263437126983  L2+L1 loss: 1.365
Epoch [97/150], Batch loss: 2.026
Epoch: 97 RMSE:  2.146834423149509  MAPE: 0.45531891171105027  L2+L1 loss: 1.252
Epoch [98/150], Batch loss: 1.989
Epoch: 98 RMSE:  2.1382523993192613  MAPE: 0.45663650972725467  L2+L1 loss: 1.246
Epoch [99/150], Batch loss: 2.044
Epoch: 99 RMSE:  2.2543793854933107  MAPE: 0.45776549016883866  L2+L1 loss: 1.266
Epoch [100/150], Batch loss: 2.044
Epoch: 100 RMSE:  2.15250813781421  MAPE: 0.5613023905666178  L2+L1 loss: 1.286
Epoch [101/150], Batch loss: 2.023
Epoch: 101 RMSE:  2.2065592135565524  MAPE: 0.46737349680799734  L2+L1 loss: 1.225
Epoch [102/150], Batch loss: 2.057
Epoch: 102 RMSE:  2.262924291971805  MAPE: 0.6901666630753229  L2+L1 loss: 1.353
Epoch [103/150], Batch loss: 1.993
Epoch: 103 RMSE:  2.1130414593960425  MAPE: 0.4508222294056963  L2+L1 loss: 1.226
Epoch [104/150], Batch loss: 1.982
Epoch: 104 RMSE:  2.20071691279602  MAPE: 0.5197889350566117  L2+L1 loss: 1.274
Epoch [105/150], Batch loss: 1.932
Epoch: 105 RMSE:  2.0977136578146682  MAPE: 0.4833628824159005  L2+L1 loss: 1.232
Epoch [106/150], Batch loss: 1.982
Epoch: 106 RMSE:  2.1023369819243682  MAPE: 0.5153960602953042  L2+L1 loss: 1.248
Epoch [107/150], Batch loss: 1.998
Epoch: 107 RMSE:  2.1298001355555463  MAPE: 0.5334555160386056  L2+L1 loss: 1.265
Epoch [108/150], Batch loss: 2.011
Epoch: 108 RMSE:  2.0957825466409568  MAPE: 0.7924774354988469  L2+L1 loss: 1.353
Epoch [109/150], Batch loss: 1.986
Epoch: 109 RMSE:  2.067050235946582  MAPE: 0.4413259712657355  L2+L1 loss: 1.226
Epoch [110/150], Batch loss: 1.996
Epoch: 110 RMSE:  2.1204240092942084  MAPE: 0.4426492251132927  L2+L1 loss: 1.237
Epoch [111/150], Batch loss: 2.002
Epoch: 111 RMSE:  2.0657858710098504  MAPE: 0.432189700244709  L2+L1 loss: 1.225
Epoch [112/150], Batch loss: 1.981
Epoch: 112 RMSE:  2.132803576081697  MAPE: 0.4995813197878949  L2+L1 loss: 1.247
Epoch [113/150], Batch loss: 2.021
Epoch: 113 RMSE:  2.3477044587641602  MAPE: 0.45403352329700825  L2+L1 loss: 1.313
Epoch [114/150], Batch loss: 1.978
Epoch: 114 RMSE:  2.0348617444320896  MAPE: 0.6281887699482492  L2+L1 loss: 1.254
Epoch [115/150], Batch loss: 2.01
Epoch: 115 RMSE:  2.058108453126989  MAPE: 0.449044243109319  L2+L1 loss: 1.186
Epoch [116/150], Batch loss: 1.94
Epoch: 116 RMSE:  2.05310638222978  MAPE: 0.4127331250153318  L2+L1 loss: 1.18
Epoch [117/150], Batch loss: 1.895
Epoch: 117 RMSE:  2.048370989818406  MAPE: 0.4040717763311127  L2+L1 loss: 1.218
Epoch [118/150], Batch loss: 1.957
Epoch: 118 RMSE:  2.0613815675796667  MAPE: 0.42225666693460356  L2+L1 loss: 1.232
Epoch [119/150], Batch loss: 1.963
Epoch: 119 RMSE:  2.047351345210077  MAPE: 0.4090866217674024  L2+L1 loss: 1.217
Epoch [120/150], Batch loss: 1.962
Epoch: 120 RMSE:  2.0343174911831787  MAPE: 0.4099186242301137  L2+L1 loss: 1.21
Epoch [121/150], Batch loss: 1.946
Epoch: 121 RMSE:  2.021657382193673  MAPE: 0.4212042928451172  L2+L1 loss: 1.205
Epoch [122/150], Batch loss: 1.93
Epoch: 122 RMSE:  2.0243579347905967  MAPE: 0.40874643488610624  L2+L1 loss: 1.204
Epoch [123/150], Batch loss: 1.931
Epoch: 123 RMSE:  2.0222876034877144  MAPE: 0.4075071385414038  L2+L1 loss: 1.201
Epoch [124/150], Batch loss: 1.908
Epoch: 124 RMSE:  2.021276555974211  MAPE: 0.4072465786609132  L2+L1 loss: 1.2
Epoch [125/150], Batch loss: 1.907
Epoch: 125 RMSE:  2.019192343771203  MAPE: 0.40669237636267197  L2+L1 loss: 1.198
Epoch [126/150], Batch loss: 1.898
Epoch: 126 RMSE:  2.019899547945227  MAPE: 0.4069761558593564  L2+L1 loss: 1.199
Epoch [127/150], Batch loss: 1.924
Epoch: 127 RMSE:  2.018104593181973  MAPE: 0.4067703512247828  L2+L1 loss: 1.197
Epoch [128/150], Batch loss: 1.866
Epoch: 128 RMSE:  2.0173902877712497  MAPE: 0.4064754673253881  L2+L1 loss: 1.197
Epoch [129/150], Batch loss: 1.916
Epoch: 129 RMSE:  2.0179919674513314  MAPE: 0.4109699431925694  L2+L1 loss: 1.199
Epoch [130/150], Batch loss: 1.916
Epoch: 130 RMSE:  2.0211162587204843  MAPE: 0.4057786934556023  L2+L1 loss: 1.201
Epoch [131/150], Batch loss: 1.891
Epoch: 131 RMSE:  2.025361081228481  MAPE: 0.40423082995313153  L2+L1 loss: 1.205
Epoch [132/150], Batch loss: 1.911
Epoch: 132 RMSE:  2.0237303186743274  MAPE: 0.4047401415285524  L2+L1 loss: 1.203
Epoch [133/150], Batch loss: 1.901
Epoch: 133 RMSE:  2.0203282068084403  MAPE: 0.4069561156925001  L2+L1 loss: 1.2
Epoch [134/150], Batch loss: 1.915
Epoch: 134 RMSE:  2.019301856621895  MAPE: 0.404134059560182  L2+L1 loss: 1.198
Epoch [135/150], Batch loss: 1.872
Epoch: 135 RMSE:  2.023651208310627  MAPE: 0.4036450007391506  L2+L1 loss: 1.202
Epoch [136/150], Batch loss: 1.953
Epoch: 136 RMSE:  2.027464662506868  MAPE: 0.40372484404781384  L2+L1 loss: 1.205
Epoch [137/150], Batch loss: 1.921
Epoch: 137 RMSE:  2.02386647565758  MAPE: 0.40506810075440486  L2+L1 loss: 1.202
Epoch [138/150], Batch loss: 1.903
Epoch: 138 RMSE:  2.0235458512929854  MAPE: 0.4030519794733205  L2+L1 loss: 1.201
Epoch [139/150], Batch loss: 1.898
Epoch: 139 RMSE:  2.0216639215220735  MAPE: 0.40710927103538747  L2+L1 loss: 1.201
Epoch [140/150], Batch loss: 1.901
Epoch: 140 RMSE:  2.024207678521424  MAPE: 0.4022965971688019  L2+L1 loss: 1.201
Epoch [141/150], Batch loss: 1.914
Epoch: 141 RMSE:  2.0212491804283617  MAPE: 0.4046494457839934  L2+L1 loss: 1.199
Epoch [142/150], Batch loss: 1.918
Epoch: 142 RMSE:  2.0230353682561053  MAPE: 0.4016802660199273  L2+L1 loss: 1.2
Epoch [143/150], Batch loss: 1.901
Epoch: 143 RMSE:  2.0230994407840908  MAPE: 0.4021744284315583  L2+L1 loss: 1.201
Epoch [144/150], Batch loss: 1.926
Epoch: 144 RMSE:  2.025038797142755  MAPE: 0.40140714231595814  L2+L1 loss: 1.202
Epoch [145/150], Batch loss: 1.903
Epoch: 145 RMSE:  2.026151184402401  MAPE: 0.4021514586367912  L2+L1 loss: 1.202
Epoch [146/150], Batch loss: 1.894
Epoch: 146 RMSE:  2.017762336079222  MAPE: 0.40156242972187767  L2+L1 loss: 1.191
Epoch [147/150], Batch loss: 1.948
Epoch: 147 RMSE:  2.0184214890276637  MAPE: 0.40385068868700713  L2+L1 loss: 1.194
Epoch [148/150], Batch loss: 1.876
Epoch: 148 RMSE:  2.0214351589804656  MAPE: 0.40352155542945956  L2+L1 loss: 1.2
Epoch [149/150], Batch loss: 1.894
Epoch: 149 RMSE:  2.0210690655071697  MAPE: 0.40034234829747645  L2+L1 loss: 1.197


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
20.160238 , 20.6817
0.56828785 , 0.4669
0.5576792 , 0.6452
37.08492 , 34.5608
0.031105995 , 0.0844
27.894075 , 30.6888
38.74832 , 39.413
19.111828 , 19.991
0.29938126 , 0.3367
7.799633 , 9.1236
0.02142334 , 0.1215
0.17153549 , 0.1019
37.667015 , 39.2624
0.6223974 , 0.6036
50.32926 , 46.142
0.109256744 , 0.1244
29.46676 , 34.2435
0.22329998 , 0.1805
0.060048103 , 0.0368
0.34612274 , 0.3088
0.29552555 , 0.3925
53.711205 , 56.0372
0.036354065 , 0.0881
10.4591675 , 11.343
22.33245 , 22.9641
59.50894 , 59.6632
16.844343 , 18.0744
56.595757 , 52.3654
23.631977 , 22.8996
0.44888496 , 0.3849
38.39103 , 35.845
821.3173 , 817.5672
0.3535185 , 0.4182
20.520964 , 18.856
32.00917 , 29.6984
1.0982566 , 0.8888
15.7917385 , 16.8629
79.046555 , 71.6403
0.0 , 0.0559
12.083835 , 14.0542
0.48570538 , 0.4127
14.292793 , 13.8617
6.3320765 , 5.839
0.0 , 0.0527
0.18430614 , 0.1728
64.62316 , 69.2234
4.29291 , 5.595
0.0 , 0.0215
35.232758 , 33.7098
0.57942677 , 0.7442
0.28137684 , 0.1897
0.18529892 , 0.2114
27.394539 , 32.3022
13.536719 , 14.0577
0.44176006 , 0.4016
0.6941862 , 0.7725
30.420858 , 35.9743
0.64645195 , 0.4821
11.650044 , 12.4966
50.892555 , 43.611
23.614328 , 22.6627
32.253082 , 34.8208
0.75719976 , 0.9689
13.193645 , 12.344
0.0736084 , 0.1026
5.8008585 , 7.7457
0.55391026 , 0.3981
12.931721 , 14.9069
7.7139378 , 7.7995
1.1848307 , 1.3756
47.125587 , 46.7605
30.456871 , 34.4114
6.887082 , 6.4975
0.65619373 , 0.6798
38.882168 , 36.7537
0.19604683 , 0.1973
0.19717312 , 0.3595
0.11728287 , 0.0634
10.7809925 , 11.8537
0.35069084 , 0.3418
24.453775 , 25.4645
39.02437 , 40.52
6.686283 , 6.3939
7.9444127 , 8.3339
0.0 , 0.0205
46.0271 , 44.8337
712.4236 , 753.9252
53.23017 , 44.5942
23.643917 , 24.7044
0.30479527 , 0.1186
9.438401 , 10.2085
20.076584 , 20.0587
38.52745 , 36.4525
29.663494 , 28.0615
16.40661 , 15.2436
61.039482 , 57.4479
43.023796 , 40.6346
18.108652 , 19.2215
0.31234074 , 0.4181
6.1355505 , 6.06
18.049295 , 17.2543
0.33813 , 0.2548
0.73065376 , 0.971
0.5894022 , 0.6415
10.90036 , 10.5128
0.17339039 , 0.1723
10.7147255 , 12.4376
21.208057 , 19.952
16.768684 , 16.5427
0.3432207 , 0.1256
15.960127 , 14.5023
1.167809 , 0.8793
61.51799 , 61.7377
9.771471 , 8.9356
37.43714 , 38.8987
39.335022 , 32.2352
39.95918 , 38.3466
0.58328056 , 0.6032
0.23596668 , 0.2456
0.16400623 , 0.2166
0.1734209 , 0.3469
0.17730236 , 0.1214
0.22767448 , 0.2995
0.01651287 , 0.0834
32.77447 , 31.3617
42.795876 , 40.1973
6.749645 , 6.0873
0.48206234 , 0.4987
36.703827 , 35.3036
0.0 , 0.0505
0.38949108 , 0.3752
58.719055 , 54.3432
33.650955 , 33.5134
6.458006 , 6.1883
0.2803173 , 0.4104
37.78211 , 34.6417
42.01985 , 44.4216
8.276647 , 7.2695
0.8917203 , 0.808
0.38253403 , 0.3984
16.420961 , 16.2233
61.946716 , 59.6045
0.28388023 , 0.3823
28.772774 , 27.1666
19.594864 , 19.9699
0.07124996 , 0.0781
0.81492805 , 0.7576
0.39579773 , 0.3694
37.79988 , 39.0645
0.22859383 , 0.2939
0.19929123 , 0.0988
62.093086 , 70.8936
0.23571396 , 0.3107
0.110957146 , 0.1124
0.34796143 , 0.305
0.13755512 , 0.158
8.500145 , 9.1087
0.27071953 , 0.2191
0.16296387 , 0.2963
0.73283243 , 0.7331
0.7988291 , 0.6598
21.93539 , 23.5542
0.23779774 , 0.1682
26.693573 , 29.522
0.29622173 , 0.2711
28.42156 , 30.2026
0.4063406 , 0.3098
0.569437 , 0.6064
15.338988 , 16.1715
31.645166 , 33.7013
17.537657 , 16.2597
10.20867 , 9.432
18.374933 , 19.5555
5.4578233 , 6.0238
31.10135 , 31.6973
0.0 , 0.0607
0.0 , 0.0484
1.2106042 , 0.9573
0.0 , 0.0358
0.43148518 , 0.2632
0.6906605 , 0.683
0.1814785 , 0.2943
0.3521471 , 0.3245
0.0 , 0.0744
27.473204 , 25.7239
0.11359882 , 0.128
0.25313187 , 0.2981
36.127357 , 37.5478
0.2225914 , 0.1036
1.3948312 , 1.6834
24.849628 , 24.2583
24.024242 , 24.9199
18.976173 , 20.254
23.886456 , 26.1395
0.5861149 , 0.5432
0.7439604 , 1.0335
8.606633 , 9.9217
25.805067 , 25.5614
18.586597 , 20.219
20.19802 , 23.7326
20.310358 , 22.3467
0.21695709 , 0.1549
0.93135166 , 1.2492
10.644654 , 11.6887
0.26236534 , 0.219
15.199278 , 14.3914
24.250492 , 22.5839
0.9586234 , 0.8786
22.079308 , 22.9074
0.3069582 , 0.2579
14.748108 , 16.0346
4.014122 , 4.6912
0.20549011 , 0.0747
0.49106598 , 0.3984
0.53631973 , 0.511
19.441425 , 20.8532
0.47437668 , 0.4988
0.15112495 , 0.1459
0.7032614 , 0.7132
17.127048 , 19.3769
0.96576643 , 1.0445
24.387646 , 20.9647
0.30148792 , 0.3426
35.6118 , 33.6991
0.23435402 , 0.1466
0.7673688 , 0.81
23.163635 , 22.7269
6.6017494 , 6.7159
1.0941153 , 1.3663
59.711685 , 57.6253
0.20531178 , 0.2448
0.12505627 , 0.1212
0.16188908 , 0.1685
32.2361 , 35.3599
4.968091 , 4.1908
0.26660442 , 0.1541
12.754146 , 13.1289
30.654083 , 29.6617
21.800894 , 19.1898
17.191795 , 17.4899
31.456415 , 28.839
0.055915833 , 0.049
0.8944721 , 1.128
4.8101535 , 3.6986
16.72429 , 17.1024
8.720154 , 8.4207
0.9346943 , 0.8518
62.302437 , 65.4319
0.1320486 , 0.0587
10.106176 , 9.6409
0.4160776 , 0.4696
32.23533 , 30.7745
0.4484558 , 0.5764
1.1459928 , 1.2379
30.847275 , 29.4515
0.27196217 , 0.0912
33.949303 , 37.9921
302.3103 , 287.8972
34.269543 , 35.6065
0.14361954 , 0.0242
1.1249394 , 0.973
0.18490124 , 0.0784
0.2059164 , 0.2792
0.10204029 , 0.1054
0.056069374 , 0.052
0.055687904 , 0.0951
13.540036 , 13.2857
0.1842289 , 0.2252
73.384995 , 76.4283
20.304636 , 21.2144
333.50394 , 334.2917
36.63694 , 41.3841
11.29141 , 10.6198
0.014365196 , 0.0686
30.86073 , 32.232
0.27536774 , 0.105
4.2572975 , 4.587
33.52761 , 31.7064
30.061295 , 28.8886
25.281467 , 22.0457
53.110718 , 51.7088
40.736534 , 43.3403
8.645725 , 10.1444
19.950514 , 19.5444
6.708022 , 7.8031
7.492648 , 7.8393
1.6520371 , 1.2367
5.567166 , 5.6568
13.979785 , 13.2532
0.46982288 , 0.5482
0.24107647 , 0.1792
0.4342661 , 0.4526
0.22333813 , 0.173
53.975998 , 55.7235
11.162076 , 11.0191
19.517235 , 17.6938
0.12863159 , 0.0742
38.240524 , 36.0001
34.22319 , 32.2686
48.910934 , 44.8536
0.12668991 , 0.1465
0.93405724 , 1.1564
27.188236 , 27.2382
0.1882124 , 0.0827
19.235657 , 16.8975
1.782495 , 1.9028
0.15417194 , 0.113
29.46264 , 31.1029
34.342163 , 35.8632
0.25827122 , 0.1073
29.26399 , 30.2284
0.5758934 , 0.5438
16.535707 , 16.155
0.493989 , 0.4128
RMSE:  3.0096702613653608  MAPE: 0.22123076015393142
5: ground truth total-  147  predicted total -  147
100: ground truth total-  163  predicted total -  163
 more 100: ground truth total -  4  predicted total -  4
