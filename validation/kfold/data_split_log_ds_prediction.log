['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1364
34
<class 'numpy.dtype'> float64
1364
train batches:  62  validate samples: 109  test samples: 272
Epoch [0/75], Batch loss: 24.539
Epoch: 0 RMSE:  8.895465731687029  MAPE: 0.5141095859726503  L2+L1 loss: 4.202
Epoch [1/75], Batch loss: 7.716
Epoch: 1 RMSE:  8.168012188644674  MAPE: 0.6817764159687174  L2+L1 loss: 3.625
Epoch [2/75], Batch loss: 7.792
Epoch: 2 RMSE:  8.081302310209004  MAPE: 1.074404440839852  L2+L1 loss: 4.395
Epoch [3/75], Batch loss: 7.551
Epoch: 3 RMSE:  8.0859818446306  MAPE: 0.9568796492943271  L2+L1 loss: 4.155
Epoch [4/75], Batch loss: 7.602
Epoch: 4 RMSE:  8.08267528101613  MAPE: 1.1038462519181143  L2+L1 loss: 4.456
Epoch [5/75], Batch loss: 8.052
Epoch: 5 RMSE:  8.083457181876218  MAPE: 1.1149792107724725  L2+L1 loss: 4.478
Epoch [6/75], Batch loss: 7.531
Epoch: 6 RMSE:  8.082571508340179  MAPE: 1.102193768932945  L2+L1 loss: 4.452
Epoch [7/75], Batch loss: 7.595
Epoch: 7 RMSE:  8.109182150041232  MAPE: 1.2714997784096065  L2+L1 loss: 4.803
Epoch [8/75], Batch loss: 7.905
Epoch: 8 RMSE:  8.082387833129562  MAPE: 0.9992446898626943  L2+L1 loss: 4.241
Epoch [9/75], Batch loss: 7.728
Epoch: 9 RMSE:  8.082793623841708  MAPE: 1.1056719439779434  L2+L1 loss: 4.459
Epoch [10/75], Batch loss: 7.897
Epoch: 10 RMSE:  8.135467098923819  MAPE: 1.3596452657060745  L2+L1 loss: 4.986
Epoch [11/75], Batch loss: 7.966
Epoch: 11 RMSE:  8.15289767679672  MAPE: 1.407227607690189  L2+L1 loss: 5.09
Epoch [12/75], Batch loss: 7.612
Epoch: 12 RMSE:  8.166703830521273  MAPE: 1.4412573036291227  L2+L1 loss: 5.166
Epoch [13/75], Batch loss: 7.997
Epoch: 13 RMSE:  8.081419858120137  MAPE: 1.0200332776051582  L2+L1 loss: 4.282
Epoch [14/75], Batch loss: 7.869
Epoch: 14 RMSE:  8.08092867250606  MAPE: 1.0498463793442179  L2+L1 loss: 4.345
Epoch [15/75], Batch loss: 23.781
Epoch: 15 RMSE:  8.100802129351157  MAPE: 1.235161530257958  L2+L1 loss: 4.726
Epoch [16/75], Batch loss: 8.01
Epoch: 16 RMSE:  8.085728412211784  MAPE: 1.1398874220312418  L2+L1 loss: 4.528
Epoch [17/75], Batch loss: 7.684
Epoch: 17 RMSE:  8.0902202116954  MAPE: 1.1756663760617516  L2+L1 loss: 4.601
Epoch [18/75], Batch loss: 7.347
Epoch: 18 RMSE:  8.092128161935625  MAPE: 1.1881718098426834  L2+L1 loss: 4.627
Epoch [19/75], Batch loss: 7.829
Epoch: 19 RMSE:  8.081245918173796  MAPE: 1.0256907102051531  L2+L1 loss: 4.294
Epoch [20/75], Batch loss: 8.026
Epoch: 20 RMSE:  8.114524380913732  MAPE: 1.2918884752798316  L2+L1 loss: 4.845
Epoch [21/75], Batch loss: 7.882
Epoch: 21 RMSE:  8.11681944554623  MAPE: 0.8072048191277419  L2+L1 loss: 3.86
Epoch [22/75], Batch loss: 7.846
Epoch: 22 RMSE:  8.101863913247685  MAPE: 1.2401412149408955  L2+L1 loss: 4.737
Epoch [23/75], Batch loss: 7.844
Epoch: 23 RMSE:  8.098352387398187  MAPE: 1.2231402453334699  L2+L1 loss: 4.701
Epoch [24/75], Batch loss: 8.085
Epoch: 24 RMSE:  8.114502798110758  MAPE: 1.291809488053048  L2+L1 loss: 4.845
Epoch [25/75], Batch loss: 8.151
Epoch: 25 RMSE:  8.087960946876326  MAPE: 0.9406900693369256  L2+L1 loss: 4.125
Epoch [26/75], Batch loss: 7.619
Epoch: 26 RMSE:  8.26283955134435  MAPE: 1.6286595112464686  L2+L1 loss: 5.587
Epoch [27/75], Batch loss: 8.128
Epoch: 27 RMSE:  8.261061296291148  MAPE: 1.64026250885652  L2+L1 loss: 5.587
Epoch [28/75], Batch loss: 8.165
Epoch: 28 RMSE:  8.202710377333032  MAPE: 0.6738799872183369  L2+L1 loss: 3.689
Epoch [29/75], Batch loss: 10.199
Epoch: 29 RMSE:  8.111012052300335  MAPE: 1.2786838094335895  L2+L1 loss: 4.818
Epoch [30/75], Batch loss: 7.912
Epoch: 30 RMSE:  8.070762597349702  MAPE: 1.0820348314774995  L2+L1 loss: 4.402
Epoch [31/75], Batch loss: 7.854
Epoch: 31 RMSE:  8.062578218770867  MAPE: 1.1066417876037227  L2+L1 loss: 4.434
Epoch [32/75], Batch loss: 7.776
Epoch: 32 RMSE:  7.873083719329152  MAPE: 1.0598239683472988  L2+L1 loss: 4.232
Epoch [33/75], Batch loss: 6.653
Epoch: 33 RMSE:  7.189121986570295  MAPE: 0.5040462731616725  L2+L1 loss: 3.086
Epoch [34/75], Batch loss: 6.228
Epoch: 34 RMSE:  6.663191804362856  MAPE: 0.29784268281185494  L2+L1 loss: 2.508
Epoch [35/75], Batch loss: 6.539
Epoch: 35 RMSE:  6.182892802640518  MAPE: 0.27637455818608353  L2+L1 loss: 2.438
Epoch [36/75], Batch loss: 6.297
Epoch: 36 RMSE:  5.866507273020949  MAPE: 0.3940083469570724  L2+L1 loss: 2.65
Epoch [37/75], Batch loss: 6.424
Epoch: 37 RMSE:  5.366362943871913  MAPE: 0.2430027257459879  L2+L1 loss: 2.086
Epoch [38/75], Batch loss: 6.085
Epoch: 38 RMSE:  5.157673070077475  MAPE: 0.930601873112496  L2+L1 loss: 3.267
Epoch [39/75], Batch loss: 5.439
Epoch: 39 RMSE:  4.516875842785057  MAPE: 0.2509182114729536  L2+L1 loss: 1.707
Epoch [40/75], Batch loss: 6.446
Epoch: 40 RMSE:  5.461539180255811  MAPE: 0.924300057078902  L2+L1 loss: 3.74
Epoch [41/75], Batch loss: 6.26
Epoch: 41 RMSE:  1.6596433508236768  MAPE: 0.37955597160543575  L2+L1 loss: 1.497
Epoch [42/75], Batch loss: 9.862
Epoch: 42 RMSE:  6.20099242372335  MAPE: 0.8280057228855066  L2+L1 loss: 3.805
Epoch [43/75], Batch loss: 6.346
Epoch: 43 RMSE:  4.9530328015188765  MAPE: 0.3092154233802236  L2+L1 loss: 2.105
Epoch [44/75], Batch loss: 4.279
Epoch: 44 RMSE:  4.093154623454278  MAPE: 0.2579931430253086  L2+L1 loss: 1.639
Epoch [45/75], Batch loss: 3.458
Epoch: 45 RMSE:  3.4882971363299555  MAPE: 0.2778192135879261  L2+L1 loss: 1.799
Epoch [46/75], Batch loss: 3.096
Epoch: 46 RMSE:  2.0375414411586794  MAPE: 0.20811829922067215  L2+L1 loss: 1.494
Epoch [47/75], Batch loss: 0.904
Epoch: 47 RMSE:  0.7255448006704406  MAPE: 0.15794864176456552  L2+L1 loss: 0.869
Epoch [48/75], Batch loss: 0.69
Epoch: 48 RMSE:  0.6988297149934604  MAPE: 0.15300014021463124  L2+L1 loss: 0.838
Epoch [49/75], Batch loss: 0.931
Epoch: 49 RMSE:  0.8842116179303295  MAPE: 0.15142526679828114  L2+L1 loss: 0.913
Epoch [50/75], Batch loss: 0.724
Epoch: 50 RMSE:  1.0242651728149106  MAPE: 0.19140836962811972  L2+L1 loss: 1.047
Epoch [51/75], Batch loss: 0.808
Epoch: 51 RMSE:  1.0519723405395027  MAPE: 0.11874823691550364  L2+L1 loss: 0.883
Epoch [52/75], Batch loss: 1.169
Epoch: 52 RMSE:  1.2414526476408763  MAPE: 0.14549192251918724  L2+L1 loss: 1.039
Epoch [53/75], Batch loss: 1.343
Epoch: 53 RMSE:  1.452839184943658  MAPE: 0.1044277362342302  L2+L1 loss: 0.983
Epoch [54/75], Batch loss: 3.606
Epoch: 54 RMSE:  2.218382341047959  MAPE: 0.3015437339208054  L2+L1 loss: 1.455
Epoch [55/75], Batch loss: 2.009
Epoch: 55 RMSE:  2.1649931972437932  MAPE: 0.3256860641025251  L2+L1 loss: 1.456
Epoch [56/75], Batch loss: 1.625
Epoch: 56 RMSE:  0.7808847981413238  MAPE: 0.14161602114738406  L2+L1 loss: 0.855
Epoch [57/75], Batch loss: 0.756
Epoch: 57 RMSE:  0.5793453118052583  MAPE: 0.0670416222164017  L2+L1 loss: 0.575
Epoch [58/75], Batch loss: 1.549
Epoch: 58 RMSE:  3.4882225963555067  MAPE: 0.17195751206375096  L2+L1 loss: 1.578
Epoch [59/75], Batch loss: 1.52
Epoch: 59 RMSE:  0.49740834002364853  MAPE: 0.10192236180226638  L2+L1 loss: 0.616
Epoch [60/75], Batch loss: 0.438
Epoch: 60 RMSE:  0.42112072682740964  MAPE: 0.08399861011723764  L2+L1 loss: 0.552
Epoch [61/75], Batch loss: 0.407
Epoch: 61 RMSE:  0.3918498886939581  MAPE: 0.07608405665141872  L2+L1 loss: 0.522
Epoch [62/75], Batch loss: 0.383
Epoch: 62 RMSE:  0.3772985394893214  MAPE: 0.07631390305961494  L2+L1 loss: 0.514
Epoch [63/75], Batch loss: 0.359
Epoch: 63 RMSE:  0.35639848858235157  MAPE: 0.06326306670322467  L2+L1 loss: 0.476
Epoch [64/75], Batch loss: 0.329
Epoch: 64 RMSE:  0.3124648621549455  MAPE: 0.05687527366161059  L2+L1 loss: 0.447
Epoch [65/75], Batch loss: 0.339
Epoch: 65 RMSE:  0.3349113770748794  MAPE: 0.05797713875846583  L2+L1 loss: 0.451
Epoch [66/75], Batch loss: 0.3
Epoch: 66 RMSE:  0.26105949134758477  MAPE: 0.05650760793954533  L2+L1 loss: 0.443
Epoch [67/75], Batch loss: 0.287
Epoch: 67 RMSE:  0.34088632784726636  MAPE: 0.05106220849238684  L2+L1 loss: 0.443
Epoch [68/75], Batch loss: 0.272
Epoch: 68 RMSE:  0.27001799292739137  MAPE: 0.06419470749844078  L2+L1 loss: 0.42
Epoch [69/75], Batch loss: 0.277
Epoch: 69 RMSE:  0.2610928065518676  MAPE: 0.051674414672760766  L2+L1 loss: 0.413
Epoch [70/75], Batch loss: 0.247
Epoch: 70 RMSE:  0.22704629227739015  MAPE: 0.04878408869987757  L2+L1 loss: 0.407
Epoch [71/75], Batch loss: 0.221
Epoch: 71 RMSE:  0.3035570321306636  MAPE: 0.04614557191165647  L2+L1 loss: 0.393
Epoch [72/75], Batch loss: 0.269
Epoch: 72 RMSE:  0.2353885429320624  MAPE: 0.042089906704331985  L2+L1 loss: 0.382
Epoch [73/75], Batch loss: 0.21
Epoch: 73 RMSE:  0.23978590443100783  MAPE: 0.045782221032761104  L2+L1 loss: 0.388
Epoch [74/75], Batch loss: 0.299
Epoch: 74 RMSE:  0.371537359535506  MAPE: 0.0423064843856413  L2+L1 loss: 0.433


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
2.8634317 , 2.9078
4.2156487 , 3.8119
2.7011096 , 2.5619
1.1620469 , 1.2065
4.0623894 , 3.9974
3.160232 , 3.4187
3.178368 , 3.1526
1.1482761 , 1.1976
2.4306722 , 2.2198
2.3777015 , 2.3052
2.6512275 , 2.647
2.7282486 , 2.7696
3.190974 , 3.1857
1.3220571 , 1.2242
3.2351778 , 3.2649
2.4455748 , 2.3918
2.460156 , 2.3847
1.643888 , 1.5664
2.8785148 , 2.9283
1.8506354 , 1.755
2.2845159 , 2.2938
24.182396 , 25.7658
8.215642 , 8.4028
2.717482 , 2.752
2.808836 , 2.8084
9.082491 , 9.2124
4.8407702 , 4.724
7.7306166 , 8.0493
5.4892635 , 5.878
2.0105112 , 1.9566
3.6329339 , 3.5233
23.26171 , 25.1461
2.7817793 , 2.9047
3.3461862 , 3.3566
2.6874661 , 2.5655
2.5468256 , 2.5572
3.0928688 , 3.1221
2.8557599 , 2.9289
1.9647859 , 1.8405
1.4498003 , 1.2819
1.5733871 , 1.5306
1.8497202 , 1.931
3.0575085 , 3.1946
3.038889 , 3.3001
1.334334 , 1.3344
2.5891569 , 2.6243
1.7512016 , 1.8216
27.466276 , 28.4275
1.4957073 , 1.3972
3.4368181 , 3.4612
3.1663547 , 3.2372
2.6648827 , 2.6181
2.429995 , 2.4797
7.379238 , 7.7326
2.4796574 , 2.4556
4.3898983 , 4.1365
4.6488676 , 4.6992
2.765215 , 2.8337
1.5609943 , 1.754
2.6280859 , 2.7898
3.8086405 , 3.5967
2.7380216 , 2.8898
1.6149805 , 1.2655
3.12168 , 3.161
1.8344663 , 1.5816
2.9612925 , 3.0113
1.5638847 , 1.5679
2.5541937 , 2.5433
5.216519 , 5.1173
2.261395 , 2.3199
2.8369877 , 2.9105
3.267556 , 3.5528
2.7509394 , 2.8282
1.6820515 , 1.7393
0.9466244 , 1.0011
1.567533 , 1.5666
3.2615504 , 3.2983
8.161393 , 7.9796
2.9596698 , 2.9399
1.5313013 , 1.3995
6.1085367 , 5.9437
5.6184616 , 5.543
2.7544804 , 2.9887
3.166504 , 3.3429
2.9917784 , 2.9059
2.5091016 , 2.5274
30.21959 , 31.738
2.5226123 , 2.4478
3.145507 , 3.1733
5.458302 , 5.9514
3.4904995 , 3.3776
1.8970667 , 1.8792
3.01353 , 3.0463
1.2212774 , 1.1403
2.2680302 , 2.0837
6.8748226 , 6.6155
1.2825192 , 1.363
29.946257 , 30.7215
2.9944675 , 3.0959
5.970319 , 5.8497
1.2516527 , 1.326
3.5822978 , 3.462
4.759033 , 5.7248
5.0570126 , 4.3996
3.186811 , 3.2349
2.9910176 , 3.0703
3.4429984 , 3.2718
6.536002 , 6.5687
2.6412265 , 2.5411
2.9189057 , 3.1086
1.3819251 , 1.2763
1.9125671 , 1.7903
2.739734 , 2.8262
1.4348161 , 1.3962
0.89938486 , 1.0718
11.049378 , 11.175
3.7222517 , 3.62
3.477438 , 3.4625
7.756422 , 7.3993
2.2781785 , 2.2171
5.185049 , 5.2916
3.1078327 , 3.255
4.0474663 , 3.8957
7.1410055 , 7.7079
5.3288293 , 5.8668
3.4434576 , 3.2781
5.68549 , 5.623
2.7419467 , 2.494
3.0983648 , 3.0184
2.9985538 , 2.9196
1.430765 , 1.2969
3.7742121 , 3.7382
2.9376993 , 2.9817
3.6826136 , 3.4479
3.7701118 , 3.5629
7.3500395 , 8.0206
3.1705155 , 3.1603
3.3027062 , 3.3066
6.738677 , 6.7234
1.5096184 , 1.6738
2.8556619 , 2.9464
22.418535 , 23.555
1.8689785 , 1.8164
4.107648 , 4.2085
2.4946585 , 2.4413
2.7583187 , 3.1139
3.4582663 , 3.2799
3.9048636 , 3.7701
9.327397 , 9.4687
2.7180386 , 2.7733
3.4009337 , 3.3901
2.364354 , 2.4835
3.7877786 , 3.5875
1.3232734 , 1.3119
1.3366046 , 1.4827
6.1309376 , 5.6394
6.1530457 , 6.7519
3.0881753 , 3.2341
6.5940742 , 7.1509
8.004271 , 8.0418
4.8880014 , 3.9268
26.237661 , 27.9218
4.086808 , 3.8399
3.8936517 , 3.8271
4.9300127 , 5.0367
1.1593963 , 1.1919
1.1734407 , 1.3039
3.372865 , 3.3481
2.9839392 , 2.9869
3.0824964 , 3.0621
3.463367 , 3.2802
1.3561926 , 1.0891
2.091596 , 2.0046
3.8869407 , 3.6496
2.766955 , 2.9382
2.961846 , 2.9872
1.7831011 , 1.8419
2.64262 , 2.6788
2.9503388 , 2.8313
4.098448 , 3.86
4.647257 , 4.7769
4.4074736 , 4.3419
3.848875 , 3.9638
3.6145892 , 3.5182
2.9460182 , 2.914
1.5128485 , 1.4988
3.095004 , 3.3297
4.931573 , 4.974
2.8794525 , 2.7148
3.2872343 , 3.1544
10.658284 , 10.6738
2.9308407 , 2.8432
1.0702794 , 1.0922
2.4101713 , 2.3782
4.152934 , 4.0023
6.2870874 , 6.1849
1.2047268 , 1.0693
3.8311372 , 3.8618
2.647599 , 2.5855
3.453039 , 3.4781
67.847404 , 71.2594
30.981117 , 31.5847
3.1792474 , 3.1492
2.9280276 , 3.3369
2.933214 , 3.0544
2.82211 , 2.9289
2.8828957 , 2.9176
8.514469 , 7.9408
1.4212513 , 1.2601
3.6009066 , 3.4413
2.7011096 , 2.7976
8.174222 , 8.1059
2.7765431 , 2.6057
1.6558287 , 1.7083
7.0351944 , 6.71
3.0807176 , 3.2694
3.0453343 , 3.1368
3.069247 , 3.1404
1.8466783 , 2.053
4.9741936 , 4.7148
4.429406 , 4.3961
2.8242633 , 2.9163
2.5512836 , 2.3525
3.1299381 , 3.1732
5.166832 , 4.9984
4.6048145 , 3.7595
1.5785496 , 1.524
2.971605 , 2.886
2.8388796 , 2.8398
2.1571774 , 2.2439
2.669665 , 2.6021
1.504071 , 1.4263
2.8715386 , 2.8334
7.0233107 , 7.7557
33.35513 , 34.3045
7.163903 , 7.2682
4.1494875 , 4.1678
3.6238294 , 3.3786
2.2860298 , 2.3776
2.3122993 , 2.4061
2.276887 , 2.262
2.8189783 , 2.8654
4.918049 , 4.8259
5.883211 , 6.4456
2.7932894 , 2.6985
30.174006 , 31.0349
2.7822223 , 2.6552
2.921536 , 2.9575
63.697906 , 66.1756
2.5475671 , 2.4871
2.3741465 , 2.3248
6.2331753 , 6.6719
1.2116365 , 1.1586
7.9126644 , 7.9716
3.8279335 , 4.1494
3.0250015 , 3.3342
4.1420264 , 4.223
1.8514025 , 1.6242
22.565329 , 23.4497
2.5026662 , 2.7062
20.791351 , 21.2415
2.4109893 , 2.2252
1.4122913 , 1.4892
9.29503 , 9.5043
4.4218345 , 5.7981
6.3362193 , 6.5646
4.4574347 , 4.1779
3.539821 , 3.3122
3.0549047 , 3.1001
4.779043 , 5.1358
1.8390932 , 1.8498
3.0492253 , 2.9552
RMSE:  0.4208563782359157  MAPE: 0.04459768609446938
5: ground truth total-  216  predicted total -  216
100: ground truth total-  56  predicted total -  56
 more 100: ground truth total -  0  predicted total -  0
