['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1349
34
<class 'numpy.dtype'> float64
1349
train batches:  61  validate samples: 107  test samples: 269
Epoch [0/75], Batch loss: 25.177
Epoch: 0 RMSE:  15.078619421950188  MAPE: 0.9740554805788592  L2+L1 loss: 7.503
Epoch [1/75], Batch loss: 8.525
Epoch: 1 RMSE:  14.758873653646676  MAPE: 0.72059528749062  L2+L1 loss: 6.773
Epoch [2/75], Batch loss: 7.849
Epoch: 2 RMSE:  14.42641585494904  MAPE: 0.49532395444089744  L2+L1 loss: 5.999
Epoch [3/75], Batch loss: 7.732
Epoch: 3 RMSE:  14.121576039194089  MAPE: 0.43955542032544986  L2+L1 loss: 5.492
Epoch [4/75], Batch loss: 7.23
Epoch: 4 RMSE:  13.855712372861996  MAPE: 0.5441021186649093  L2+L1 loss: 5.382
Epoch [5/75], Batch loss: 7.187
Epoch: 5 RMSE:  13.665201852054574  MAPE: 0.7414740260922661  L2+L1 loss: 5.658
Epoch [6/75], Batch loss: 7.003
Epoch: 6 RMSE:  13.557517654133717  MAPE: 0.9033596949386169  L2+L1 loss: 5.942
Epoch [7/75], Batch loss: 7.265
Epoch: 7 RMSE:  13.504663906857441  MAPE: 1.0018244361290087  L2+L1 loss: 6.129
Epoch [8/75], Batch loss: 7.425
Epoch: 8 RMSE:  13.482560843998684  MAPE: 1.0474139105924165  L2+L1 loss: 6.214
Epoch [9/75], Batch loss: 7.099
Epoch: 9 RMSE:  13.46260003471144  MAPE: 1.091550928475418  L2+L1 loss: 6.294
Epoch [10/75], Batch loss: 7.58
Epoch: 10 RMSE:  13.477511940802465  MAPE: 1.058286990305671  L2+L1 loss: 6.234
Epoch [11/75], Batch loss: 7.359
Epoch: 11 RMSE:  13.462813551525189  MAPE: 1.0910616022130626  L2+L1 loss: 6.293
Epoch [12/75], Batch loss: 7.169
Epoch: 12 RMSE:  13.495881052612207  MAPE: 1.0195706886705669  L2+L1 loss: 6.162
Epoch [13/75], Batch loss: 7.242
Epoch: 13 RMSE:  13.519951326988386  MAPE: 0.9719723694035873  L2+L1 loss: 6.074
Epoch [14/75], Batch loss: 7.198
Epoch: 14 RMSE:  13.486284730714607  MAPE: 1.0395103947842166  L2+L1 loss: 6.199
Epoch [15/75], Batch loss: 7.553
Epoch: 15 RMSE:  13.466190197906716  MAPE: 1.083375698700163  L2+L1 loss: 6.279
Epoch [16/75], Batch loss: 7.325
Epoch: 16 RMSE:  13.469614224118684  MAPE: 1.0756801503209903  L2+L1 loss: 6.265
Epoch [17/75], Batch loss: 7.159
Epoch: 17 RMSE:  13.46090752757478  MAPE: 1.0954439883261804  L2+L1 loss: 6.3
Epoch [18/75], Batch loss: 7.264
Epoch: 18 RMSE:  13.441862659157428  MAPE: 1.1417127348510288  L2+L1 loss: 6.388
Epoch [19/75], Batch loss: 7.424
Epoch: 19 RMSE:  13.449907826051403  MAPE: 1.1216076311115974  L2+L1 loss: 6.349
Epoch [20/75], Batch loss: 7.466
Epoch: 20 RMSE:  13.491439878134766  MAPE: 1.0287244202694965  L2+L1 loss: 6.179
Epoch [21/75], Batch loss: 7.173
Epoch: 21 RMSE:  13.432918038559396  MAPE: 1.165065222021171  L2+L1 loss: 6.433
Epoch [22/75], Batch loss: 7.41
Epoch: 22 RMSE:  13.461725308814444  MAPE: 1.0935597732789895  L2+L1 loss: 6.297
Epoch [23/75], Batch loss: 7.335
Epoch: 23 RMSE:  13.484773596701036  MAPE: 1.0427060102425612  L2+L1 loss: 6.205
Epoch [24/75], Batch loss: 6.906
Epoch: 24 RMSE:  13.45501222436945  MAPE: 1.1092839340363903  L2+L1 loss: 6.326
Epoch [25/75], Batch loss: 7.492
Epoch: 25 RMSE:  13.473851572096292  MAPE: 1.066288009683857  L2+L1 loss: 6.248
Epoch [26/75], Batch loss: 7.308
Epoch: 26 RMSE:  13.484282557122297  MAPE: 1.0437478065011576  L2+L1 loss: 6.207
Epoch [27/75], Batch loss: 7.152
Epoch: 27 RMSE:  13.460832831569407  MAPE: 1.0956163903108227  L2+L1 loss: 6.301
Epoch [28/75], Batch loss: 7.33
Epoch: 28 RMSE:  13.473309323507626  MAPE: 1.067482013988335  L2+L1 loss: 6.25
Epoch [29/75], Batch loss: 7.213
Epoch: 29 RMSE:  13.493583825641057  MAPE: 1.0242898915980867  L2+L1 loss: 6.171
Epoch [30/75], Batch loss: 7.456
Epoch: 30 RMSE:  13.48906885081174  MAPE: 1.0336633453068145  L2+L1 loss: 6.188
Epoch [31/75], Batch loss: 7.002
Epoch: 31 RMSE:  13.484547264033122  MAPE: 1.043185993040714  L2+L1 loss: 6.206
Epoch [32/75], Batch loss: 7.458
Epoch: 32 RMSE:  13.483262589993991  MAPE: 1.045917147907565  L2+L1 loss: 6.211
Epoch [33/75], Batch loss: 7.033
Epoch: 33 RMSE:  13.481503821345255  MAPE: 1.0496750289294596  L2+L1 loss: 6.218
Epoch [34/75], Batch loss: 7.231
Epoch: 34 RMSE:  13.481212112073425  MAPE: 1.050300438226896  L2+L1 loss: 6.219
Epoch [35/75], Batch loss: 6.956
Epoch: 35 RMSE:  13.476770425611829  MAPE: 1.0598996420868427  L2+L1 loss: 6.237
Epoch [36/75], Batch loss: 7.12
Epoch: 36 RMSE:  13.4775118714201  MAPE: 1.0582871410067065  L2+L1 loss: 6.234
Epoch [37/75], Batch loss: 7.196
Epoch: 37 RMSE:  13.475736543733992  MAPE: 1.0621550337845358  L2+L1 loss: 6.241
Epoch [38/75], Batch loss: 7.21
Epoch: 38 RMSE:  13.474750959040842  MAPE: 1.0643126205101723  L2+L1 loss: 6.245
Epoch [39/75], Batch loss: 7.099
Epoch: 39 RMSE:  13.47553143158921  MAPE: 1.062582444376745  L2+L1 loss: 6.241
Epoch [40/75], Batch loss: 7.209
Epoch: 40 RMSE:  13.363225430795389  MAPE: 0.729082955184542  L2+L1 loss: 5.554
Epoch [41/75], Batch loss: 6.275
Epoch: 41 RMSE:  13.193943092693502  MAPE: 0.3155887752723177  L2+L1 loss: 4.603
Epoch [42/75], Batch loss: 6.159
Epoch: 42 RMSE:  13.093065592894806  MAPE: 0.21593425678536401  L2+L1 loss: 4.296
Epoch [43/75], Batch loss: 6.097
Epoch: 43 RMSE:  13.003882468933899  MAPE: 0.18074516367326401  L2+L1 loss: 4.184
Epoch [44/75], Batch loss: 6.047
Epoch: 44 RMSE:  12.921095781920181  MAPE: 0.19592400717039657  L2+L1 loss: 4.186
Epoch [45/75], Batch loss: 5.521
Epoch: 45 RMSE:  5.307998590557168  MAPE: 0.564800622287894  L2+L1 loss: 3.508
Epoch [46/75], Batch loss: 2.925
Epoch: 46 RMSE:  2.488016667765917  MAPE: 0.24539596613645862  L2+L1 loss: 1.668
Epoch [47/75], Batch loss: 1.251
Epoch: 47 RMSE:  0.8209786335595772  MAPE: 0.09269916458577308  L2+L1 loss: 0.725
Epoch [48/75], Batch loss: 0.565
Epoch: 48 RMSE:  0.71434787327247  MAPE: 0.07135339630010862  L2+L1 loss: 0.636
Epoch [49/75], Batch loss: 0.838
Epoch: 49 RMSE:  1.1948338001358336  MAPE: 0.0701678819451186  L2+L1 loss: 0.79
Epoch [50/75], Batch loss: 1.896
Epoch: 50 RMSE:  1.6301569499035748  MAPE: 0.08463404717414537  L2+L1 loss: 0.947
Epoch [51/75], Batch loss: 1.923
Epoch: 51 RMSE:  0.8260148690644723  MAPE: 0.11365270139415992  L2+L1 loss: 0.809
Epoch [52/75], Batch loss: 0.687
Epoch: 52 RMSE:  1.2465491561948856  MAPE: 0.08658596564198669  L2+L1 loss: 0.77
Epoch [53/75], Batch loss: 0.88
Epoch: 53 RMSE:  1.7987865742938292  MAPE: 0.056275183974088205  L2+L1 loss: 0.766
Epoch [54/75], Batch loss: 0.687
Epoch: 54 RMSE:  1.3643421285155781  MAPE: 0.06214668272036875  L2+L1 loss: 0.76
Epoch [55/75], Batch loss: 0.77
Epoch: 55 RMSE:  1.4431990376113282  MAPE: 0.06994998892006642  L2+L1 loss: 0.796
Epoch [56/75], Batch loss: 0.781
Epoch: 56 RMSE:  1.1645712092816618  MAPE: 0.06334830118595233  L2+L1 loss: 0.75
Epoch [57/75], Batch loss: 1.186
Epoch: 57 RMSE:  0.9210777966926678  MAPE: 0.06417254555479844  L2+L1 loss: 0.645
Epoch [58/75], Batch loss: 0.622
Epoch: 58 RMSE:  3.4579008456772065  MAPE: 0.10729900847237699  L2+L1 loss: 1.564
Epoch [59/75], Batch loss: 0.972
Epoch: 59 RMSE:  2.500809168106868  MAPE: 0.10412171920128728  L2+L1 loss: 1.288
Epoch [60/75], Batch loss: 0.63
Epoch: 60 RMSE:  0.9235728500310969  MAPE: 0.040630148737160775  L2+L1 loss: 0.557
Epoch [61/75], Batch loss: 0.426
Epoch: 61 RMSE:  0.8278050323693118  MAPE: 0.036595124572593764  L2+L1 loss: 0.528
Epoch [62/75], Batch loss: 0.393
Epoch: 62 RMSE:  0.7065231010458073  MAPE: 0.03571992956300017  L2+L1 loss: 0.485
Epoch [63/75], Batch loss: 0.344
Epoch: 63 RMSE:  0.571020254953063  MAPE: 0.033932883331733124  L2+L1 loss: 0.442
Epoch [64/75], Batch loss: 0.312
Epoch: 64 RMSE:  0.4545987528635912  MAPE: 0.037419548727937534  L2+L1 loss: 0.417
Epoch [65/75], Batch loss: 0.269
Epoch: 65 RMSE:  0.37881093216015893  MAPE: 0.03028000890262234  L2+L1 loss: 0.377
Epoch [66/75], Batch loss: 0.276
Epoch: 66 RMSE:  1.5755715056494102  MAPE: 0.039918620387308444  L2+L1 loss: 0.729
Epoch [67/75], Batch loss: 0.384
Epoch: 67 RMSE:  0.4037061390931607  MAPE: 0.03172156244571232  L2+L1 loss: 0.385
Epoch [68/75], Batch loss: 0.262
Epoch: 68 RMSE:  0.5923996618254792  MAPE: 0.02878336624320737  L2+L1 loss: 0.43
Epoch [69/75], Batch loss: 0.236
Epoch: 69 RMSE:  0.3065769233498916  MAPE: 0.027206974694014067  L2+L1 loss: 0.348
Epoch [70/75], Batch loss: 0.258
Epoch: 70 RMSE:  0.3395736223966219  MAPE: 0.02443302936693087  L2+L1 loss: 0.338
Epoch [71/75], Batch loss: 0.246
Epoch: 71 RMSE:  0.38502632329744285  MAPE: 0.0231366205076351  L2+L1 loss: 0.337
Epoch [72/75], Batch loss: 0.231
Epoch: 72 RMSE:  0.4628323237460709  MAPE: 0.028740094628384467  L2+L1 loss: 0.393
Epoch [73/75], Batch loss: 0.236
Epoch: 73 RMSE:  0.29304983562917725  MAPE: 0.023733103607374063  L2+L1 loss: 0.326
Epoch [74/75], Batch loss: 0.251
Epoch: 74 RMSE:  0.7022452686603592  MAPE: 0.027259408103096787  L2+L1 loss: 0.441


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth 
1.4526248 , 1.5501
4.846924 , 4.9999
2.7606764 , 2.7991
3.0631928 , 3.0302
4.1735563 , 4.2921
2.8408937 , 2.8684
1.4936757 , 1.4943
2.8914773 , 2.9881
3.2988138 , 3.2628
5.911958 , 5.748
3.910046 , 3.9618
3.4309669 , 3.3971
0.8537202 , 1.0524
3.0502214 , 3.062
2.602283 , 2.5799
4.2876987 , 4.1797
2.8657088 , 2.8786
3.528642 , 3.5969
5.4968576 , 5.4764
3.2856882 , 3.2919
2.3032026 , 2.3897
5.3906374 , 5.2307
1.6997547 , 1.6095
4.5185537 , 4.4992
3.3213816 , 3.3262
9.538902 , 9.3249
3.665749 , 3.741
3.4791522 , 3.5001
2.583661 , 2.724
1.2045779 , 1.389
2.4732833 , 2.6278
6.352587 , 6.6712
3.000654 , 3.036
24.96865 , 25.9874
2.8263412 , 2.8094
3.7199697 , 3.7019
2.9060042 , 2.7028
2.4455485 , 2.499
7.359235 , 7.4733
4.937793 , 5.1567
2.732614 , 2.7711
1.4672799 , 1.4082
3.242876 , 3.1772
37.36287 , 38.681
1.5657783 , 1.5491
1.1712961 , 1.2232
2.928804 , 2.9125
3.0883534 , 3.0748
3.6641598 , 3.7807
5.9689937 , 6.1877
3.9821641 , 4.0361
3.3620534 , 3.3909
5.7487473 , 5.6187
2.8143272 , 2.8523
3.206262 , 3.3285
2.663694 , 2.6867
3.4528544 , 3.6674
0.9435468 , 1.0444
2.484881 , 2.4924
2.5909114 , 2.5639
3.8360212 , 3.8259
6.481648 , 6.8424
5.2621365 , 5.1422
4.856824 , 4.7919
6.2630005 , 6.3703
3.1328735 , 3.1329
0.8769045 , 1.0416
3.5467317 , 3.5388
3.306063 , 3.3223
1.216227 , 1.2877
6.940374 , 6.3166
1.4031358 , 1.4114
5.596176 , 5.7934
3.2835212 , 3.2805
2.1614766 , 2.3332
2.681343 , 2.7486
2.9029949 , 2.7423
3.2946215 , 3.3419
5.8544593 , 5.528
3.1316283 , 3.1562
4.616062 , 4.5327
4.876522 , 4.8145
2.7749126 , 2.8491
2.21066 , 2.2837
6.39334 , 6.525
1.4357915 , 1.3678
1.9350963 , 1.9532
4.964427 , 4.847
9.026996 , 9.0157
2.4369893 , 2.4422
2.814128 , 2.82
5.430986 , 5.1978
2.0099607 , 2.0489
3.1291106 , 3.1279
1.973567 , 2.0912
2.0276346 , 1.8599
2.9544325 , 2.9214
3.8190036 , 3.8956
1.9189925 , 2.1283
1.8719497 , 1.7902
3.9049292 , 3.9107
7.1778626 , 7.2531
7.8351583 , 8.0761
2.4438252 , 2.4778
1.5281625 , 1.535
1.1793647 , 1.2314
2.6191254 , 2.7016
10.613715 , 10.837
1.6029615 , 1.5115
5.641584 , 5.5607
2.5719056 , 2.6154
7.9867954 , 8.0675
2.0346894 , 2.1141
4.8199577 , 4.9546
4.2037883 , 4.2779
3.7003665 , 3.8486
2.5216966 , 2.4265
4.200556 , 4.3503
0.8726616 , 1.0368
27.187675 , 28.9874
6.096406 , 6.1664
4.2295156 , 4.2542
3.9369855 , 3.9413
1.7680736 , 1.6173
2.9641538 , 2.9061
1.3105273 , 1.3671
2.1771631 , 2.1402
2.1573973 , 2.1642
2.9544888 , 3.0152
28.33982 , 29.2883
6.105151 , 6.0989
1.9636459 , 1.832
5.085987 , 4.7864
2.4489565 , 2.5459
2.0265012 , 2.1564
2.3060617 , 2.2705
3.4222202 , 3.4619
3.3024929 , 3.304
1.002809 , 1.0801
5.134224 , 5.1414
2.282844 , 1.9515
7.570253 , 7.8129
6.1007147 , 6.0163
2.603283 , 2.3797
3.750367 , 3.9505
3.2516704 , 3.0003
2.0891738 , 2.1812
2.258119 , 2.4028
4.228763 , 4.2112
2.899137 , 2.809
33.519707 , 33.5891
2.6550374 , 2.7182
3.1416368 , 3.1808
3.6464 , 3.6918
1.3913999 , 1.4319
34.09589 , 35.2349
5.799225 , 5.8238
2.2948217 , 2.359
2.841854 , 2.7666
9.659164 , 9.839
3.5537157 , 3.6228
3.1658814 , 3.1646
2.0874038 , 2.1338
2.9831736 , 2.9536
2.9371958 , 2.9535
3.0646749 , 3.1505
4.9906893 , 4.7467
3.0555315 , 3.1249
1.9788342 , 1.9646
9.138632 , 9.0906
5.8219833 , 6.0519
3.0693245 , 3.0696
3.0638442 , 2.9822
4.946357 , 4.9034
3.9788313 , 3.854
5.9466543 , 5.9243
1.7947221 , 1.7272
2.5028114 , 2.7581
3.7320302 , 3.7051
3.3966684 , 3.3573
4.5851183 , 4.7251
2.8876102 , 2.8179
4.8230643 , 4.9596
3.0105767 , 3.0423
2.5059261 , 2.3828
2.8898392 , 2.9261
3.3713183 , 3.2385
2.7307696 , 2.7056
2.6559708 , 2.811
2.7099924 , 2.7825
1.5288396 , 1.4519
4.864729 , 4.8485
3.0032523 , 3.0145
4.9555316 , 5.0283
3.6207118 , 3.5626
4.273672 , 4.2539
30.496216 , 31.7013
2.598434 , 2.6121
1.4352622 , 1.3909
2.453354 , 2.4843
1.6469874 , 1.4077
1.566834 , 1.6524
3.500534 , 3.5574
2.309338 , 2.2881
4.079354 , 4.0135
1.4591208 , 1.49
2.0782056 , 1.956
1.486402 , 1.567
5.793258 , 5.6708
2.9717202 , 2.8828
3.5961826 , 3.5557
2.6404762 , 2.6067
5.2367187 , 5.1409
1.3399739 , 1.3347
2.909134 , 2.9217
4.0976634 , 4.1102
7.876509 , 8.0814
2.3083992 , 2.3825
4.6562977 , 4.5901
26.667725 , 27.4657
4.993968 , 4.934
4.2572865 , 4.2615
2.434032 , 2.6079
3.5589135 , 3.5306
2.549417 , 2.3897
1.1409879 , 1.0489
2.0780954 , 2.2034
1.4092851 , 1.4761
1.8318667 , 1.7353
1.1428399 , 1.2544
2.2152462 , 2.1684
2.2263756 , 2.1204
4.1965613 , 4.2051
22.875872 , 23.7837
75.370255 , 77.7917
10.637003 , 10.9957
2.5702958 , 2.601
3.576832 , 3.6328
6.9741545 , 7.1456
3.2570448 , 3.0986
1.8620915 , 1.8663
6.7537084 , 6.9923
3.6014066 , 3.5643
2.8343089 , 2.8248
3.1185591 , 3.0922
1.9455705 , 1.8891
2.9973218 , 2.9028
4.054725 , 4.2483
3.7315948 , 3.7877
1.4006186 , 1.2598
1.6605535 , 1.7343
4.6686535 , 4.4867
1.5649481 , 1.5924
3.8357978 , 3.7922
2.759275 , 2.7377
5.3297625 , 5.2465
3.7537827 , 3.6963
3.3746426 , 3.3889
1.0905704 , 1.091
3.6276562 , 3.6942
2.722001 , 2.7443
0.9916501 , 1.0459
2.004066 , 1.8704
2.0114856 , 2.0613
7.544329 , 8.072
5.919575 , 5.955
2.2029424 , 2.181
4.8340015 , 5.0251
2.0889206 , 2.0053
RMSE:  0.27900081435911844  MAPE: 0.030004533516588897
5: ground truth total-  215  predicted total -  215
100: ground truth total-  54  predicted total -  54
 more 100: ground truth total -  0  predicted total -  0
