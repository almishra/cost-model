['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1750
34
<class 'numpy.dtype'> float64
1750
train batches:  79  validate samples: 140  test samples: 350
Epoch [0/100], Batch loss: 19.239
Epoch: 0 RMSE:  0.7361341294003886  MAPE: 2.0644325168160718  L2+L1 loss: 0.891
Epoch [1/100], Batch loss: 0.65
Epoch: 1 RMSE:  0.8910712748653478  MAPE: 1.0291426362305502  L2+L1 loss: 0.805
Epoch [2/100], Batch loss: 0.679
Epoch: 2 RMSE:  0.6945944370799775  MAPE: 0.7556022279857254  L2+L1 loss: 0.548
Epoch [3/100], Batch loss: 0.727
Epoch: 3 RMSE:  0.6769842854761422  MAPE: 0.6670067894746246  L2+L1 loss: 0.533
Epoch [4/100], Batch loss: 1.225
Epoch: 4 RMSE:  0.6802223345590933  MAPE: 0.7082685080370646  L2+L1 loss: 0.539
Epoch [5/100], Batch loss: 2.507
Epoch: 5 RMSE:  0.6613597940916679  MAPE: 0.9543330428859258  L2+L1 loss: 0.561
Epoch [6/100], Batch loss: 0.676
Epoch: 6 RMSE:  0.7097599448869296  MAPE: 1.4634958047758695  L2+L1 loss: 0.676
Epoch [7/100], Batch loss: 0.617
Epoch: 7 RMSE:  0.7046565158793312  MAPE: 1.225019133970611  L2+L1 loss: 0.636
Epoch [8/100], Batch loss: 3.454
Epoch: 8 RMSE:  0.7594065133107981  MAPE: 0.544099949685299  L2+L1 loss: 0.6
Epoch [9/100], Batch loss: 0.628
Epoch: 9 RMSE:  0.7117011205305581  MAPE: 0.9149221377600603  L2+L1 loss: 0.602
Epoch [10/100], Batch loss: 0.615
Epoch: 10 RMSE:  0.7049814038342369  MAPE: 1.1228992895398995  L2+L1 loss: 0.62
Epoch [11/100], Batch loss: 0.6
Epoch: 11 RMSE:  0.7046974201542627  MAPE: 1.2327080103793082  L2+L1 loss: 0.637
Epoch [12/100], Batch loss: 0.795
Epoch: 12 RMSE:  0.7542605539854126  MAPE: 2.085809853397325  L2+L1 loss: 0.785
Epoch [13/100], Batch loss: 148.434
Epoch: 13 RMSE:  1.4826800601656478  MAPE: 4.192429277002796  L2+L1 loss: 1.73
Epoch [14/100], Batch loss: 1.466
Epoch: 14 RMSE:  1.4567422094923428  MAPE: 4.076202137397659  L2+L1 loss: 1.699
Epoch [15/100], Batch loss: 1.44
Epoch: 15 RMSE:  1.417969345521094  MAPE: 3.9012651365511943  L2+L1 loss: 1.652
Epoch [16/100], Batch loss: 1.394
Epoch: 16 RMSE:  1.3633970908380184  MAPE: 3.6523296338674665  L2+L1 loss: 1.585
Epoch [17/100], Batch loss: 1.317
Epoch: 17 RMSE:  1.2884718238772086  MAPE: 3.3044453798253386  L2+L1 loss: 1.491
Epoch [18/100], Batch loss: 1.24
Epoch: 18 RMSE:  1.190375556658483  MAPE: 2.83529375309891  L2+L1 loss: 1.363
Epoch [19/100], Batch loss: 1.123
Epoch: 19 RMSE:  1.0694953181054307  MAPE: 2.226335809090165  L2+L1 loss: 1.193
Epoch [20/100], Batch loss: 0.969
Epoch: 20 RMSE:  0.9347257682307217  MAPE: 1.4777035100947107  L2+L1 loss: 0.975
Epoch [21/100], Batch loss: 0.769
Epoch: 21 RMSE:  0.8164429268182716  MAPE: 0.6843573224960124  L2+L1 loss: 0.709
Epoch [22/100], Batch loss: 0.679
Epoch: 22 RMSE:  0.7404330431686302  MAPE: 0.6347664735092475  L2+L1 loss: 0.598
Epoch [23/100], Batch loss: 0.623
Epoch: 23 RMSE:  0.71130695833691  MAPE: 0.9219412157651464  L2+L1 loss: 0.602
Epoch [24/100], Batch loss: 0.598
Epoch: 24 RMSE:  0.7050230124367493  MAPE: 1.119386714359019  L2+L1 loss: 0.619
Epoch [25/100], Batch loss: 0.621
Epoch: 25 RMSE:  0.704620568778167  MAPE: 1.1715384989855924  L2+L1 loss: 0.627
Epoch [26/100], Batch loss: 12.733
Epoch: 26 RMSE:  15.436531309246917  MAPE: 6.011660841342161  L2+L1 loss: 3.603
Epoch [27/100], Batch loss: 2.63
Epoch: 27 RMSE:  0.6727805552907132  MAPE: 1.0693168785627876  L2+L1 loss: 0.598
Epoch [28/100], Batch loss: 0.693
Epoch: 28 RMSE:  0.7216328081781341  MAPE: 1.694889777816359  L2+L1 loss: 0.711
Epoch [29/100], Batch loss: 0.638
Epoch: 29 RMSE:  2.8585261840802523  MAPE: 4.8638235253032045  L2+L1 loss: 1.22
Epoch [30/100], Batch loss: 0.727
Epoch: 30 RMSE:  0.7076495371520659  MAPE: 1.3870275790962343  L2+L1 loss: 0.663
Epoch [31/100], Batch loss: 0.637
Epoch: 31 RMSE:  0.7069284340030255  MAPE: 1.357240377478096  L2+L1 loss: 0.658
Epoch [32/100], Batch loss: 0.595
Epoch: 32 RMSE:  0.7060620123100947  MAPE: 1.313681564033394  L2+L1 loss: 0.651
Epoch [33/100], Batch loss: 0.62
Epoch: 33 RMSE:  0.7055332730662996  MAPE: 1.2771133028992605  L2+L1 loss: 0.644
Epoch [34/100], Batch loss: 0.613
Epoch: 34 RMSE:  0.7052271859351292  MAPE: 1.252882240862074  L2+L1 loss: 0.64
Epoch [35/100], Batch loss: 0.615
Epoch: 35 RMSE:  0.7049289813304469  MAPE: 1.2196782972723794  L2+L1 loss: 0.635
Epoch [36/100], Batch loss: 0.601
Epoch: 36 RMSE:  0.7048345778007333  MAPE: 1.2311306977291017  L2+L1 loss: 0.637
Epoch [37/100], Batch loss: 0.6
Epoch: 37 RMSE:  0.7046211229247995  MAPE: 1.1915491924382986  L2+L1 loss: 0.63
Epoch [38/100], Batch loss: 0.628
Epoch: 38 RMSE:  0.7044359380495042  MAPE: 1.174514081872804  L2+L1 loss: 0.626
Epoch [39/100], Batch loss: 0.61
Epoch: 39 RMSE:  0.7043491818743113  MAPE: 1.1899868081817788  L2+L1 loss: 0.628
Epoch [40/100], Batch loss: 0.622
Epoch: 40 RMSE:  0.7052860043320248  MAPE: 1.0937715382453883  L2+L1 loss: 0.615
Epoch [41/100], Batch loss: 0.606
Epoch: 41 RMSE:  0.7044824288258411  MAPE: 1.192100288230475  L2+L1 loss: 0.63
Epoch [42/100], Batch loss: 0.616
Epoch: 42 RMSE:  0.7047659428708538  MAPE: 1.1279624343431436  L2+L1 loss: 0.619
Epoch [43/100], Batch loss: 0.615
Epoch: 43 RMSE:  0.7049900740561712  MAPE: 1.2069678562619384  L2+L1 loss: 0.631
Epoch [44/100], Batch loss: 0.63
Epoch: 44 RMSE:  0.7043552923348024  MAPE: 1.2024857491291385  L2+L1 loss: 0.63
Epoch [45/100], Batch loss: 0.59
Epoch: 45 RMSE:  0.693345593220765  MAPE: 0.8513091827671754  L2+L1 loss: 0.564
Epoch [46/100], Batch loss: 0.572
Epoch: 46 RMSE:  0.6618321510919681  MAPE: 0.6684945085271695  L2+L1 loss: 0.495
Epoch [47/100], Batch loss: 0.547
Epoch: 47 RMSE:  0.6477406979473443  MAPE: 0.5669552724383554  L2+L1 loss: 0.469
Epoch [48/100], Batch loss: 0.845
Epoch: 48 RMSE:  1.1065839375950497  MAPE: 1.248923955327104  L2+L1 loss: 0.723
Epoch [49/100], Batch loss: 0.772
Epoch: 49 RMSE:  0.28812519628538674  MAPE: 0.3907752228281429  L2+L1 loss: 0.384
Epoch [50/100], Batch loss: 0.361
Epoch: 50 RMSE:  0.2208426426891688  MAPE: 0.37233947968460185  L2+L1 loss: 0.361
Epoch [51/100], Batch loss: 0.23
Epoch: 51 RMSE:  0.3416171420390764  MAPE: 0.46825652636218756  L2+L1 loss: 0.389
Epoch [52/100], Batch loss: 0.186
Epoch: 52 RMSE:  0.5747734066569541  MAPE: 0.7524364668570499  L2+L1 loss: 0.503
Epoch [53/100], Batch loss: 0.205
Epoch: 53 RMSE:  0.7155864415656878  MAPE: 0.6897481349068686  L2+L1 loss: 0.48
Epoch [54/100], Batch loss: 0.39
Epoch: 54 RMSE:  0.5239132647524097  MAPE: 0.7011476581274126  L2+L1 loss: 0.443
Epoch [55/100], Batch loss: 0.358
Epoch: 55 RMSE:  0.7554087482795601  MAPE: 0.5215055298146406  L2+L1 loss: 0.44
Epoch [56/100], Batch loss: 0.245
Epoch: 56 RMSE:  1.4056865576199902  MAPE: 0.6862764858572377  L2+L1 loss: 0.489
Epoch [57/100], Batch loss: 0.239
Epoch: 57 RMSE:  0.19597365690732005  MAPE: 0.4069067964221964  L2+L1 loss: 0.362
Epoch [58/100], Batch loss: 0.158
Epoch: 58 RMSE:  0.2360253188383834  MAPE: 0.3676689915420996  L2+L1 loss: 0.351
Epoch [59/100], Batch loss: 0.379
Epoch: 59 RMSE:  0.2566163380597552  MAPE: 0.4401474209286987  L2+L1 loss: 0.41
Epoch [60/100], Batch loss: 0.163
Epoch: 60 RMSE:  0.14679563048714797  MAPE: 0.3601373785707005  L2+L1 loss: 0.337
Epoch [61/100], Batch loss: 0.137
Epoch: 61 RMSE:  0.14941479442797562  MAPE: 0.36439949405848904  L2+L1 loss: 0.34
Epoch [62/100], Batch loss: 0.128
Epoch: 62 RMSE:  0.14444684315076228  MAPE: 0.33409362744065846  L2+L1 loss: 0.326
Epoch [63/100], Batch loss: 0.123
Epoch: 63 RMSE:  0.13769509240298555  MAPE: 0.32453207401989875  L2+L1 loss: 0.32
Epoch [64/100], Batch loss: 0.117
Epoch: 64 RMSE:  0.1359318033433913  MAPE: 0.306602478185801  L2+L1 loss: 0.309
Epoch [65/100], Batch loss: 0.111
Epoch: 65 RMSE:  0.13068400581652187  MAPE: 0.28563260764321674  L2+L1 loss: 0.301
Epoch [66/100], Batch loss: 0.111
Epoch: 66 RMSE:  0.15416865472524927  MAPE: 0.3021220395294271  L2+L1 loss: 0.316
Epoch [67/100], Batch loss: 0.124
Epoch: 67 RMSE:  0.12718393685800514  MAPE: 0.2695035872151555  L2+L1 loss: 0.295
Epoch [68/100], Batch loss: 0.104
Epoch: 68 RMSE:  0.12164373851411521  MAPE: 0.25940408726050845  L2+L1 loss: 0.289
Epoch [69/100], Batch loss: 0.104
Epoch: 69 RMSE:  0.12258920709342802  MAPE: 0.26311897135508033  L2+L1 loss: 0.292
Epoch [70/100], Batch loss: 0.105
Epoch: 70 RMSE:  0.12343443521209202  MAPE: 0.24706894503799534  L2+L1 loss: 0.282
Epoch [71/100], Batch loss: 0.104
Epoch: 71 RMSE:  0.12874352233801936  MAPE: 0.23796088798824394  L2+L1 loss: 0.286
Epoch [72/100], Batch loss: 0.102
Epoch: 72 RMSE:  0.12062433928338506  MAPE: 0.22075366329783536  L2+L1 loss: 0.276
Epoch [73/100], Batch loss: 0.102
Epoch: 73 RMSE:  0.11401627754907266  MAPE: 0.22040337684829842  L2+L1 loss: 0.276
Epoch [74/100], Batch loss: 0.102
Epoch: 74 RMSE:  0.12421509625273668  MAPE: 0.23649742711713934  L2+L1 loss: 0.281
Epoch [75/100], Batch loss: 0.102
Epoch: 75 RMSE:  0.12181675127223832  MAPE: 0.22986182961259252  L2+L1 loss: 0.282
Epoch [76/100], Batch loss: 0.1
Epoch: 76 RMSE:  0.11511646787414526  MAPE: 0.2387493430873679  L2+L1 loss: 0.281
Epoch [77/100], Batch loss: 0.105
Epoch: 77 RMSE:  0.11430776189429265  MAPE: 0.2284158695428459  L2+L1 loss: 0.28
Epoch [78/100], Batch loss: 0.108
Epoch: 78 RMSE:  0.11302026609308863  MAPE: 0.21670607130811675  L2+L1 loss: 0.273
Epoch [79/100], Batch loss: 0.099
Epoch: 79 RMSE:  0.11299745352649504  MAPE: 0.22095837762981352  L2+L1 loss: 0.277
Epoch [80/100], Batch loss: 0.095
Epoch: 80 RMSE:  0.11541298024189997  MAPE: 0.2307803983589375  L2+L1 loss: 0.279
Epoch [81/100], Batch loss: 0.105
Epoch: 81 RMSE:  0.1283159755675636  MAPE: 0.22900710877701208  L2+L1 loss: 0.273
Epoch [82/100], Batch loss: 0.095
Epoch: 82 RMSE:  0.11478220022550138  MAPE: 0.22031194876562005  L2+L1 loss: 0.275
Epoch [83/100], Batch loss: 0.103
Epoch: 83 RMSE:  0.1220410086783878  MAPE: 0.21526160812209494  L2+L1 loss: 0.272
Epoch [84/100], Batch loss: 0.115
Epoch: 84 RMSE:  0.1223331504207502  MAPE: 0.21708318146134  L2+L1 loss: 0.269
Epoch [85/100], Batch loss: 0.102
Epoch: 85 RMSE:  0.1257019178403891  MAPE: 0.2214379244972707  L2+L1 loss: 0.275
Epoch [86/100], Batch loss: 0.094
Epoch: 86 RMSE:  0.12388621976422833  MAPE: 0.2274777231419877  L2+L1 loss: 0.272
Epoch [87/100], Batch loss: 0.097
Epoch: 87 RMSE:  0.12832081693742675  MAPE: 0.224820288480344  L2+L1 loss: 0.278
Epoch [88/100], Batch loss: 0.097
Epoch: 88 RMSE:  0.1569526899794439  MAPE: 0.22361272922694939  L2+L1 loss: 0.285
Epoch [89/100], Batch loss: 0.111
Epoch: 89 RMSE:  0.13213296125854138  MAPE: 0.23808002283469568  L2+L1 loss: 0.282
Epoch [90/100], Batch loss: 0.095
Epoch: 90 RMSE:  0.13254045994211286  MAPE: 0.22433157470282694  L2+L1 loss: 0.276
Epoch [91/100], Batch loss: 0.093
Epoch: 91 RMSE:  0.13212276204780646  MAPE: 0.22107986809146243  L2+L1 loss: 0.273
Epoch [92/100], Batch loss: 0.092
Epoch: 92 RMSE:  0.1316559785522249  MAPE: 0.22098164280692473  L2+L1 loss: 0.274
Epoch [93/100], Batch loss: 0.093
Epoch: 93 RMSE:  0.1316880916444747  MAPE: 0.2203824572832279  L2+L1 loss: 0.275
Epoch [94/100], Batch loss: 0.092
Epoch: 94 RMSE:  0.13055970265640995  MAPE: 0.21939643239780782  L2+L1 loss: 0.274
Epoch [95/100], Batch loss: 0.092
Epoch: 95 RMSE:  0.13004854878398445  MAPE: 0.21903352034074558  L2+L1 loss: 0.273
Epoch [96/100], Batch loss: 0.092
Epoch: 96 RMSE:  0.12947448062189862  MAPE: 0.21925789990352884  L2+L1 loss: 0.273
Epoch [97/100], Batch loss: 0.092
Epoch: 97 RMSE:  0.1282036844449302  MAPE: 0.22060758211906792  L2+L1 loss: 0.273
Epoch [98/100], Batch loss: 0.091
Epoch: 98 RMSE:  0.1283518725211465  MAPE: 0.21975235482391464  L2+L1 loss: 0.274
Epoch [99/100], Batch loss: 0.092
Epoch: 99 RMSE:  0.12751908999095882  MAPE: 0.223049914723742  L2+L1 loss: 0.272


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
0.7871625 , 0.6207
0.1499054 , 0.2741
0.47631884 , 0.6895
0.29325768 , 0.3715
0.4170646 , 0.35
0.5512974 , 0.6519
0.54532856 , 0.6548
0.31473657 , 0.1783
0.153345 , 0.1336
0.15322231 , 0.1174
0.14879985 , 0.1303
0.15389015 , 0.1483
0.15634118 , 0.1127
0.33526674 , 0.5181
0.48441255 , 0.4282
0.15535353 , 0.1028
0.5840993 , 0.5374
0.6271515 , 0.4707
0.66556215 , 0.5427
0.20074272 , 0.2007
0.14916362 , 0.1027
0.78544974 , 0.9693
0.16081066 , 0.1314
0.6293769 , 0.6797
0.39214784 , 0.3245
0.604017 , 0.8153
0.5065002 , 0.3467
0.15882136 , 0.1639
0.5724426 , 0.4722
0.180394 , 0.1191
0.15386586 , 0.1751
0.15185767 , 0.1404
1.1713147 , 1.4664
0.16324183 , 0.2278
0.9415233 , 0.8738
0.7657217 , 0.6134
0.13564989 , 0.1256
1.661252 , 1.4903
0.16109587 , 0.1189
9.006208 , 9.0582
0.9092102 , 0.7845
0.745186 , 1.0163
0.15939829 , 0.13
0.86205995 , 0.7465
0.16191113 , 0.1075
0.15829664 , 0.1496
0.16452165 , 0.1109
0.1764586 , 0.1733
0.30971754 , 0.4895
0.1749613 , 0.3125
1.0108608 , 0.8658
0.18225075 , 0.2916
0.34118858 , 0.2972
0.78794956 , 0.9228
0.30874574 , 0.265
0.82182163 , 0.8578
0.46281505 , 0.3635
0.3614847 , 0.3896
0.17113075 , 0.1673
1.1999528 , 1.4087
1.2891886 , 1.4281
0.5095418 , 0.5252
0.8734161 , 0.5138
0.44725057 , 0.4159
0.5055665 , 0.4769
0.7856276 , 0.7317
0.14409162 , 0.1633
0.35211906 , 0.4057
0.5955763 , 0.4563
0.15150861 , 0.1016
0.19402382 , 0.1591
0.1567203 , 0.1133
0.5931287 , 0.8052
0.15753144 , 0.1388
0.13369156 , 0.1125
0.15045276 , 0.1192
0.37728533 , 0.3274
0.69751614 , 0.7264
0.2668612 , 0.1692
0.62406623 , 0.5416
1.133398 , 1.2896
0.15334275 , 0.2165
0.89545125 , 0.9683
0.20314516 , 0.1497
1.1363266 , 0.3262
0.18104365 , 0.1784
0.17664032 , 0.1418
0.5868498 , 0.4617
0.35837805 , 0.4603
0.44442785 , 0.3829
1.0990517 , 0.9745
0.40261185 , 0.2749
0.3487672 , 0.5141
0.17298777 , 0.2619
0.52377427 , 0.4691
0.15931547 , 0.1231
0.14245297 , 0.12
0.30186686 , 0.227
0.15541196 , 0.1481
0.3812884 , 0.3006
0.2719671 , 0.2393
0.15471493 , 0.1245
0.25210956 , 0.3099
0.15659826 , 0.1435
1.2001812 , 1.4218
0.26237616 , 0.2638
0.831552 , 0.7505
0.1559131 , 0.1304
0.43511388 , 0.3634
0.42586324 , 0.4678
0.23809417 , 0.2839
0.38390422 , 0.406
0.15591788 , 0.1587
0.77733004 , 0.874
0.46442515 , 0.3058
0.63283193 , 0.8059
0.20254323 , 0.1923
0.38586378 , 0.3445
1.0004067 , 0.7641
0.1492395 , 0.1905
0.264032 , 0.2291
0.19147357 , 0.1289
0.1538641 , 0.143
0.15179048 , 0.1001
0.63528293 , 0.7292
0.1693136 , 0.1699
0.1927498 , 0.2161
0.49974748 , 0.4871
0.1512273 , 0.1043
0.90556955 , 1.024
0.1509346 , 0.1402
0.3974974 , 0.3244
0.1463313 , 0.101
0.39159334 , 0.3328
0.34337735 , 0.4337
0.6149507 , 0.4909
0.15538222 , 0.1522
1.3786894 , 1.4673
0.15460353 , 0.1635
0.1457855 , 0.1029
0.174938 , 0.1415
0.8937717 , 0.8043
0.32892653 , 0.484
0.14065783 , 0.105
0.7924764 , 0.6536
0.16211286 , 0.1913
0.6789444 , 0.6431
0.18056124 , 0.132
0.3581531 , 0.3503
0.15139088 , 0.1598
0.55315953 , 0.4199
0.17228882 , 0.2279
0.5138804 , 0.6169
0.15213266 , 0.1219
0.4892779 , 0.3257
0.14418736 , 0.1531
0.17051753 , 0.1198
0.14957902 , 0.139
0.76509243 , 0.8429
1.1909409 , 1.2365
0.8486815 , 0.9964
0.373239 , 0.2474
0.1437775 , 0.1504
0.3364907 , 0.4435
0.23320633 , 0.2334
0.33421004 , 0.3717
0.5002209 , 0.3974
0.2209737 , 0.3115
0.92943937 , 0.86
0.27247924 , 0.2675
0.2851276 , 0.3115
0.15061021 , 0.1095
0.15724458 , 0.1771
0.26516083 , 0.1936
0.21377614 , 0.2822
0.36517334 , 0.4206
0.15778016 , 0.2288
0.2791007 , 0.2436
0.15693292 , 0.1076
0.31813747 , 0.3665
0.15228635 , 0.2221
1.2247367 , 1.1419
0.1507484 , 0.1296
0.14514977 , 0.1661
0.18464449 , 0.1937
0.6368326 , 0.6774
0.6738398 , 0.4002
0.1572611 , 0.1151
0.8322958 , 0.7539
0.14607446 , 0.1398
0.15600483 , 0.2111
0.62163556 , 1.0357
0.14071344 , 0.1381
1.1502278 , 1.0161
0.26357424 , 0.1679
0.7631635 , 0.5985
0.18055098 , 0.2051
0.3236189 , 0.3622
0.4988297 , 0.5562
0.5881306 , 0.5193
0.6844188 , 0.5706
0.55214167 , 0.6345
0.1730468 , 0.3407
1.0003304 , 0.9462
0.37676525 , 0.5137
0.15032625 , 0.1069
0.88155985 , 0.8851
0.6800599 , 0.6262
0.1524175 , 0.149
0.66349375 , 0.6224
0.16203879 , 0.1086
0.82158864 , 1.0397
0.15114656 , 0.1953
0.26701877 , 0.2212
0.21190543 , 0.2939
0.17368951 , 0.1339
0.15575553 , 0.1233
0.3771593 , 0.2897
0.14665446 , 0.1586
0.92720175 , 1.1921
0.8811953 , 0.7564
0.18396527 , 0.1531
0.5847026 , 0.5513
0.3426033 , 0.5548
0.16894439 , 0.2525
0.18869746 , 0.1473
0.16101134 , 0.2193
0.1593329 , 0.1748
1.1293546 , 1.0644
0.7397859 , 0.9336
0.3633004 , 0.5143
1.3779403 , 1.297
0.9294744 , 0.806
0.14438026 , 0.1142
0.4437053 , 0.3174
0.78857434 , 0.5746
0.9014392 , 0.9153
0.6987393 , 0.5439
0.16904813 , 0.1411
0.15130895 , 0.2011
0.2038279 , 0.3269
0.2796278 , 0.4208
0.8567896 , 0.6834
0.27207476 , 0.223
0.23454057 , 0.25
0.18907219 , 0.1585
0.5723959 , 0.4816
0.20214072 , 0.2322
0.70752406 , 0.61
0.45829618 , 0.4116
0.18862545 , 0.128
0.54548377 , 0.4608
0.15001221 , 0.1087
0.15097627 , 0.1022
0.15001218 , 0.1217
0.37186337 , 0.3744
0.17226188 , 0.1825
0.2999276 , 0.2804
0.41629428 , 0.3028
0.15001193 , 0.1099
0.71040046 , 1.0553
0.15943968 , 0.153
0.42102838 , 0.3246
0.21657375 , 0.154
0.15321887 , 0.1052
0.2850548 , 0.2787
0.15352708 , 0.1206
0.19743836 , 0.1897
0.19750208 , 0.1392
0.13596736 , 0.1437
0.155332 , 0.1137
0.15276688 , 0.1927
0.29142603 , 0.2458
0.4901541 , 0.4657
0.58048666 , 0.9233
0.15368575 , 0.1046
1.096307 , 1.0478
0.44519973 , 0.4815
0.34995928 , 0.5624
0.2924024 , 0.1229
1.1790498 , 1.1784
0.5912236 , 0.5572
0.67307997 , 0.6046
1.1931679 , 1.3972
0.15244925 , 0.1873
0.17673576 , 0.1597
0.18529963 , 0.2182
0.17895778 , 0.1251
0.3791934 , 0.3499
0.15214129 , 0.1151
0.20871836 , 0.2588
0.14689237 , 0.1666
0.14753598 , 0.1536
0.15798306 , 0.1377
0.16615516 , 0.1205
0.47544998 , 0.6399
0.33109373 , 0.4597
0.15056846 , 0.2026
0.1717273 , 0.2211
0.15432471 , 0.1035
0.1550594 , 0.1011
0.86248934 , 0.8447
0.15202507 , 0.1113
0.1836923 , 0.2737
0.16169755 , 0.1265
0.6294422 , 0.5646
0.20568672 , 0.142
0.18418097 , 0.1355
0.5922138 , 0.5275
0.15672372 , 0.1105
0.1550821 , 0.208
1.0344286 , 0.899
0.14852335 , 0.1359
0.22738066 , 0.1544
0.24421024 , 0.1815
0.96510154 , 1.0378
0.8232646 , 1.1522
0.31586558 , 0.3101
1.2369523 , 1.5514
0.14847122 , 0.1235
0.1943571 , 0.1353
0.79932964 , 0.895
0.37172168 , 0.6371
0.16306886 , 0.1077
0.15147826 , 0.1106
0.30105624 , 0.3508
0.9741055 , 0.8467
0.575757 , 0.4643
1.356268 , 1.3405
0.20276564 , 0.1656
0.15367977 , 0.1186
0.91379356 , 0.8052
0.14553152 , 0.1341
0.57296085 , 0.5657
0.30506882 , 0.411
0.15509757 , 0.1111
0.35858917 , 0.2467
0.4965847 , 0.362
0.72205186 , 0.7237
0.15680891 , 0.1687
0.29698402 , 0.2417
0.88950354 , 1.2396
0.8424105 , 1.1293
1.0408227 , 1.1059
0.14917083 , 0.1107
0.6092597 , 0.5827
0.14936906 , 0.1071
0.14682429 , 0.128
0.6526931 , 0.7906
0.15195815 , 0.1022
RMSE:  0.11220595382466596  MAPE: 0.22724832266421902
5: ground truth total-  349  predicted total -  349
100: ground truth total-  1  predicted total -  1
 more 100: ground truth total -  0  predicted total -  0
