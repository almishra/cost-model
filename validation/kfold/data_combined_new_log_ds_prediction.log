['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
1967
34
<class 'numpy.dtype'> float64
1967
train batches:  89  validate samples: 157  test samples: 393
Epoch [0/75], Batch loss: 13.892
Epoch: 0 RMSE:  1.3983917326632933  MAPE: 1.8453588459888002  L2+L1 loss: 1.434
Epoch [1/75], Batch loss: 1.135
Epoch: 1 RMSE:  0.9321536706278037  MAPE: 2.5756696494905653  L2+L1 loss: 1.039
Epoch [2/75], Batch loss: 1.013
Epoch: 2 RMSE:  0.9316284388824835  MAPE: 2.997239563768448  L2+L1 loss: 1.068
Epoch [3/75], Batch loss: 1.004
Epoch: 3 RMSE:  0.9320022114865035  MAPE: 3.019738916793985  L2+L1 loss: 1.07
Epoch [4/75], Batch loss: 2.14
Epoch: 4 RMSE:  1.4469472779023531  MAPE: 2.1082882385947155  L2+L1 loss: 1.506
Epoch [5/75], Batch loss: 1.708
Epoch: 5 RMSE:  0.9415025026331704  MAPE: 2.2754895310786645  L2+L1 loss: 1.015
Epoch [6/75], Batch loss: 1.016
Epoch: 6 RMSE:  0.9336451250847649  MAPE: 3.0994954876072134  L2+L1 loss: 1.077
Epoch [7/75], Batch loss: 1.018
Epoch: 7 RMSE:  0.9312649427892986  MAPE: 2.972695257164451  L2+L1 loss: 1.066
Epoch [8/75], Batch loss: 1.017
Epoch: 8 RMSE:  0.9384420225105311  MAPE: 3.2620043123235414  L2+L1 loss: 1.096
Epoch [9/75], Batch loss: 1.015
Epoch: 9 RMSE:  0.9305465237152003  MAPE: 2.909665516319789  L2+L1 loss: 1.061
Epoch [10/75], Batch loss: 6.134
Epoch: 10 RMSE:  8.73503115234297  MAPE: 39.38767758403806  L2+L1 loss: 8.062
Epoch [11/75], Batch loss: 1.209
Epoch: 11 RMSE:  0.9403986159899406  MAPE: 3.3146219346311656  L2+L1 loss: 1.104
Epoch [12/75], Batch loss: 1.021
Epoch: 12 RMSE:  0.9440266425599834  MAPE: 3.4005585217961967  L2+L1 loss: 1.115
Epoch [13/75], Batch loss: 1.026
Epoch: 13 RMSE:  0.9308134382037121  MAPE: 2.9364111435692597  L2+L1 loss: 1.063
Epoch [14/75], Batch loss: 1.021
Epoch: 14 RMSE:  0.9303294565712166  MAPE: 2.7206461200319962  L2+L1 loss: 1.051
Epoch [15/75], Batch loss: 1.023
Epoch: 15 RMSE:  0.9424793882616685  MAPE: 2.2536548434426376  L2+L1 loss: 1.013
Epoch [16/75], Batch loss: 2.853
Epoch: 16 RMSE:  1.0364072512063558  MAPE: 4.536657193597053  L2+L1 loss: 1.266
Epoch [17/75], Batch loss: 1.051
Epoch: 17 RMSE:  0.8059530013483865  MAPE: 2.1317228483996327  L2+L1 loss: 0.916
Epoch [18/75], Batch loss: 0.978
Epoch: 18 RMSE:  0.9300679635650435  MAPE: 2.789575998988284  L2+L1 loss: 1.055
Epoch [19/75], Batch loss: 1.044
Epoch: 19 RMSE:  0.9326681336353144  MAPE: 3.0550122550041796  L2+L1 loss: 1.074
Epoch [20/75], Batch loss: 1.449
Epoch: 20 RMSE:  0.936103812276762  MAPE: 3.190768306359741  L2+L1 loss: 1.088
Epoch [21/75], Batch loss: 1.011
Epoch: 21 RMSE:  0.9351712367372844  MAPE: 3.1586664796643147  L2+L1 loss: 1.083
Epoch [22/75], Batch loss: 1.03
Epoch: 22 RMSE:  0.933224207418065  MAPE: 3.0811477869140074  L2+L1 loss: 1.076
Epoch [23/75], Batch loss: 1.021
Epoch: 23 RMSE:  0.934566278197768  MAPE: 3.136326341600084  L2+L1 loss: 1.08
Epoch [24/75], Batch loss: 11.806
Epoch: 24 RMSE:  1.1662157297332203  MAPE: 0.7353623528863862  L2+L1 loss: 1.026
Epoch [25/75], Batch loss: 1.258
Epoch: 25 RMSE:  0.9564467528135137  MAPE: 2.007798060907071  L2+L1 loss: 0.999
Epoch [26/75], Batch loss: 1.019
Epoch: 26 RMSE:  0.9320181224788865  MAPE: 2.5831090461656103  L2+L1 loss: 1.04
Epoch [27/75], Batch loss: 1.009
Epoch: 27 RMSE:  0.9303754810301971  MAPE: 2.888421144699365  L2+L1 loss: 1.061
Epoch [28/75], Batch loss: 10.441
Epoch: 28 RMSE:  1.0738839239623859  MAPE: 4.846410492133451  L2+L1 loss: 1.308
Epoch [29/75], Batch loss: 1.098
Epoch: 29 RMSE:  0.9908604145154241  MAPE: 4.088009391506356  L2+L1 loss: 1.202
Epoch [30/75], Batch loss: 1.055
Epoch: 30 RMSE:  0.9841397109185028  MAPE: 4.010663818154158  L2+L1 loss: 1.191
Epoch [31/75], Batch loss: 1.058
Epoch: 31 RMSE:  0.9749664177215401  MAPE: 3.897956715548324  L2+L1 loss: 1.175
Epoch [32/75], Batch loss: 1.039
Epoch: 32 RMSE:  0.9643792837263014  MAPE: 3.7540149622596237  L2+L1 loss: 1.155
Epoch [33/75], Batch loss: 1.044
Epoch: 33 RMSE:  0.9519351583578883  MAPE: 3.556216349447177  L2+L1 loss: 1.133
Epoch [34/75], Batch loss: 1.025
Epoch: 34 RMSE:  0.9435425040381277  MAPE: 3.3897836559184475  L2+L1 loss: 1.114
Epoch [35/75], Batch loss: 1.02
Epoch: 35 RMSE:  0.9374234267289921  MAPE: 3.2323164638499615  L2+L1 loss: 1.093
Epoch [36/75], Batch loss: 1.013
Epoch: 36 RMSE:  0.9349453180727313  MAPE: 3.1504786177600694  L2+L1 loss: 1.082
Epoch [37/75], Batch loss: 1.018
Epoch: 37 RMSE:  0.9349943471199009  MAPE: 3.152271259566492  L2+L1 loss: 1.083
Epoch [38/75], Batch loss: 1.016
Epoch: 38 RMSE:  0.9326295002903454  MAPE: 3.0530975569622663  L2+L1 loss: 1.073
Epoch [39/75], Batch loss: 1.01
Epoch: 39 RMSE:  0.9319181156997705  MAPE: 3.014878605524167  L2+L1 loss: 1.07
Epoch [40/75], Batch loss: 1.023
Epoch: 40 RMSE:  0.9338596039525956  MAPE: 3.108431922548719  L2+L1 loss: 1.078
Epoch [41/75], Batch loss: 1.009
Epoch: 41 RMSE:  0.9337662754979995  MAPE: 3.0914049392980294  L2+L1 loss: 1.077
Epoch [42/75], Batch loss: 1.021
Epoch: 42 RMSE:  0.9336305227847413  MAPE: 3.0988775238681683  L2+L1 loss: 1.077
Epoch [43/75], Batch loss: 1.019
Epoch: 43 RMSE:  0.9333424775802104  MAPE: 3.0864198865506483  L2+L1 loss: 1.076
Epoch [44/75], Batch loss: 1.003
Epoch: 44 RMSE:  0.9328779529924217  MAPE: 3.065165612511564  L2+L1 loss: 1.074
Epoch [45/75], Batch loss: 1.015
Epoch: 45 RMSE:  0.935471811135274  MAPE: 3.1690268677847344  L2+L1 loss: 1.085
Epoch [46/75], Batch loss: 1.022
Epoch: 46 RMSE:  0.9319155750555594  MAPE: 3.0147300783138964  L2+L1 loss: 1.07
Epoch [47/75], Batch loss: 0.997
Epoch: 47 RMSE:  0.6780382123125114  MAPE: 2.7860494898604835  L2+L1 loss: 0.94
Epoch [48/75], Batch loss: 0.57
Epoch: 48 RMSE:  0.5391319536166942  MAPE: 1.906178808320068  L2+L1 loss: 0.723
Epoch [49/75], Batch loss: 0.48
Epoch: 49 RMSE:  0.5345819563825808  MAPE: 1.7078105736955025  L2+L1 loss: 0.684
Epoch [50/75], Batch loss: 0.501
Epoch: 50 RMSE:  0.541891554772688  MAPE: 1.6711656408516213  L2+L1 loss: 0.691
Epoch [51/75], Batch loss: 0.483
Epoch: 51 RMSE:  0.5390117403860548  MAPE: 1.758370415032679  L2+L1 loss: 0.724
Epoch [52/75], Batch loss: 0.477
Epoch: 52 RMSE:  0.5273780223039021  MAPE: 1.8121396495928401  L2+L1 loss: 0.674
Epoch [53/75], Batch loss: 0.546
Epoch: 53 RMSE:  0.5286514893659255  MAPE: 1.814298259141384  L2+L1 loss: 0.69
Epoch [54/75], Batch loss: 0.464
Epoch: 54 RMSE:  0.534288414164329  MAPE: 1.7231222336763805  L2+L1 loss: 0.673
Epoch [55/75], Batch loss: 0.456
Epoch: 55 RMSE:  0.5388134015892267  MAPE: 1.781882239848892  L2+L1 loss: 0.713
Epoch [56/75], Batch loss: 0.46
Epoch: 56 RMSE:  0.5260790342682498  MAPE: 1.7302399053519313  L2+L1 loss: 0.656
Epoch [57/75], Batch loss: 0.454
Epoch: 57 RMSE:  0.5279047774628279  MAPE: 1.7003234271009622  L2+L1 loss: 0.678
Epoch [58/75], Batch loss: 0.516
Epoch: 58 RMSE:  0.7088046436408163  MAPE: 1.9110033689242323  L2+L1 loss: 0.902
Epoch [59/75], Batch loss: 0.514
Epoch: 59 RMSE:  0.5268971904623636  MAPE: 1.754994749127293  L2+L1 loss: 0.677
Epoch [60/75], Batch loss: 0.449
Epoch: 60 RMSE:  0.5249382293485114  MAPE: 1.7361228727749891  L2+L1 loss: 0.655
Epoch [61/75], Batch loss: 0.447
Epoch: 61 RMSE:  0.5247802133074951  MAPE: 1.7303482742525673  L2+L1 loss: 0.652
Epoch [62/75], Batch loss: 0.451
Epoch: 62 RMSE:  0.524212020966778  MAPE: 1.724714188116941  L2+L1 loss: 0.653
Epoch [63/75], Batch loss: 0.451
Epoch: 63 RMSE:  0.5237985459934006  MAPE: 1.7211959481974868  L2+L1 loss: 0.653
Epoch [64/75], Batch loss: 0.447
Epoch: 64 RMSE:  0.5237339219517296  MAPE: 1.720385336291985  L2+L1 loss: 0.658
Epoch [65/75], Batch loss: 0.451
Epoch: 65 RMSE:  0.5238453861229369  MAPE: 1.710611518531346  L2+L1 loss: 0.648
Epoch [66/75], Batch loss: 0.45
Epoch: 66 RMSE:  0.5238452571245586  MAPE: 1.7088102510581245  L2+L1 loss: 0.647
Epoch [67/75], Batch loss: 0.45
Epoch: 67 RMSE:  0.5234871961300719  MAPE: 1.716012756861164  L2+L1 loss: 0.66
Epoch [68/75], Batch loss: 0.451
Epoch: 68 RMSE:  0.5244098353434978  MAPE: 1.7084550370383071  L2+L1 loss: 0.646
Epoch [69/75], Batch loss: 0.452
Epoch: 69 RMSE:  0.5232819358659047  MAPE: 1.7099517086872242  L2+L1 loss: 0.649
Epoch [70/75], Batch loss: 0.452
Epoch: 70 RMSE:  0.5244805670706919  MAPE: 1.7156514558676572  L2+L1 loss: 0.656
Epoch [71/75], Batch loss: 0.446
Epoch: 71 RMSE:  0.5232839449819733  MAPE: 1.712886467676762  L2+L1 loss: 0.647
Epoch [72/75], Batch loss: 0.449
Epoch: 72 RMSE:  0.503269065083777  MAPE: 0.9695236779946951  L2+L1 loss: 0.586
Epoch [73/75], Batch loss: 0.203
Epoch: 73 RMSE:  0.15861729146928988  MAPE: 0.18544255684298389  L2+L1 loss: 0.228
Epoch [74/75], Batch loss: 0.097
Epoch: 74 RMSE:  0.1273021338599656  MAPE: 0.14994425327718272  L2+L1 loss: 0.2


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
4.4212036 , 4.9685
1.5573604 , 1.55164
0.19354105 , 0.1598
1.1243424 , 1.151769
0.082387626 , 0.079969
0.118970215 , 0.0646
0.13709688 , 0.0931
0.38589603 , 0.3663
0.74987173 , 0.704117
1.4571333 , 1.470436
1.7356998 , 1.730707
0.606644 , 0.5683
4.3819504 , 4.4947
0.19252542 , 0.191172
0.35823408 , 0.369319
0.90027666 , 0.922066
4.7177505 , 4.7931
1.6096265 , 1.628765
2.3987455 , 2.3937
0.19831932 , 0.188357
1.2153072 , 1.2177
0.16181618 , 0.1294
1.5594554 , 1.553022
0.42316228 , 0.482621
2.900306 , 3.5232
1.5766315 , 1.5979
0.464973 , 0.3939
0.08329731 , 0.106425
0.35823408 , 0.426251
1.968601 , 1.9493
0.2121037 , 0.202608
0.22640595 , 0.248158
0.13385063 , 0.162907
0.1446845 , 0.0969
1.5186785 , 1.4747
0.17876193 , 0.160968
0.20566815 , 0.221452
0.34534568 , 0.2872
1.2911999 , 1.283558
1.1738669 , 1.1706
0.19387624 , 0.206257
0.86098135 , 0.86372
1.1355 , 1.1371
1.1500947 , 1.1665
1.0685174 , 1.085815
0.33677673 , 0.411959
0.17756909 , 0.1641
0.15138817 , 0.139459
0.15119085 , 0.154059
0.23281571 , 0.192
2.4173887 , 2.3342
2.0243173 , 2.037377
2.0482965 , 2.1008
0.24022326 , 0.275436
0.12950486 , 0.1461
1.6096265 , 1.638854
0.64052904 , 0.540229
1.9543198 , 1.933091
1.3753208 , 1.370053
4.636406 , 4.6728
1.220629 , 1.2592
2.7349393 , 2.632
0.53694004 , 0.5304
1.3602787 , 1.3092
4.885729 , 4.8909
0.2069419 , 0.203553
0.07949698 , 0.058264
0.23252621 , 0.309551
0.40133083 , 0.423418
2.070295 , 2.1961
1.6096265 , 1.630678
0.89817476 , 0.8925
1.0685174 , 1.24401
1.5573604 , 1.536141
0.14180282 , 0.137186
1.6481102 , 1.673119
1.1243424 , 1.145255
0.10992974 , 0.12424
0.14180282 , 0.13803
0.22168708 , 0.226863
0.24425274 , 0.2445
0.22168708 , 0.230768
1.3542128 , 1.4644
0.9368976 , 0.959842
1.4571333 , 1.466184
0.19831932 , 0.198629
2.410023 , 2.427
0.9564922 , 0.969919
1.5594554 , 1.570052
1.3183727 , 1.2899
0.37976858 , 0.462038
0.12590921 , 0.12369
0.51816314 , 0.2709
0.08652687 , 0.092292
0.20566815 , 0.22207
0.6353621 , 0.5861
0.19734398 , 0.187
5.0825596 , 4.9638
0.07949698 , 0.050546
0.35823408 , 0.482743
0.3145942 , 0.357883
0.094280005 , 0.103838
0.17675027 , 0.1839
0.37976858 , 0.490113
1.3130984 , 1.2962
2.0243173 , 2.038739
0.55029994 , 0.53172
0.5272702 , 0.4878
0.19387624 , 0.218597
0.13823116 , 0.1004
0.18868822 , 0.176042
0.08652687 , 0.100625
1.3483788 , 1.3892
0.2871277 , 0.2584
0.15138817 , 0.186847
1.3753208 , 1.37222
0.31910136 , 0.2821
0.19873402 , 0.215753
0.08138859 , 0.068466
0.081638336 , 0.076437
0.44672114 , 0.450596
0.39044124 , 0.3662
0.15917212 , 0.1387
0.07893604 , 0.037108
1.3753208 , 1.372113
0.079146564 , 0.039252
0.15138817 , 0.138901
1.0659109 , 0.417416
0.19252542 , 0.190654
0.5387456 , 0.404046
0.08138859 , 0.068638
2.0867615 , 2.134584
0.2194488 , 0.214634
0.32245564 , 0.2965
0.24467716 , 0.213
0.079146564 , 0.038982
0.20566815 , 0.220844
0.24022326 , 0.275346
0.27045438 , 0.314547
0.9368976 , 0.952743
0.18901053 , 0.163369
0.17970008 , 0.215152
1.2356749 , 1.24896
1.1491494 , 1.1633
0.52013767 , 0.4877
0.14180282 , 0.138779
0.2223824 , 0.2364
1.8842868 , 1.871532
0.31071195 , 0.2551
0.78629047 , 0.756073
0.90027666 , 0.944153
2.6914885 , 2.5852
0.19387624 , 0.207018
0.08652687 , 0.089502
0.25293446 , 0.341253
0.45913205 , 0.4074
0.18901053 , 0.162526
1.2840315 , 1.281904
4.2882957 , 4.2777
0.40133083 , 0.430845
4.7402306 , 4.6856
0.69367194 , 0.8434
1.6481102 , 1.600407
0.26078936 , 0.2329
1.410078 , 1.396
0.54440826 , 0.4881
0.13800454 , 0.1241
2.3540895 , 2.4786
0.5455961 , 0.516329
0.83512676 , 0.8024
0.4064191 , 0.3841
0.67715925 , 0.591383
1.5446795 , 1.5052
0.63067806 , 0.5762
0.20566815 , 0.221472
0.60112035 , 0.6147
4.586731 , 4.9581
0.23470905 , 0.1965
0.34443066 , 0.3094
0.17876193 , 0.160982
1.8845208 , 1.8274
0.6523302 , 0.6151
1.6481102 , 1.585764
1.401899 , 1.56758
0.5695177 , 0.5279
0.42316228 , 0.483268
1.5664293 , 1.6265
0.33409587 , 0.2866
1.5190942 , 1.5022
4.4257126 , 4.2849
0.117979765 , 0.128742
1.8142767 , 1.775363
0.3145942 , 0.358138
0.20566815 , 0.228926
0.21901879 , 0.210101
0.46294227 , 0.3875
0.6620834 , 0.6629
1.0227869 , 1.083581
0.22640595 , 0.250154
0.12577528 , 0.132327
0.2813473 , 0.2597
4.72171 , 4.8759
0.12590921 , 0.123971
0.12577528 , 0.139124
0.5870128 , 0.618787
0.09414637 , 0.099704
0.9082477 , 0.9658
0.24022326 , 0.275259
0.633857 , 0.666838
0.102029026 , 0.112286
0.09336871 , 0.0678
0.081638336 , 0.076351
1.2167612 , 1.2504
1.0533131 , 1.067
0.24022326 , 0.320047
0.13678974 , 0.0921
0.64052904 , 0.6499
0.094280005 , 0.132185
0.2756312 , 0.2483
0.22597426 , 0.217783
1.8455747 , 2.076
2.17747 , 2.279909
0.120708525 , 0.0651
0.2510523 , 0.2776
1.2911999 , 1.290569
4.205082 , 4.1431
1.825826 , 1.879
0.31292298 , 0.3007
0.29247284 , 0.3313
1.7728575 , 1.7557
0.14122686 , 0.1133
1.5183785 , 1.4697
1.7356998 , 1.731266
1.1621554 , 1.1619
1.0821667 , 1.0243
4.790102 , 4.8358
0.12577528 , 0.131828
2.1398354 , 2.133551
1.8142767 , 1.75349
0.16030931 , 0.184595
1.8842868 , 1.87328
2.0243173 , 2.039546
0.12577528 , 0.131404
1.0125718 , 1.027257
0.12577528 , 0.136674
0.102163374 , 0.111257
0.18307212 , 0.167
1.5594554 , 1.573698
0.102029026 , 0.106428
0.08304179 , 0.108438
0.22597426 , 0.215518
0.117979765 , 0.11332
2.17747 , 2.282164
0.31435475 , 0.2638
1.6486006 , 1.6471
0.081638336 , 0.076419
1.0667462 , 1.0255
1.2911999 , 1.308064
0.43645477 , 0.3908
0.07893604 , 0.037145
0.16943407 , 0.167055
0.49859715 , 0.630346
0.6870769 , 0.6475
0.6469752 , 0.5872
0.15791708 , 0.1332
1.2500263 , 1.2311
0.07893604 , 0.050898
0.33677673 , 0.41043
0.8175721 , 0.7951
0.84080815 , 0.822
2.238596 , 2.455667
0.08263767 , 0.081723
0.4725948 , 0.553396
0.22168708 , 0.225333
0.9564922 , 0.972668
0.35823408 , 0.390063
4.786646 , 4.8441
0.079707205 , 0.057712
1.0685174 , 1.089097
0.1313529 , 0.0967
1.3753208 , 1.372744
0.11088014 , 0.147088
1.2840315 , 1.280387
0.13385063 , 0.125436
1.5573604 , 1.54919
0.11879605 , 0.161258
0.9039831 , 0.9385
0.10000658 , 0.1026
0.5250199 , 0.605692
0.13371623 , 0.138005
0.40136448 , 0.3705
0.15138817 , 0.144523
1.401899 , 1.413819
0.42316228 , 0.484852
0.42316228 , 0.484796
0.12590921 , 0.124307
0.14362657 , 0.1286
1.2356749 , 1.290783
1.0685174 , 1.089133
0.5250199 , 0.600886
1.0685174 , 1.090062
1.1346602 , 1.139
4.0967336 , 3.9459
0.15950727 , 0.1394
0.786496 , 0.7592
0.12577528 , 0.132173
0.24816138 , 0.2359
0.17856905 , 0.184853
1.8142767 , 1.75276
0.28270265 , 0.2558
1.5092685 , 1.532397
0.15119085 , 0.151746
2.4350452 , 2.4073
0.29247284 , 0.3319
1.346594 , 1.35986
1.1937487 , 1.229886
0.86098135 , 0.868373
2.238596 , 2.475735
0.3670767 , 0.3311
0.6223962 , 0.229719
0.45491213 , 0.3792
0.17874226 , 0.1287
0.2702463 , 0.2635
0.31031588 , 0.3165
0.15119085 , 0.147976
0.44672114 , 0.461135
1.2911999 , 1.28771
0.90027666 , 0.922221
0.53599 , 0.4738
1.8142767 , 1.758141
0.15119085 , 0.175217
1.3641405 , 1.3656
0.0981195 , 0.0344
0.5387456 , 0.403665
0.26809937 , 0.2479
0.08263767 , 0.08177
0.16144031 , 0.204651
0.11006397 , 0.110113
0.42316478 , 0.4001
0.25092608 , 0.2369
2.050367 , 2.2052
0.11780268 , 0.0642
1.3468218 , 1.3292
1.0374292 , 1.0403
1.5983782 , 1.5991
0.55029994 , 0.619291
4.1581783 , 4.2924
1.0685174 , 1.081203
0.40640607 , 0.3704
1.7285197 , 1.7204
0.79439116 , 0.7735
0.4268689 , 0.3835
2.653082 , 2.4831
0.8642776 , 0.87119
0.3619481 , 0.2946
4.242471 , 4.3279
1.7707592 , 1.7504
0.2194488 , 0.220509
0.14180282 , 0.137724
0.4725948 , 0.553428
0.2364015 , 0.1913
2.2080252 , 2.400935
0.10992974 , 0.131816
0.15838873 , 0.1283
1.3735443 , 1.3954
0.63814366 , 0.577423
0.86361533 , 0.8304
1.0125718 , 1.040113
0.44672114 , 0.458069
1.6481102 , 1.667526
0.5605687 , 0.5054
0.117845714 , 0.122425
0.63814366 , 0.57444
0.80930907 , 0.7975
0.28373224 , 0.2626
0.25293446 , 0.341718
0.90027666 , 0.918172
1.0227869 , 1.085601
0.3029638 , 0.2544
1.9685615 , 1.9568
0.6346236 , 0.6093
1.7356998 , 1.713191
0.5465782 , 0.4995
4.2851152 , 4.7698
1.7356998 , 1.712502
0.22095251 , 0.2202
0.15119085 , 0.151812
1.8021799 , 1.726
0.09414637 , 0.11414
0.67192084 , 0.6497
1.7356998 , 1.738508
0.18901053 , 0.16217
1.1854341 , 1.2218
RMSE:  0.08345344709669646  MAPE: 0.11064218315851876
5: ground truth total-  393  predicted total -  393
100: ground truth total-  0  predicted total -  0
 more 100: ground truth total -  0  predicted total -  0
