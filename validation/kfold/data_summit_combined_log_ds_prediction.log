['Outer', 'Inner', 'Reduction', 'VarDecl', 'refExpr', 'intLiteral', 'floatLiteral', 'mem_to', 'mem_from', 'add_sub_int', 'add_sub_double', 'mul_int', 'mul_double', 'div_int', 'div_double', 'assign_int', 'assign_double', 'log_Outer', 'log_Inner', 'log_VarDecl', 'log_refExpr', 'log_intLiteral', 'log_floatLiteral', 'log_mem_to', 'log_mem_from', 'log_add_sub_int', 'log_add_sub_double', 'log_mul_int', 'log_mul_double', 'log_div_int', 'log_div_double', 'log_assign_int', 'log_assign_double', 'runtimes']
914
34
<class 'numpy.dtype'> float64
914
train batches:  83  validate samples: 73  test samples: 182
Epoch [0/150], Batch loss: 28.119
Epoch: 0 RMSE:  16.694605170653062  MAPE: 3.3500764520775177  L2+L1 loss: 4.135
Epoch [1/150], Batch loss: 11.715
Epoch: 1 RMSE:  11.208208949843463  MAPE: 7.875994304605286  L2+L1 loss: 4.24
Epoch [2/150], Batch loss: 10.309
Epoch: 2 RMSE:  11.448603889637853  MAPE: 11.26205544284843  L2+L1 loss: 5.505
Epoch [3/150], Batch loss: 11.168
Epoch: 3 RMSE:  11.587474237661294  MAPE: 12.409877727681147  L2+L1 loss: 6.033
Epoch [4/150], Batch loss: 11.505
Epoch: 4 RMSE:  11.32204217225403  MAPE: 9.91414455407575  L2+L1 loss: 4.9
Epoch [5/150], Batch loss: 10.762
Epoch: 5 RMSE:  11.62831059954579  MAPE: 12.709574865051168  L2+L1 loss: 6.171
Epoch [6/150], Batch loss: 10.464
Epoch: 6 RMSE:  11.372878351490387  MAPE: 10.507519101934754  L2+L1 loss: 5.156
Epoch [7/150], Batch loss: 11.169
Epoch: 7 RMSE:  11.497705978913364  MAPE: 11.695500651563348  L2+L1 loss: 5.706
Epoch [8/150], Batch loss: 11.162
Epoch: 8 RMSE:  11.299136074239907  MAPE: 9.611048326840539  L2+L1 loss: 4.767
Epoch [9/150], Batch loss: 10.565
Epoch: 9 RMSE:  11.466240685015233  MAPE: 11.42194843195036  L2+L1 loss: 5.579
Epoch [10/150], Batch loss: 10.51
Epoch: 10 RMSE:  11.627020182035887  MAPE: 12.700294999717702  L2+L1 loss: 6.167
Epoch [11/150], Batch loss: 11.046
Epoch: 11 RMSE:  12.154100222221487  MAPE: 15.805463861841165  L2+L1 loss: 7.682
Epoch [12/150], Batch loss: 11.557
Epoch: 12 RMSE:  11.826287214671067  MAPE: 14.009789142756718  L2+L1 loss: 6.81
Epoch [13/150], Batch loss: 11.375
Epoch: 13 RMSE:  11.254500973099525  MAPE: 8.921917187430362  L2+L1 loss: 4.533
Epoch [14/150], Batch loss: 11.13
Epoch: 14 RMSE:  11.235087733639116  MAPE: 8.551267448142767  L2+L1 loss: 4.417
Epoch [15/150], Batch loss: 10.662
Epoch: 15 RMSE:  11.50975156200029  MAPE: 11.796652351423576  L2+L1 loss: 5.753
Epoch [16/150], Batch loss: 10.812
Epoch: 16 RMSE:  11.421549928323948  MAPE: 11.00621493622001  L2+L1 loss: 5.384
Epoch [17/150], Batch loss: 11.435
Epoch: 17 RMSE:  11.272480569211515  MAPE: 9.2199125624034  L2+L1 loss: 4.627
Epoch [18/150], Batch loss: 11.33
Epoch: 18 RMSE:  11.34957259328676  MAPE: 10.246765967136776  L2+L1 loss: 5.043
Epoch [19/150], Batch loss: 11.048
Epoch: 19 RMSE:  11.3268687249336  MAPE: 9.97474068335477  L2+L1 loss: 4.927
Epoch [20/150], Batch loss: 11.095
Epoch: 20 RMSE:  11.293828919712361  MAPE: 9.536884388602713  L2+L1 loss: 4.738
Epoch [21/150], Batch loss: 25.424
Epoch: 21 RMSE:  11.21227842738517  MAPE: 7.999386731230939  L2+L1 loss: 4.271
Epoch [22/150], Batch loss: 10.496
Epoch: 22 RMSE:  11.795941734816138  MAPE: 13.824162095486688  L2+L1 loss: 6.72
Epoch [23/150], Batch loss: 11.102
Epoch: 23 RMSE:  11.407631638946143  MAPE: 10.869032297473776  L2+L1 loss: 5.319
Epoch [24/150], Batch loss: 12.555
Epoch: 24 RMSE:  11.562736212352315  MAPE: 12.221430126124766  L2+L1 loss: 5.948
Epoch [25/150], Batch loss: 12.561
Epoch: 25 RMSE:  82.24174211180987  MAPE: 7.064154226115232  L2+L1 loss: 29.326
Epoch [26/150], Batch loss: 12.342
Epoch: 26 RMSE:  11.551350709525744  MAPE: 12.132708726961198  L2+L1 loss: 5.907
Epoch [27/150], Batch loss: 14.7
Epoch: 27 RMSE:  11.398294623906242  MAPE: 10.796215959690223  L2+L1 loss: 5.282
Epoch [28/150], Batch loss: 13.232
Epoch: 28 RMSE:  11.545152753641966  MAPE: 12.083852871853884  L2+L1 loss: 5.885
Epoch [29/150], Batch loss: 12.16
Epoch: 29 RMSE:  11.193591230398559  MAPE: 7.254790618142638  L2+L1 loss: 4.079
Epoch [30/150], Batch loss: 10.045
Epoch: 30 RMSE:  11.194717857409282  MAPE: 7.322985715777421  L2+L1 loss: 4.096
Epoch [31/150], Batch loss: 9.752
Epoch: 31 RMSE:  11.206458151423018  MAPE: 7.940079228053971  L2+L1 loss: 4.235
Epoch [32/150], Batch loss: 10.645
Epoch: 32 RMSE:  11.231044411318692  MAPE: 8.465079609745016  L2+L1 loss: 4.391
Epoch [33/150], Batch loss: 10.469
Epoch: 33 RMSE:  11.254581568312595  MAPE: 8.923336904501495  L2+L1 loss: 4.533
Epoch [34/150], Batch loss: 11.253
Epoch: 34 RMSE:  11.284214967313547  MAPE: 9.459263890730533  L2+L1 loss: 4.692
Epoch [35/150], Batch loss: 10.707
Epoch: 35 RMSE:  11.314703807292396  MAPE: 10.013721810929217  L2+L1 loss: 4.864
Epoch [36/150], Batch loss: 9.797
Epoch: 36 RMSE:  3.695551166997667  MAPE: 2.805719444708407  L2+L1 loss: 3.016
Epoch [37/150], Batch loss: 3.18
Epoch: 37 RMSE:  3.9555937498751383  MAPE: 0.8680844921060097  L2+L1 loss: 1.547
Epoch [38/150], Batch loss: 2.075
Epoch: 38 RMSE:  1.167080857667669  MAPE: 0.4720021715831344  L2+L1 loss: 0.955
Epoch [39/150], Batch loss: 1.676
Epoch: 39 RMSE:  1.3011472726609417  MAPE: 0.4959562693146974  L2+L1 loss: 0.9
Epoch [40/150], Batch loss: 1.771
Epoch: 40 RMSE:  5.804027105975956  MAPE: 4.983862323372273  L2+L1 loss: 2.568
Epoch [41/150], Batch loss: 3.866
Epoch: 41 RMSE:  8.194892065173466  MAPE: 5.925150497419257  L2+L1 loss: 3.365
Epoch [42/150], Batch loss: 3.442
Epoch: 42 RMSE:  2.7209820678546315  MAPE: 5.527617129041092  L2+L1 loss: 2.156
Epoch [43/150], Batch loss: 2.644
Epoch: 43 RMSE:  1.0652447036629233  MAPE: 0.8968132306658912  L2+L1 loss: 1.176
Epoch [44/150], Batch loss: 1.43
Epoch: 44 RMSE:  0.6739143171161114  MAPE: 0.7772155206894856  L2+L1 loss: 0.907
Epoch [45/150], Batch loss: 1.329
Epoch: 45 RMSE:  1.1956614228961364  MAPE: 0.8897736390311647  L2+L1 loss: 0.992
Epoch [46/150], Batch loss: 1.333
Epoch: 46 RMSE:  3.6685660771202717  MAPE: 0.32872307351395375  L2+L1 loss: 1.168
Epoch [47/150], Batch loss: 1.735
Epoch: 47 RMSE:  0.6516590006263836  MAPE: 0.5851657299215659  L2+L1 loss: 0.766
Epoch [48/150], Batch loss: 1.307
Epoch: 48 RMSE:  1.8635611154344636  MAPE: 0.5461264014418772  L2+L1 loss: 1.237
Epoch [49/150], Batch loss: 2.071
Epoch: 49 RMSE:  1.8246803410625718  MAPE: 0.8131881314946449  L2+L1 loss: 1.409
Epoch [50/150], Batch loss: 2.912
Epoch: 50 RMSE:  3.0875259948522693  MAPE: 1.221780897626999  L2+L1 loss: 1.579
Epoch [51/150], Batch loss: 1.865
Epoch: 51 RMSE:  1.4229636812878288  MAPE: 1.5240644704474524  L2+L1 loss: 1.36
Epoch [52/150], Batch loss: 4.141
Epoch: 52 RMSE:  1.0981484464517415  MAPE: 1.4456081240727119  L2+L1 loss: 1.224
Epoch [53/150], Batch loss: 1.363
Epoch: 53 RMSE:  0.8964353746504578  MAPE: 1.550030247798474  L2+L1 loss: 1.03
Epoch [54/150], Batch loss: 3.129
Epoch: 54 RMSE:  1.8921933217456675  MAPE: 0.5135573988206  L2+L1 loss: 0.955
Epoch [55/150], Batch loss: 1.574
Epoch: 55 RMSE:  0.7087189706660385  MAPE: 0.7971456596668769  L2+L1 loss: 0.839
Epoch [56/150], Batch loss: 1.376
Epoch: 56 RMSE:  2.5906191200204214  MAPE: 0.5340594415167504  L2+L1 loss: 1.214
Epoch [57/150], Batch loss: 1.706
Epoch: 57 RMSE:  0.605917286400735  MAPE: 0.5845626429998445  L2+L1 loss: 0.774
Epoch [58/150], Batch loss: 1.969
Epoch: 58 RMSE:  5.664227997728816  MAPE: 1.1296579198188215  L2+L1 loss: 1.53
Epoch [59/150], Batch loss: 2.944
Epoch: 59 RMSE:  2.2505768373742416  MAPE: 2.732053702818041  L2+L1 loss: 1.794
Epoch [60/150], Batch loss: 1.322
Epoch: 60 RMSE:  0.5705088535130182  MAPE: 0.6040178031978717  L2+L1 loss: 0.724
Epoch [61/150], Batch loss: 1.058
Epoch: 61 RMSE:  0.7443899021608837  MAPE: 0.3947600980577097  L2+L1 loss: 0.708
Epoch [62/150], Batch loss: 1.017
Epoch: 62 RMSE:  0.7369742667232441  MAPE: 0.31724377697745404  L2+L1 loss: 0.683
Epoch [63/150], Batch loss: 1.062
Epoch: 63 RMSE:  0.7679359018894555  MAPE: 0.442077873813253  L2+L1 loss: 0.68
Epoch [64/150], Batch loss: 0.936
Epoch: 64 RMSE:  0.607232527250895  MAPE: 0.24413641918836174  L2+L1 loss: 0.6
Epoch [65/150], Batch loss: 0.927
Epoch: 65 RMSE:  0.5964837974161943  MAPE: 0.22468182886421034  L2+L1 loss: 0.621
Epoch [66/150], Batch loss: 0.997
Epoch: 66 RMSE:  0.6168145609314004  MAPE: 0.30648317945774817  L2+L1 loss: 0.637
Epoch [67/150], Batch loss: 0.83
Epoch: 67 RMSE:  0.6836752462662107  MAPE: 0.2098842458544821  L2+L1 loss: 0.583
Epoch [68/150], Batch loss: 0.738
Epoch: 68 RMSE:  0.635871585540782  MAPE: 0.2160851729035429  L2+L1 loss: 0.608
Epoch [69/150], Batch loss: 0.771
Epoch: 69 RMSE:  0.36422298028675815  MAPE: 0.2354969809294959  L2+L1 loss: 0.529
Epoch [70/150], Batch loss: 0.782
Epoch: 70 RMSE:  0.631100806837305  MAPE: 0.22831517011406718  L2+L1 loss: 0.578
Epoch [71/150], Batch loss: 0.892
Epoch: 71 RMSE:  0.4694331400595978  MAPE: 0.265110347651135  L2+L1 loss: 0.58
Epoch [72/150], Batch loss: 0.931
Epoch: 72 RMSE:  0.44027434492321216  MAPE: 0.22086848715404384  L2+L1 loss: 0.547
Epoch [73/150], Batch loss: 0.758
Epoch: 73 RMSE:  0.8962303889206594  MAPE: 0.19016331094682523  L2+L1 loss: 0.599
Epoch [74/150], Batch loss: 0.815
Epoch: 74 RMSE:  0.6179054125130519  MAPE: 0.17379097130502147  L2+L1 loss: 0.529
Epoch [75/150], Batch loss: 0.85
Epoch: 75 RMSE:  0.8151027960646826  MAPE: 0.14977474059308124  L2+L1 loss: 0.575
Epoch [76/150], Batch loss: 0.745
Epoch: 76 RMSE:  0.4589274726607894  MAPE: 0.16872461386656648  L2+L1 loss: 0.503
Epoch [77/150], Batch loss: 0.714
Epoch: 77 RMSE:  0.6180342539239779  MAPE: 0.14107764908347345  L2+L1 loss: 0.568
Epoch [78/150], Batch loss: 0.69
Epoch: 78 RMSE:  0.41486916338273677  MAPE: 0.32763434392766205  L2+L1 loss: 0.572
Epoch [79/150], Batch loss: 0.931
Epoch: 79 RMSE:  0.7942641333977036  MAPE: 0.13936058902829468  L2+L1 loss: 0.541
Epoch [80/150], Batch loss: 0.683
Epoch: 80 RMSE:  1.3233197172326232  MAPE: 0.15344338104387306  L2+L1 loss: 0.696
Epoch [81/150], Batch loss: 0.763
Epoch: 81 RMSE:  0.40834484370401675  MAPE: 0.28965418721141395  L2+L1 loss: 0.519
Epoch [82/150], Batch loss: 0.765
Epoch: 82 RMSE:  0.9099002537708161  MAPE: 0.39904912120960256  L2+L1 loss: 0.744
Epoch [83/150], Batch loss: 0.83
Epoch: 83 RMSE:  0.7967180206593317  MAPE: 0.1759719155037099  L2+L1 loss: 0.547
Epoch [84/150], Batch loss: 0.709
Epoch: 84 RMSE:  0.5347786295209107  MAPE: 0.1923590665982147  L2+L1 loss: 0.523
Epoch [85/150], Batch loss: 0.632
Epoch: 85 RMSE:  0.43066650146273155  MAPE: 0.1483401223222733  L2+L1 loss: 0.483
Epoch [86/150], Batch loss: 0.842
Epoch: 86 RMSE:  0.32300779180348577  MAPE: 0.27240347236037243  L2+L1 loss: 0.47
Epoch [87/150], Batch loss: 0.704
Epoch: 87 RMSE:  0.6578556960070939  MAPE: 0.4377508524077575  L2+L1 loss: 0.619
Epoch [88/150], Batch loss: 0.689
Epoch: 88 RMSE:  0.6610054107506009  MAPE: 0.20420575229282734  L2+L1 loss: 0.56
Epoch [89/150], Batch loss: 0.791
Epoch: 89 RMSE:  1.2808214150404236  MAPE: 0.49787000720313535  L2+L1 loss: 0.682
Epoch [90/150], Batch loss: 0.711
Epoch: 90 RMSE:  0.6943016896073979  MAPE: 0.11877419291812973  L2+L1 loss: 0.476
Epoch [91/150], Batch loss: 0.58
Epoch: 91 RMSE:  0.62758356494727  MAPE: 0.12209962877580222  L2+L1 loss: 0.468
Epoch [92/150], Batch loss: 0.596
Epoch: 92 RMSE:  0.5339263265335537  MAPE: 0.12160383810966599  L2+L1 loss: 0.465
Epoch [93/150], Batch loss: 0.602
Epoch: 93 RMSE:  0.5238000405163117  MAPE: 0.12443487098926773  L2+L1 loss: 0.471
Epoch [94/150], Batch loss: 0.58
Epoch: 94 RMSE:  0.5628731899481315  MAPE: 0.1234865895909545  L2+L1 loss: 0.456
Epoch [95/150], Batch loss: 0.577
Epoch: 95 RMSE:  0.4834008847882716  MAPE: 0.12390783867061872  L2+L1 loss: 0.449
Epoch [96/150], Batch loss: 0.584
Epoch: 96 RMSE:  0.7633389962882255  MAPE: 0.16177894373359203  L2+L1 loss: 0.499
Epoch [97/150], Batch loss: 0.559
Epoch: 97 RMSE:  0.47025398528091517  MAPE: 0.11288131288875396  L2+L1 loss: 0.454
Epoch [98/150], Batch loss: 0.597
Epoch: 98 RMSE:  0.6029412459204234  MAPE: 0.10169708486175393  L2+L1 loss: 0.462
Epoch [99/150], Batch loss: 0.581
Epoch: 99 RMSE:  0.49705628227392834  MAPE: 0.1404805738418933  L2+L1 loss: 0.463
Epoch [100/150], Batch loss: 0.587
Epoch: 100 RMSE:  0.5317518821542865  MAPE: 0.10548270185418589  L2+L1 loss: 0.459
Epoch [101/150], Batch loss: 0.577
Epoch: 101 RMSE:  0.6450857145225539  MAPE: 0.10634178611664902  L2+L1 loss: 0.474
Epoch [102/150], Batch loss: 0.591
Epoch: 102 RMSE:  0.6012353015228672  MAPE: 0.10855499807657666  L2+L1 loss: 0.464
Epoch [103/150], Batch loss: 0.577
Epoch: 103 RMSE:  0.6652991087129109  MAPE: 0.1239583470140601  L2+L1 loss: 0.488
Epoch [104/150], Batch loss: 0.552
Epoch: 104 RMSE:  0.5560479768020865  MAPE: 0.10324657834252815  L2+L1 loss: 0.458
Epoch [105/150], Batch loss: 0.559
Epoch: 105 RMSE:  0.6385219891319003  MAPE: 0.14329489514371588  L2+L1 loss: 0.484
Epoch [106/150], Batch loss: 0.534
Epoch: 106 RMSE:  0.45085487395146884  MAPE: 0.10512632757112433  L2+L1 loss: 0.443
Epoch [107/150], Batch loss: 0.569
Epoch: 107 RMSE:  0.7390634701424383  MAPE: 0.10529313772997315  L2+L1 loss: 0.487
Epoch [108/150], Batch loss: 0.608
Epoch: 108 RMSE:  0.7068472202602112  MAPE: 0.11737823893282172  L2+L1 loss: 0.486
Epoch [109/150], Batch loss: 0.581
Epoch: 109 RMSE:  0.6070454525491239  MAPE: 0.11306389727050233  L2+L1 loss: 0.474
Epoch [110/150], Batch loss: 0.566
Epoch: 110 RMSE:  0.7907330127359665  MAPE: 0.11359887978098239  L2+L1 loss: 0.503
Epoch [111/150], Batch loss: 0.574
Epoch: 111 RMSE:  0.7336658961933854  MAPE: 0.12205484581468427  L2+L1 loss: 0.504
Epoch [112/150], Batch loss: 0.552
Epoch: 112 RMSE:  0.6411825648761363  MAPE: 0.11447053279859182  L2+L1 loss: 0.477
Epoch [113/150], Batch loss: 0.553
Epoch: 113 RMSE:  0.50633982795337  MAPE: 0.10843247630171572  L2+L1 loss: 0.458
Epoch [114/150], Batch loss: 0.569
Epoch: 114 RMSE:  0.6081471915433869  MAPE: 0.10491773684014953  L2+L1 loss: 0.471
Epoch [115/150], Batch loss: 0.577
Epoch: 115 RMSE:  0.5052229811703935  MAPE: 0.1255983361220383  L2+L1 loss: 0.465
Epoch [116/150], Batch loss: 0.575
Epoch: 116 RMSE:  0.6283027401065837  MAPE: 0.12906342726521755  L2+L1 loss: 0.486
Epoch [117/150], Batch loss: 0.553
Epoch: 117 RMSE:  0.6366581824522797  MAPE: 0.1179363476920936  L2+L1 loss: 0.481
Epoch [118/150], Batch loss: 0.578
Epoch: 118 RMSE:  0.6596009131542862  MAPE: 0.10439280823406961  L2+L1 loss: 0.476
Epoch [119/150], Batch loss: 0.582
Epoch: 119 RMSE:  0.6364671627897852  MAPE: 0.11241512357991439  L2+L1 loss: 0.481
Epoch [120/150], Batch loss: 0.542
Epoch: 120 RMSE:  0.6388175669886563  MAPE: 0.10245831446359786  L2+L1 loss: 0.475
Epoch [121/150], Batch loss: 0.544
Epoch: 121 RMSE:  0.6368439725609466  MAPE: 0.10223588373821363  L2+L1 loss: 0.47
Epoch [122/150], Batch loss: 0.55
Epoch: 122 RMSE:  0.6389710773703254  MAPE: 0.10222572585025906  L2+L1 loss: 0.469
Epoch [123/150], Batch loss: 0.548
Epoch: 123 RMSE:  0.6401350654637401  MAPE: 0.1024838449884071  L2+L1 loss: 0.468
Epoch [124/150], Batch loss: 0.545
Epoch: 124 RMSE:  0.6429160602731482  MAPE: 0.10223325544385146  L2+L1 loss: 0.468
Epoch [125/150], Batch loss: 0.542
Epoch: 125 RMSE:  0.6366407683829067  MAPE: 0.10396019231700766  L2+L1 loss: 0.47
Epoch [126/150], Batch loss: 0.555
Epoch: 126 RMSE:  0.641284508707392  MAPE: 0.10449683995096622  L2+L1 loss: 0.471
Epoch [127/150], Batch loss: 0.534
Epoch: 127 RMSE:  0.6426468434106651  MAPE: 0.1019887982860417  L2+L1 loss: 0.469
Epoch [128/150], Batch loss: 0.533
Epoch: 128 RMSE:  0.6453440330431097  MAPE: 0.10267892155323378  L2+L1 loss: 0.469
Epoch [129/150], Batch loss: 0.539
Epoch: 129 RMSE:  0.6415186827281828  MAPE: 0.10644915627519137  L2+L1 loss: 0.472
Epoch [130/150], Batch loss: 0.544
Epoch: 130 RMSE:  0.6455950230793794  MAPE: 0.10331997048613152  L2+L1 loss: 0.471
Epoch [131/150], Batch loss: 0.548
Epoch: 131 RMSE:  0.6490196330003845  MAPE: 0.10550737080123711  L2+L1 loss: 0.473
Epoch [132/150], Batch loss: 0.544
Epoch: 132 RMSE:  0.6468461866680563  MAPE: 0.10502896897579164  L2+L1 loss: 0.473
Epoch [133/150], Batch loss: 0.56
Epoch: 133 RMSE:  0.6447280664369361  MAPE: 0.10650978048628698  L2+L1 loss: 0.473
Epoch [134/150], Batch loss: 0.551
Epoch: 134 RMSE:  0.6496846857469784  MAPE: 0.10202498180101933  L2+L1 loss: 0.471
Epoch [135/150], Batch loss: 0.533
Epoch: 135 RMSE:  0.6561531177501395  MAPE: 0.10208588063317262  L2+L1 loss: 0.472
Epoch [136/150], Batch loss: 0.532
Epoch: 136 RMSE:  0.6437909116928007  MAPE: 0.10439550824989144  L2+L1 loss: 0.472
Epoch [137/150], Batch loss: 0.536
Epoch: 137 RMSE:  0.6545419608417974  MAPE: 0.1031888441298493  L2+L1 loss: 0.473
Epoch [138/150], Batch loss: 0.555
Epoch: 138 RMSE:  0.648979048995454  MAPE: 0.10599037703969173  L2+L1 loss: 0.474
Epoch [139/150], Batch loss: 0.548
Epoch: 139 RMSE:  0.6520566323324578  MAPE: 0.10362711356864256  L2+L1 loss: 0.473
Epoch [140/150], Batch loss: 0.549
Epoch: 140 RMSE:  0.6474298980599533  MAPE: 0.1060249690004504  L2+L1 loss: 0.474
Epoch [141/150], Batch loss: 0.543
Epoch: 141 RMSE:  0.6484840728343821  MAPE: 0.10378250799184009  L2+L1 loss: 0.473
Epoch [142/150], Batch loss: 0.545
Epoch: 142 RMSE:  0.645991223673578  MAPE: 0.10526291149218406  L2+L1 loss: 0.474
Epoch [143/150], Batch loss: 0.531
Epoch: 143 RMSE:  0.6533003498210818  MAPE: 0.10296747448272695  L2+L1 loss: 0.474
Epoch [144/150], Batch loss: 0.545
Epoch: 144 RMSE:  0.6560837947099427  MAPE: 0.10271719311732709  L2+L1 loss: 0.474
Epoch [145/150], Batch loss: 0.518
Epoch: 145 RMSE:  0.6583750607993517  MAPE: 0.1064049299485565  L2+L1 loss: 0.477
Epoch [146/150], Batch loss: 0.547
Epoch: 146 RMSE:  0.6579373879748326  MAPE: 0.10589882596474105  L2+L1 loss: 0.477
Epoch [147/150], Batch loss: 0.529
Epoch: 147 RMSE:  0.655971138440606  MAPE: 0.10536102214125434  L2+L1 loss: 0.476
Epoch [148/150], Batch loss: 0.546
Epoch: 148 RMSE:  0.6496043825717023  MAPE: 0.10501452640847636  L2+L1 loss: 0.475
Epoch [149/150], Batch loss: 0.539
Epoch: 149 RMSE:  0.6561303081590643  MAPE: 0.10504436014404438  L2+L1 loss: 0.476


Evaluating Model.......
Best Model - RMSE: inf  MAPE: inf  L2+L1- inf
predicted_runtime, ground_truth
3.2831519 , 3.3893
3.7935658 , 3.3912
0.13823414 , 0.1589
0.8700018 , 0.9483
0.1189847 , 0.165
1.5395246 , 1.5746
2.3451877 , 2.1911
0.20000005 , 0.1878
6.4649944 , 4.6659
2.6852098 , 2.83
3.994999 , 4.1888
1.33747 , 1.4115
0.09589267 , 0.1511
1.6541224 , 1.2674
2.902441 , 3.3777
0.5483346 , 0.4372
3.2460446 , 3.6653
0.76146173 , 0.7782
0.40940142 , 0.2898
5.5505934 , 5.4434
1.0591209 , 0.8381
0.20101547 , 0.2028
4.5979977 , 5.9476
2.6006098 , 2.7448
0.13175488 , 0.1274
1.9847264 , 1.8589
0.3210907 , 0.2363
0.083607435 , 0.1104
0.65778255 , 0.6252
0.44994593 , 0.4716
5.9477324 , 5.3243
6.265272 , 6.0656
2.1791995 , 2.723
0.112798214 , 0.1018
4.136862 , 3.7069
2.372048 , 2.2758
1.774152 , 1.68
5.5264177 , 5.24
0.15066814 , 0.1723
3.5067995 , 3.213
2.15767 , 1.8938
4.534355 , 4.3553
2.4920754 , 2.5739
1.4215815 , 1.3787
1.2490304 , 1.4833
5.5758696 , 5.9266
4.7833414 , 5.1906
0.15374136 , 0.1685
2.803348 , 3.4844
9.280979 , 11.8104
4.13591 , 3.9698
7.3031163 , 6.6144
1.2181904 , 1.1618
0.58631325 , 0.6068
6.007201 , 6.0992
5.264941 , 4.8013
2.7894588 , 2.6153
2.5264077 , 2.3509
5.0436935 , 5.4757
3.8063068 , 3.7147
3.1221354 , 3.2914
6.4290104 , 6.2717
2.8983297 , 3.5311
4.8896723 , 3.9082
103.34753 , 95.2006
6.4842415 , 6.634
2.1568918 , 1.9934
7.509491 , 7.694
4.318241 , 4.1839
1.9063516 , 2.2586
5.767495 , 6.0883
1.6433203 , 1.7909
2.425365 , 2.606
0.84001327 , 0.5502
4.1633406 , 4.4432
0.23144317 , 0.2001
6.166376 , 6.1804
5.9263306 , 5.9234
2.1089087 , 2.2014
3.3022292 , 3.2456
6.087757 , 6.5623
2.8869512 , 3.1296
9.261899 , 11.7139
1.7789917 , 1.5808
3.8921478 , 4.2954
2.2266338 , 2.2423
0.73429394 , 0.6475
1.4486325 , 1.3784
5.5333962 , 5.5112
4.9170413 , 4.7329
9.236482 , 10.0909
2.5824604 , 2.1179
1.8342657 , 1.8377
8.712101 , 8.9992
4.1142254 , 4.7954
5.7964587 , 5.7381
0.099197865 , 0.1421
88.97333 , 91.9696
0.32567644 , 0.2732
2.5134025 , 2.3859
2.0508738 , 1.9492
0.09122348 , 0.1509
4.871808 , 4.4991
0.5427189 , 0.6039
5.4505153 , 5.5911
7.0244465 , 7.1733
4.059814 , 3.3138
5.3702574 , 5.1342
2.193237 , 2.1557
0.19426012 , 0.159
5.1781645 , 5.4281
0.5562298 , 0.4664
5.363488 , 5.3111
3.562885 , 3.8508
2.2545838 , 2.4107
7.290048 , 5.3878
5.428616 , 3.9578
1.4529278 , 1.5012
4.250614 , 4.2888
0.25290942 , 0.2558
0.8461158 , 0.6106
1.7674472 , 1.707
0.07708311 , 0.1272
3.7811863 , 3.7607
1.8725908 , 1.8477
5.265615 , 5.7496
6.727771 , 6.8089
2.0269253 , 2.1163
7.062003 , 5.2338
5.4017982 , 5.2436
3.7805383 , 3.8682
8.833057 , 9.0391
2.5219772 , 2.606
0.066860676 , 0.1351
5.055508 , 4.8233
1.5818336 , 1.5004
5.692172 , 5.5153
3.8795104 , 4.0092
0.1613028 , 0.1781
6.767598 , 6.8355
1.4240708 , 1.3525
0.10155797 , 0.1186
6.0229526 , 5.6257
3.6836538 , 3.7964
5.8366184 , 5.3603
0.09648776 , 0.1015
2.4633737 , 2.4978
0.2636881 , 0.2186
3.403265 , 4.104
1.8869481 , 1.8545
0.17406464 , 0.1534
3.704455 , 4.3527
8.995005 , 13.6383
0.0955677 , 0.1407
1.3962994 , 1.3681
2.716332 , 2.5838
1.1554391 , 1.0798
0.1192348 , 0.1003
0.11779976 , 0.1518
6.298326 , 4.6319
1.5184073 , 1.2715
7.4998846 , 7.2647
3.12608 , 2.3793
1.5185571 , 1.1725
0.60044456 , 0.5404
1.7357676 , 1.6682
1.4659703 , 1.5297
7.828333 , 8.7653
6.5561624 , 6.1009
0.21704054 , 0.1891
1.6438777 , 1.6155
5.317964 , 5.6347
0.1422031 , 0.1358
0.12109041 , 0.1159
1.6200037 , 1.5714
4.4020047 , 4.183
1.4913692 , 1.5344
1.3501666 , 1.2488
5.103572 , 4.9503
2.0970051 , 1.8169
1.9797766 , 2.378
0.13689494 , 0.127
RMSE:  0.8752851845585006  MAPE: 0.11043946035028927
5: ground truth total-  137  predicted total -  137
100: ground truth total-  45  predicted total -  45
 more 100: ground truth total -  0  predicted total -  0
